{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "apKMXdhSeYFz",
        "outputId": "c120545d-3ae4-45bc-da91-f7750c99cf3f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement tensorflow==2.11.0 (from versions: 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0, 2.18.1, 2.19.0rc0, 2.19.0)\n",
            "ERROR: No matching distribution found for tensorflow==2.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.11.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ti4UoDIIQ5nt",
        "outputId": "dfe495ca-b643-4049-dbe3-0a79a6e99247"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/VenkateshwaranB/stellargraph.git\n",
            "  Cloning https://github.com/VenkateshwaranB/stellargraph.git to c:\\users\\venka\\appdata\\local\\temp\\pip-req-build-iaz5s6p_\n",
            "  Resolved https://github.com/VenkateshwaranB/stellargraph.git to commit efa1f847109a4ba490e7a5105646a20ee09a3243\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting tensorflow>=2.1.0 (from stellargraph==1.3.0b0)\n",
            "  Downloading tensorflow-2.19.0-cp312-cp312-win_amd64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: numpy>=1.14 in c:\\users\\venka\\anaconda2024\\lib\\site-packages (from stellargraph==1.3.0b0) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\venka\\anaconda2024\\lib\\site-packages (from stellargraph==1.3.0b0) (1.13.1)\n",
            "Requirement already satisfied: networkx>=2.2 in c:\\users\\venka\\anaconda2024\\lib\\site-packages (from stellargraph==1.3.0b0) (3.3)\n",
            "Requirement already satisfied: scikit_learn>=0.20 in c:\\users\\venka\\anaconda2024\\lib\\site-packages (from stellargraph==1.3.0b0) (1.5.1)\n",
            "Requirement already satisfied: matplotlib>=2.2 in c:\\users\\venka\\anaconda2024\\lib\\site-packages (from stellargraph==1.3.0b0) (3.9.2)\n",
            "Requirement already satisfied: pandas>=0.24 in c:\\users\\venka\\anaconda2024\\lib\\site-packages (from stellargraph==1.3.0b0) (2.2.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\venka\\anaconda2024\\lib\\site-packages (from matplotlib>=2.2->stellargraph==1.3.0b0) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\venka\\anaconda2024\\lib\\site-packages (from matplotlib>=2.2->stellargraph==1.3.0b0) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\venka\\anaconda2024\\lib\\site-packages (from matplotlib>=2.2->stellargraph==1.3.0b0) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\venka\\anaconda2024\\lib\\site-packages (from matplotlib>=2.2->stellargraph==1.3.0b0) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\venka\\anaconda2024\\lib\\site-packages (from matplotlib>=2.2->stellargraph==1.3.0b0) (24.1)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\venka\\anaconda2024\\lib\\site-packages (from matplotlib>=2.2->stellargraph==1.3.0b0) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\venka\\anaconda2024\\lib\\site-packages (from matplotlib>=2.2->stellargraph==1.3.0b0) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\venka\\anaconda2024\\lib\\site-packages (from matplotlib>=2.2->stellargraph==1.3.0b0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\venka\\anaconda2024\\lib\\site-packages (from pandas>=0.24->stellargraph==1.3.0b0) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\venka\\anaconda2024\\lib\\site-packages (from pandas>=0.24->stellargraph==1.3.0b0) (2023.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\venka\\anaconda2024\\lib\\site-packages (from scikit_learn>=0.20->stellargraph==1.3.0b0) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\venka\\anaconda2024\\lib\\site-packages (from scikit_learn>=0.20->stellargraph==1.3.0b0) (3.5.0)\n",
            "Collecting absl-py>=1.0.0 (from tensorflow>=2.1.0->stellargraph==1.3.0b0)\n",
            "  Downloading absl_py-2.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow>=2.1.0->stellargraph==1.3.0b0)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow>=2.1.0->stellargraph==1.3.0b0)\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow>=2.1.0->stellargraph==1.3.0b0)\n",
            "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow>=2.1.0->stellargraph==1.3.0b0)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow>=2.1.0->stellargraph==1.3.0b0)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
            "Collecting opt-einsum>=2.3.2 (from tensorflow>=2.1.0->stellargraph==1.3.0b0)\n",
            "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\venka\\anaconda2024\\lib\\site-packages (from tensorflow>=2.1.0->stellargraph==1.3.0b0) (4.25.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\venka\\anaconda2024\\lib\\site-packages (from tensorflow>=2.1.0->stellargraph==1.3.0b0) (2.32.3)\n",
            "Requirement already satisfied: setuptools in c:\\users\\venka\\anaconda2024\\lib\\site-packages (from tensorflow>=2.1.0->stellargraph==1.3.0b0) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\venka\\anaconda2024\\lib\\site-packages (from tensorflow>=2.1.0->stellargraph==1.3.0b0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\venka\\anaconda2024\\lib\\site-packages (from tensorflow>=2.1.0->stellargraph==1.3.0b0) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\venka\\anaconda2024\\lib\\site-packages (from tensorflow>=2.1.0->stellargraph==1.3.0b0) (4.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\venka\\anaconda2024\\lib\\site-packages (from tensorflow>=2.1.0->stellargraph==1.3.0b0) (1.14.1)\n",
            "Collecting grpcio<2.0,>=1.24.3 (from tensorflow>=2.1.0->stellargraph==1.3.0b0)\n",
            "  Downloading grpcio-1.71.0-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow>=2.1.0->stellargraph==1.3.0b0)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting keras>=3.5.0 (from tensorflow>=2.1.0->stellargraph==1.3.0b0)\n",
            "  Downloading keras-3.9.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\venka\\anaconda2024\\lib\\site-packages (from tensorflow>=2.1.0->stellargraph==1.3.0b0) (3.12.1)\n",
            "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow>=2.1.0->stellargraph==1.3.0b0)\n",
            "  Downloading ml_dtypes-0.5.1-cp312-cp312-win_amd64.whl.metadata (22 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\venka\\anaconda2024\\lib\\site-packages (from astunparse>=1.6.0->tensorflow>=2.1.0->stellargraph==1.3.0b0) (0.44.0)\n",
            "Requirement already satisfied: rich in c:\\users\\venka\\anaconda2024\\lib\\site-packages (from keras>=3.5.0->tensorflow>=2.1.0->stellargraph==1.3.0b0) (13.7.1)\n",
            "Collecting namex (from keras>=3.5.0->tensorflow>=2.1.0->stellargraph==1.3.0b0)\n",
            "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
            "Collecting optree (from keras>=3.5.0->tensorflow>=2.1.0->stellargraph==1.3.0b0)\n",
            "  Downloading optree-0.15.0-cp312-cp312-win_amd64.whl.metadata (49 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\venka\\anaconda2024\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow>=2.1.0->stellargraph==1.3.0b0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\venka\\anaconda2024\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow>=2.1.0->stellargraph==1.3.0b0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\venka\\anaconda2024\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow>=2.1.0->stellargraph==1.3.0b0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\venka\\anaconda2024\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow>=2.1.0->stellargraph==1.3.0b0) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\venka\\anaconda2024\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow>=2.1.0->stellargraph==1.3.0b0) (3.4.1)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow>=2.1.0->stellargraph==1.3.0b0)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\venka\\anaconda2024\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow>=2.1.0->stellargraph==1.3.0b0) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\venka\\anaconda2024\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow>=2.1.0->stellargraph==1.3.0b0) (2.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\venka\\anaconda2024\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow>=2.1.0->stellargraph==1.3.0b0) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\venka\\anaconda2024\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow>=2.1.0->stellargraph==1.3.0b0) (2.15.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\venka\\anaconda2024\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow>=2.1.0->stellargraph==1.3.0b0) (0.1.0)\n",
            "Downloading tensorflow-2.19.0-cp312-cp312-win_amd64.whl (376.0 MB)\n",
            "   ---------------------------------------- 0.0/376.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 2.1/376.0 MB 11.8 MB/s eta 0:00:32\n",
            "    --------------------------------------- 4.7/376.0 MB 11.4 MB/s eta 0:00:33\n",
            "    --------------------------------------- 7.3/376.0 MB 11.6 MB/s eta 0:00:32\n",
            "   - -------------------------------------- 9.7/376.0 MB 11.6 MB/s eta 0:00:32\n",
            "   - -------------------------------------- 12.3/376.0 MB 11.7 MB/s eta 0:00:32\n",
            "   - -------------------------------------- 14.7/376.0 MB 11.7 MB/s eta 0:00:31\n",
            "   - -------------------------------------- 17.3/376.0 MB 11.7 MB/s eta 0:00:31\n",
            "   -- ------------------------------------- 19.9/376.0 MB 11.8 MB/s eta 0:00:31\n",
            "   -- ------------------------------------- 22.3/376.0 MB 11.7 MB/s eta 0:00:31\n",
            "   -- ------------------------------------- 24.9/376.0 MB 11.8 MB/s eta 0:00:30\n",
            "   -- ------------------------------------- 27.5/376.0 MB 11.8 MB/s eta 0:00:30\n",
            "   --- ------------------------------------ 29.9/376.0 MB 11.8 MB/s eta 0:00:30\n",
            "   --- ------------------------------------ 32.5/376.0 MB 11.7 MB/s eta 0:00:30\n",
            "   --- ------------------------------------ 34.9/376.0 MB 11.8 MB/s eta 0:00:29\n",
            "   --- ------------------------------------ 37.5/376.0 MB 11.8 MB/s eta 0:00:29\n",
            "   ---- ----------------------------------- 40.1/376.0 MB 11.8 MB/s eta 0:00:29\n",
            "   ---- ----------------------------------- 42.5/376.0 MB 11.7 MB/s eta 0:00:29\n",
            "   ---- ----------------------------------- 44.8/376.0 MB 11.7 MB/s eta 0:00:29\n",
            "   ----- ---------------------------------- 47.4/376.0 MB 11.8 MB/s eta 0:00:28\n",
            "   ----- ---------------------------------- 50.1/376.0 MB 11.8 MB/s eta 0:00:28\n",
            "   ----- ---------------------------------- 52.4/376.0 MB 11.8 MB/s eta 0:00:28\n",
            "   ----- ---------------------------------- 55.1/376.0 MB 11.8 MB/s eta 0:00:28\n",
            "   ------ --------------------------------- 57.7/376.0 MB 11.8 MB/s eta 0:00:28\n",
            "   ------ --------------------------------- 60.0/376.0 MB 11.8 MB/s eta 0:00:27\n",
            "   ------ --------------------------------- 62.7/376.0 MB 11.8 MB/s eta 0:00:27\n",
            "   ------ --------------------------------- 65.0/376.0 MB 11.8 MB/s eta 0:00:27\n",
            "   ------- -------------------------------- 67.6/376.0 MB 11.8 MB/s eta 0:00:27\n",
            "   ------- -------------------------------- 70.0/376.0 MB 11.7 MB/s eta 0:00:27\n",
            "   ------- -------------------------------- 72.4/376.0 MB 11.7 MB/s eta 0:00:26\n",
            "   ------- -------------------------------- 74.7/376.0 MB 11.7 MB/s eta 0:00:26\n",
            "   -------- ------------------------------- 76.8/376.0 MB 11.7 MB/s eta 0:00:26\n",
            "   -------- ------------------------------- 79.4/376.0 MB 11.7 MB/s eta 0:00:26\n",
            "   -------- ------------------------------- 81.8/376.0 MB 11.7 MB/s eta 0:00:26\n",
            "   -------- ------------------------------- 84.4/376.0 MB 11.7 MB/s eta 0:00:25\n",
            "   --------- ------------------------------ 87.0/376.0 MB 11.7 MB/s eta 0:00:25\n",
            "   --------- ------------------------------ 89.4/376.0 MB 11.7 MB/s eta 0:00:25\n",
            "   --------- ------------------------------ 91.8/376.0 MB 11.7 MB/s eta 0:00:25\n",
            "   ---------- ----------------------------- 94.1/376.0 MB 11.7 MB/s eta 0:00:25\n",
            "   ---------- ----------------------------- 96.7/376.0 MB 11.7 MB/s eta 0:00:24\n",
            "   ---------- ----------------------------- 99.4/376.0 MB 11.7 MB/s eta 0:00:24\n",
            "   ---------- ---------------------------- 101.7/376.0 MB 11.7 MB/s eta 0:00:24\n",
            "   ---------- ---------------------------- 104.3/376.0 MB 11.7 MB/s eta 0:00:24\n",
            "   ----------- --------------------------- 106.7/376.0 MB 11.7 MB/s eta 0:00:24\n",
            "   ----------- --------------------------- 109.3/376.0 MB 11.7 MB/s eta 0:00:23\n",
            "   ----------- --------------------------- 111.9/376.0 MB 11.7 MB/s eta 0:00:23\n",
            "   ----------- --------------------------- 114.3/376.0 MB 11.7 MB/s eta 0:00:23\n",
            "   ------------ -------------------------- 116.9/376.0 MB 11.7 MB/s eta 0:00:23\n",
            "   ------------ -------------------------- 119.3/376.0 MB 11.7 MB/s eta 0:00:22\n",
            "   ------------ -------------------------- 121.9/376.0 MB 11.7 MB/s eta 0:00:22\n",
            "   ------------ -------------------------- 124.5/376.0 MB 11.7 MB/s eta 0:00:22\n",
            "   ------------- ------------------------- 127.1/376.0 MB 11.7 MB/s eta 0:00:22\n",
            "   ------------- ------------------------- 129.5/376.0 MB 11.7 MB/s eta 0:00:22\n",
            "   ------------- ------------------------- 132.1/376.0 MB 11.7 MB/s eta 0:00:21\n",
            "   ------------- ------------------------- 134.5/376.0 MB 11.7 MB/s eta 0:00:21\n",
            "   -------------- ------------------------ 137.1/376.0 MB 11.7 MB/s eta 0:00:21\n",
            "   -------------- ------------------------ 139.7/376.0 MB 11.7 MB/s eta 0:00:21\n",
            "   -------------- ------------------------ 142.3/376.0 MB 11.7 MB/s eta 0:00:20\n",
            "   --------------- ----------------------- 144.7/376.0 MB 11.7 MB/s eta 0:00:20\n",
            "   --------------- ----------------------- 147.3/376.0 MB 11.7 MB/s eta 0:00:20\n",
            "   --------------- ----------------------- 149.9/376.0 MB 11.7 MB/s eta 0:00:20\n",
            "   --------------- ----------------------- 152.6/376.0 MB 11.7 MB/s eta 0:00:20\n",
            "   ---------------- ---------------------- 154.9/376.0 MB 11.7 MB/s eta 0:00:19\n",
            "   ---------------- ---------------------- 157.5/376.0 MB 11.7 MB/s eta 0:00:19\n",
            "   ---------------- ---------------------- 160.2/376.0 MB 11.7 MB/s eta 0:00:19\n",
            "   ---------------- ---------------------- 162.8/376.0 MB 11.7 MB/s eta 0:00:19\n",
            "   ----------------- --------------------- 165.2/376.0 MB 11.7 MB/s eta 0:00:18\n",
            "   ----------------- --------------------- 167.8/376.0 MB 11.7 MB/s eta 0:00:18\n",
            "   ----------------- --------------------- 170.4/376.0 MB 11.7 MB/s eta 0:00:18\n",
            "   ----------------- --------------------- 172.8/376.0 MB 11.7 MB/s eta 0:00:18\n",
            "   ------------------ -------------------- 174.6/376.0 MB 11.7 MB/s eta 0:00:18\n",
            "   ------------------ -------------------- 176.9/376.0 MB 11.7 MB/s eta 0:00:18\n",
            "   ------------------ -------------------- 179.6/376.0 MB 11.7 MB/s eta 0:00:17\n",
            "   ------------------ -------------------- 181.9/376.0 MB 11.7 MB/s eta 0:00:17\n",
            "   ------------------- ------------------- 184.3/376.0 MB 11.7 MB/s eta 0:00:17\n",
            "   ------------------- ------------------- 186.1/376.0 MB 11.6 MB/s eta 0:00:17\n",
            "   ------------------- ------------------- 188.2/376.0 MB 11.6 MB/s eta 0:00:17\n",
            "   ------------------- ------------------- 190.1/376.0 MB 11.6 MB/s eta 0:00:17\n",
            "   ------------------- ------------------- 192.4/376.0 MB 11.6 MB/s eta 0:00:16\n",
            "   -------------------- ------------------ 194.8/376.0 MB 11.5 MB/s eta 0:00:16\n",
            "   -------------------- ------------------ 197.1/376.0 MB 11.5 MB/s eta 0:00:16\n",
            "   -------------------- ------------------ 199.5/376.0 MB 11.5 MB/s eta 0:00:16\n",
            "   -------------------- ------------------ 201.9/376.0 MB 11.5 MB/s eta 0:00:16\n",
            "   --------------------- ----------------- 204.5/376.0 MB 11.5 MB/s eta 0:00:15\n",
            "   --------------------- ----------------- 206.8/376.0 MB 11.5 MB/s eta 0:00:15\n",
            "   --------------------- ----------------- 209.5/376.0 MB 11.5 MB/s eta 0:00:15\n",
            "   --------------------- ----------------- 211.8/376.0 MB 11.5 MB/s eta 0:00:15\n",
            "   ---------------------- ---------------- 214.4/376.0 MB 11.5 MB/s eta 0:00:14\n",
            "   ---------------------- ---------------- 217.1/376.0 MB 11.5 MB/s eta 0:00:14\n",
            "   ---------------------- ---------------- 219.7/376.0 MB 11.5 MB/s eta 0:00:14\n",
            "   ----------------------- --------------- 222.3/376.0 MB 11.6 MB/s eta 0:00:14\n",
            "   ----------------------- --------------- 224.7/376.0 MB 11.6 MB/s eta 0:00:14\n",
            "   ----------------------- --------------- 227.3/376.0 MB 11.6 MB/s eta 0:00:13\n",
            "   ----------------------- --------------- 229.6/376.0 MB 11.6 MB/s eta 0:00:13\n",
            "   ------------------------ -------------- 232.3/376.0 MB 11.6 MB/s eta 0:00:13\n",
            "   ------------------------ -------------- 234.9/376.0 MB 11.6 MB/s eta 0:00:13\n",
            "   ------------------------ -------------- 237.2/376.0 MB 11.6 MB/s eta 0:00:13\n",
            "   ------------------------ -------------- 239.9/376.0 MB 11.6 MB/s eta 0:00:12\n",
            "   ------------------------- ------------- 242.2/376.0 MB 11.6 MB/s eta 0:00:12\n",
            "   ------------------------- ------------- 244.8/376.0 MB 11.6 MB/s eta 0:00:12\n",
            "   ------------------------- ------------- 247.2/376.0 MB 11.6 MB/s eta 0:00:12\n",
            "   ------------------------- ------------- 249.6/376.0 MB 11.6 MB/s eta 0:00:11\n",
            "   -------------------------- ------------ 252.2/376.0 MB 11.6 MB/s eta 0:00:11\n",
            "   -------------------------- ------------ 254.5/376.0 MB 11.6 MB/s eta 0:00:11\n",
            "   -------------------------- ------------ 256.6/376.0 MB 11.5 MB/s eta 0:00:11\n",
            "   -------------------------- ------------ 259.0/376.0 MB 11.5 MB/s eta 0:00:11\n",
            "   --------------------------- ----------- 261.6/376.0 MB 11.6 MB/s eta 0:00:10\n",
            "   --------------------------- ----------- 264.0/376.0 MB 11.6 MB/s eta 0:00:10\n",
            "   --------------------------- ----------- 266.6/376.0 MB 11.6 MB/s eta 0:00:10\n",
            "   --------------------------- ----------- 269.0/376.0 MB 11.5 MB/s eta 0:00:10\n",
            "   ---------------------------- ---------- 271.6/376.0 MB 11.6 MB/s eta 0:00:10\n",
            "   ---------------------------- ---------- 273.4/376.0 MB 11.5 MB/s eta 0:00:09\n",
            "   ---------------------------- ---------- 275.3/376.0 MB 11.5 MB/s eta 0:00:09\n",
            "   ---------------------------- ---------- 277.3/376.0 MB 11.5 MB/s eta 0:00:09\n",
            "   ---------------------------- ---------- 279.4/376.0 MB 11.5 MB/s eta 0:00:09\n",
            "   ----------------------------- --------- 281.8/376.0 MB 11.5 MB/s eta 0:00:09\n",
            "   ----------------------------- --------- 284.4/376.0 MB 11.5 MB/s eta 0:00:08\n",
            "   ----------------------------- --------- 286.8/376.0 MB 11.5 MB/s eta 0:00:08\n",
            "   ----------------------------- --------- 289.1/376.0 MB 11.5 MB/s eta 0:00:08\n",
            "   ------------------------------ -------- 291.8/376.0 MB 11.5 MB/s eta 0:00:08\n",
            "   ------------------------------ -------- 294.4/376.0 MB 11.5 MB/s eta 0:00:08\n",
            "   ------------------------------ -------- 297.0/376.0 MB 11.5 MB/s eta 0:00:07\n",
            "   ------------------------------- ------- 299.6/376.0 MB 11.5 MB/s eta 0:00:07\n",
            "   ------------------------------- ------- 302.0/376.0 MB 11.5 MB/s eta 0:00:07\n",
            "   ------------------------------- ------- 304.6/376.0 MB 11.5 MB/s eta 0:00:07\n",
            "   ------------------------------- ------- 307.2/376.0 MB 11.5 MB/s eta 0:00:06\n",
            "   -------------------------------- ------ 309.9/376.0 MB 11.5 MB/s eta 0:00:06\n",
            "   -------------------------------- ------ 312.2/376.0 MB 11.5 MB/s eta 0:00:06\n",
            "   -------------------------------- ------ 314.8/376.0 MB 11.5 MB/s eta 0:00:06\n",
            "   -------------------------------- ------ 317.5/376.0 MB 11.5 MB/s eta 0:00:06\n",
            "   --------------------------------- ----- 320.1/376.0 MB 11.5 MB/s eta 0:00:05\n",
            "   --------------------------------- ----- 322.4/376.0 MB 11.5 MB/s eta 0:00:05\n",
            "   --------------------------------- ----- 324.8/376.0 MB 11.5 MB/s eta 0:00:05\n",
            "   --------------------------------- ----- 327.4/376.0 MB 11.5 MB/s eta 0:00:05\n",
            "   ---------------------------------- ---- 330.0/376.0 MB 11.5 MB/s eta 0:00:05\n",
            "   ---------------------------------- ---- 332.7/376.0 MB 11.5 MB/s eta 0:00:04\n",
            "   ---------------------------------- ---- 335.0/376.0 MB 11.5 MB/s eta 0:00:04\n",
            "   ---------------------------------- ---- 337.4/376.0 MB 11.5 MB/s eta 0:00:04\n",
            "   ----------------------------------- --- 340.0/376.0 MB 11.5 MB/s eta 0:00:04\n",
            "   ----------------------------------- --- 342.6/376.0 MB 11.5 MB/s eta 0:00:03\n",
            "   ----------------------------------- --- 345.0/376.0 MB 11.5 MB/s eta 0:00:03\n",
            "   ------------------------------------ -- 347.6/376.0 MB 11.5 MB/s eta 0:00:03\n",
            "   ------------------------------------ -- 350.0/376.0 MB 11.5 MB/s eta 0:00:03\n",
            "   ------------------------------------ -- 352.6/376.0 MB 11.5 MB/s eta 0:00:03\n",
            "   ------------------------------------ -- 355.2/376.0 MB 11.5 MB/s eta 0:00:02\n",
            "   ------------------------------------- - 357.6/376.0 MB 11.5 MB/s eta 0:00:02\n",
            "   ------------------------------------- - 360.2/376.0 MB 11.5 MB/s eta 0:00:02\n",
            "   ------------------------------------- - 362.8/376.0 MB 11.5 MB/s eta 0:00:02\n",
            "   ------------------------------------- - 365.4/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  367.8/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  370.4/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  373.0/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.7/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  375.9/376.0 MB 11.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 376.0/376.0 MB 3.8 MB/s eta 0:00:00\n",
            "Downloading absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
            "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Downloading grpcio-1.71.0-cp312-cp312-win_amd64.whl (4.3 MB)\n",
            "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
            "   ---------------------- ----------------- 2.4/4.3 MB 12.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------  4.2/4.3 MB 12.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 4.3/4.3 MB 8.3 MB/s eta 0:00:00\n",
            "Downloading keras-3.9.2-py3-none-any.whl (1.3 MB)\n",
            "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
            "   ---------------------------------------- 1.3/1.3 MB 6.9 MB/s eta 0:00:00\n",
            "Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
            "   ---------------------------------------- 0.0/26.4 MB ? eta -:--:--\n",
            "   --- ------------------------------------ 2.4/26.4 MB 12.2 MB/s eta 0:00:02\n",
            "   ------- -------------------------------- 4.7/26.4 MB 11.9 MB/s eta 0:00:02\n",
            "   ----------- ---------------------------- 7.3/26.4 MB 11.9 MB/s eta 0:00:02\n",
            "   -------------- ------------------------- 9.7/26.4 MB 11.6 MB/s eta 0:00:02\n",
            "   ----------------- ---------------------- 11.8/26.4 MB 11.4 MB/s eta 0:00:02\n",
            "   --------------------- ------------------ 14.4/26.4 MB 11.3 MB/s eta 0:00:02\n",
            "   ------------------------- -------------- 16.8/26.4 MB 11.5 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 19.1/26.4 MB 11.5 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 21.8/26.4 MB 11.6 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 24.4/26.4 MB 11.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------  26.2/26.4 MB 11.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------  26.2/26.4 MB 11.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------  26.2/26.4 MB 11.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------  26.2/26.4 MB 11.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------  26.2/26.4 MB 11.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------  26.2/26.4 MB 11.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------  26.2/26.4 MB 11.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------  26.2/26.4 MB 11.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------  26.2/26.4 MB 11.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------  26.2/26.4 MB 11.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 26.4/26.4 MB 6.2 MB/s eta 0:00:00\n",
            "Downloading ml_dtypes-0.5.1-cp312-cp312-win_amd64.whl (210 kB)\n",
            "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
            "Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
            "   --------------- ------------------------ 2.1/5.5 MB 11.8 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 4.7/5.5 MB 11.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 5.5/5.5 MB 11.2 MB/s eta 0:00:00\n",
            "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
            "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Downloading optree-0.15.0-cp312-cp312-win_amd64.whl (307 kB)\n",
            "Building wheels for collected packages: stellargraph\n",
            "  Building wheel for stellargraph (setup.py): started\n",
            "  Building wheel for stellargraph (setup.py): finished with status 'done'\n",
            "  Created wheel for stellargraph: filename=stellargraph-1.3.0b0-py3-none-any.whl size=434654 sha256=c3a831ae37e7b479a40f46712342748641ca497b2a4a699716de5239c389537b\n",
            "  Stored in directory: C:\\Users\\venka\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-3brfqw1k\\wheels\\dc\\f2\\b8\\ac94f45a270c91d5fe3598e87c658e50d20752f4544759cefc\n",
            "Successfully built stellargraph\n",
            "Installing collected packages: namex, libclang, flatbuffers, tensorboard-data-server, optree, opt-einsum, ml-dtypes, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, keras, tensorflow, stellargraph\n",
            "Successfully installed absl-py-2.2.2 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.71.0 keras-3.9.2 libclang-18.1.1 ml-dtypes-0.5.1 namex-0.0.8 opt-einsum-3.4.0 optree-0.15.0 stellargraph-1.3.0b0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  Running command git clone --filter=blob:none --quiet https://github.com/VenkateshwaranB/stellargraph.git 'C:\\Users\\venka\\AppData\\Local\\Temp\\pip-req-build-iaz5s6p_'\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/VenkateshwaranB/stellargraph.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Bd8WkV9MuI6",
        "outputId": "950a4baa-1ea9-46f7-b258-6500ce9fa29e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkgIFGuKclb8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import os\n",
        "\n",
        "import stellargraph as sg\n",
        "from stellargraph import StellarGraph\n",
        "from stellargraph.mapper import FullBatchNodeGenerator\n",
        "from stellargraph.layer import GCN\n",
        "\n",
        "from tensorflow.keras import layers, optimizers, losses, metrics, Model\n",
        "from sklearn import preprocessing, model_selection\n",
        "from IPython.display import display, HTML\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czm8iticltU8"
      },
      "source": [
        "The below cell include all the things you just run this cell only. and make sure the drive name was appropriate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sivrh3G3Y2v7",
        "outputId": "c1f0d108-52a8-4344-8673-1eb988a3c030"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5a\n",
            "Using GCN (local pooling) filters...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1/1 - 2s - loss: 1.2492 - acc: 0.8660 - val_loss: 1.2365 - val_acc: 0.8738 - 2s/epoch - 2s/step\n",
            "Epoch 2/200\n",
            "1/1 - 1s - loss: 1.2351 - acc: 0.8688 - val_loss: 1.2153 - val_acc: 0.8738 - 603ms/epoch - 603ms/step\n",
            "Epoch 3/200\n",
            "1/1 - 1s - loss: 1.2238 - acc: 0.8669 - val_loss: 1.1943 - val_acc: 0.8738 - 635ms/epoch - 635ms/step\n",
            "Epoch 4/200\n",
            "1/1 - 1s - loss: 1.2098 - acc: 0.8708 - val_loss: 1.1740 - val_acc: 0.8738 - 625ms/epoch - 625ms/step\n",
            "Epoch 5/200\n",
            "1/1 - 1s - loss: 1.1667 - acc: 0.8725 - val_loss: 1.1537 - val_acc: 0.8738 - 561ms/epoch - 561ms/step\n",
            "Epoch 6/200\n",
            "1/1 - 1s - loss: 1.1535 - acc: 0.8683 - val_loss: 1.1335 - val_acc: 0.8738 - 531ms/epoch - 531ms/step\n",
            "Epoch 7/200\n",
            "1/1 - 1s - loss: 1.1354 - acc: 0.8733 - val_loss: 1.1135 - val_acc: 0.8738 - 565ms/epoch - 565ms/step\n",
            "Epoch 8/200\n",
            "1/1 - 1s - loss: 1.1070 - acc: 0.8750 - val_loss: 1.0935 - val_acc: 0.8738 - 548ms/epoch - 548ms/step\n",
            "Epoch 9/200\n",
            "1/1 - 1s - loss: 1.0958 - acc: 0.8742 - val_loss: 1.0737 - val_acc: 0.8738 - 553ms/epoch - 553ms/step\n",
            "Epoch 10/200\n",
            "1/1 - 1s - loss: 1.0668 - acc: 0.8742 - val_loss: 1.0540 - val_acc: 0.8738 - 760ms/epoch - 760ms/step\n",
            "Epoch 11/200\n",
            "1/1 - 1s - loss: 1.0523 - acc: 0.8739 - val_loss: 1.0345 - val_acc: 0.8738 - 968ms/epoch - 968ms/step\n",
            "Epoch 12/200\n",
            "1/1 - 1s - loss: 1.0410 - acc: 0.8731 - val_loss: 1.0152 - val_acc: 0.8738 - 950ms/epoch - 950ms/step\n",
            "Epoch 13/200\n",
            "1/1 - 1s - loss: 1.0062 - acc: 0.8742 - val_loss: 0.9959 - val_acc: 0.8738 - 1s/epoch - 1s/step\n",
            "Epoch 14/200\n",
            "1/1 - 1s - loss: 0.9883 - acc: 0.8756 - val_loss: 0.9769 - val_acc: 0.8738 - 1s/epoch - 1s/step\n",
            "Epoch 15/200\n",
            "1/1 - 2s - loss: 0.9835 - acc: 0.8739 - val_loss: 0.9580 - val_acc: 0.8738 - 2s/epoch - 2s/step\n",
            "Epoch 16/200\n",
            "1/1 - 1s - loss: 0.9538 - acc: 0.8728 - val_loss: 0.9393 - val_acc: 0.8738 - 528ms/epoch - 528ms/step\n",
            "Epoch 17/200\n",
            "1/1 - 1s - loss: 0.9370 - acc: 0.8748 - val_loss: 0.9208 - val_acc: 0.8738 - 515ms/epoch - 515ms/step\n",
            "Epoch 18/200\n",
            "1/1 - 1s - loss: 0.8882 - acc: 0.8832 - val_loss: 0.9026 - val_acc: 0.8738 - 610ms/epoch - 610ms/step\n",
            "Epoch 19/200\n",
            "1/1 - 1s - loss: 0.9119 - acc: 0.8756 - val_loss: 0.8845 - val_acc: 0.8738 - 610ms/epoch - 610ms/step\n",
            "Epoch 20/200\n",
            "1/1 - 1s - loss: 0.8816 - acc: 0.8731 - val_loss: 0.8666 - val_acc: 0.8738 - 609ms/epoch - 609ms/step\n",
            "Epoch 21/200\n",
            "1/1 - 1s - loss: 0.8680 - acc: 0.8759 - val_loss: 0.8490 - val_acc: 0.8738 - 525ms/epoch - 525ms/step\n",
            "Epoch 22/200\n",
            "1/1 - 1s - loss: 0.8445 - acc: 0.8736 - val_loss: 0.8316 - val_acc: 0.8738 - 526ms/epoch - 526ms/step\n",
            "Epoch 23/200\n",
            "1/1 - 1s - loss: 0.8274 - acc: 0.8759 - val_loss: 0.8144 - val_acc: 0.8738 - 611ms/epoch - 611ms/step\n",
            "Epoch 24/200\n",
            "1/1 - 1s - loss: 0.8131 - acc: 0.8739 - val_loss: 0.7975 - val_acc: 0.8738 - 611ms/epoch - 611ms/step\n",
            "Epoch 25/200\n",
            "1/1 - 1s - loss: 0.8019 - acc: 0.8818 - val_loss: 0.7809 - val_acc: 0.8738 - 607ms/epoch - 607ms/step\n",
            "Epoch 26/200\n",
            "1/1 - 1s - loss: 0.7688 - acc: 0.8750 - val_loss: 0.7645 - val_acc: 0.8738 - 617ms/epoch - 617ms/step\n",
            "Epoch 27/200\n",
            "1/1 - 1s - loss: 0.7632 - acc: 0.8767 - val_loss: 0.7484 - val_acc: 0.8738 - 508ms/epoch - 508ms/step\n",
            "Epoch 28/200\n",
            "1/1 - 1s - loss: 0.7606 - acc: 0.8750 - val_loss: 0.7325 - val_acc: 0.8738 - 616ms/epoch - 616ms/step\n",
            "Epoch 29/200\n",
            "1/1 - 1s - loss: 0.7278 - acc: 0.8742 - val_loss: 0.7168 - val_acc: 0.8738 - 1s/epoch - 1s/step\n",
            "Epoch 30/200\n",
            "1/1 - 1s - loss: 0.7014 - acc: 0.8745 - val_loss: 0.7015 - val_acc: 0.8738 - 930ms/epoch - 930ms/step\n",
            "Epoch 31/200\n",
            "1/1 - 1s - loss: 0.6868 - acc: 0.8770 - val_loss: 0.6865 - val_acc: 0.8738 - 953ms/epoch - 953ms/step\n",
            "Epoch 32/200\n",
            "1/1 - 1s - loss: 0.6668 - acc: 0.8852 - val_loss: 0.6718 - val_acc: 0.8738 - 1s/epoch - 1s/step\n",
            "Epoch 33/200\n",
            "1/1 - 1s - loss: 0.6651 - acc: 0.8815 - val_loss: 0.6573 - val_acc: 0.8738 - 800ms/epoch - 800ms/step\n",
            "Epoch 34/200\n",
            "1/1 - 1s - loss: 0.6524 - acc: 0.8762 - val_loss: 0.6432 - val_acc: 0.8738 - 777ms/epoch - 777ms/step\n",
            "Epoch 35/200\n",
            "1/1 - 1s - loss: 0.6442 - acc: 0.8759 - val_loss: 0.6295 - val_acc: 0.8738 - 536ms/epoch - 536ms/step\n",
            "Epoch 36/200\n",
            "1/1 - 1s - loss: 0.6370 - acc: 0.8756 - val_loss: 0.6159 - val_acc: 0.8738 - 510ms/epoch - 510ms/step\n",
            "Epoch 37/200\n",
            "1/1 - 1s - loss: 0.6213 - acc: 0.8866 - val_loss: 0.6027 - val_acc: 0.8738 - 525ms/epoch - 525ms/step\n",
            "Epoch 38/200\n",
            "1/1 - 1s - loss: 0.6120 - acc: 0.8764 - val_loss: 0.5898 - val_acc: 0.8738 - 615ms/epoch - 615ms/step\n",
            "Epoch 39/200\n",
            "1/1 - 1s - loss: 0.5911 - acc: 0.8866 - val_loss: 0.5772 - val_acc: 0.8738 - 511ms/epoch - 511ms/step\n",
            "Epoch 40/200\n",
            "1/1 - 1s - loss: 0.5620 - acc: 0.8742 - val_loss: 0.5649 - val_acc: 0.8738 - 514ms/epoch - 514ms/step\n",
            "Epoch 41/200\n",
            "1/1 - 1s - loss: 0.5684 - acc: 0.8764 - val_loss: 0.5530 - val_acc: 0.8771 - 519ms/epoch - 519ms/step\n",
            "Epoch 42/200\n",
            "1/1 - 1s - loss: 0.5647 - acc: 0.8838 - val_loss: 0.5412 - val_acc: 0.8771 - 623ms/epoch - 623ms/step\n",
            "Epoch 43/200\n",
            "1/1 - 1s - loss: 0.5520 - acc: 0.8773 - val_loss: 0.5299 - val_acc: 0.8771 - 606ms/epoch - 606ms/step\n",
            "Epoch 44/200\n",
            "1/1 - 1s - loss: 0.5282 - acc: 0.8787 - val_loss: 0.5188 - val_acc: 0.8771 - 610ms/epoch - 610ms/step\n",
            "Epoch 45/200\n",
            "1/1 - 1s - loss: 0.5074 - acc: 0.8880 - val_loss: 0.5080 - val_acc: 0.8771 - 505ms/epoch - 505ms/step\n",
            "Epoch 46/200\n",
            "1/1 - 1s - loss: 0.5091 - acc: 0.8739 - val_loss: 0.4975 - val_acc: 0.8771 - 521ms/epoch - 521ms/step\n",
            "Epoch 47/200\n",
            "1/1 - 1s - loss: 0.5039 - acc: 0.8764 - val_loss: 0.4873 - val_acc: 0.8771 - 619ms/epoch - 619ms/step\n",
            "Epoch 48/200\n",
            "1/1 - 1s - loss: 0.4918 - acc: 0.8857 - val_loss: 0.4774 - val_acc: 0.8771 - 619ms/epoch - 619ms/step\n",
            "Epoch 49/200\n",
            "1/1 - 1s - loss: 0.4803 - acc: 0.8807 - val_loss: 0.4678 - val_acc: 0.8771 - 541ms/epoch - 541ms/step\n",
            "Epoch 50/200\n",
            "1/1 - 1s - loss: 0.4652 - acc: 0.8902 - val_loss: 0.4584 - val_acc: 0.8771 - 1s/epoch - 1s/step\n",
            "Epoch 51/200\n",
            "1/1 - 1s - loss: 0.4519 - acc: 0.8874 - val_loss: 0.4493 - val_acc: 0.8771 - 1s/epoch - 1s/step\n",
            "Epoch 52/200\n",
            "1/1 - 1s - loss: 0.4477 - acc: 0.8778 - val_loss: 0.4405 - val_acc: 0.8771 - 1s/epoch - 1s/step\n",
            "Epoch 53/200\n",
            "1/1 - 2s - loss: 0.4295 - acc: 0.8891 - val_loss: 0.4320 - val_acc: 0.8771 - 2s/epoch - 2s/step\n",
            "Epoch 54/200\n",
            "1/1 - 2s - loss: 0.4263 - acc: 0.8905 - val_loss: 0.4236 - val_acc: 0.8771 - 2s/epoch - 2s/step\n",
            "Epoch 55/200\n",
            "1/1 - 1s - loss: 0.4243 - acc: 0.8869 - val_loss: 0.4155 - val_acc: 0.8771 - 1s/epoch - 1s/step\n",
            "Epoch 56/200\n",
            "1/1 - 1s - loss: 0.4129 - acc: 0.8793 - val_loss: 0.4076 - val_acc: 0.8771 - 678ms/epoch - 678ms/step\n",
            "Epoch 57/200\n",
            "1/1 - 1s - loss: 0.4021 - acc: 0.8894 - val_loss: 0.3999 - val_acc: 0.8771 - 620ms/epoch - 620ms/step\n",
            "Epoch 58/200\n",
            "1/1 - 1s - loss: 0.4045 - acc: 0.8891 - val_loss: 0.3925 - val_acc: 0.8771 - 540ms/epoch - 540ms/step\n",
            "Epoch 59/200\n",
            "1/1 - 1s - loss: 0.3865 - acc: 0.8897 - val_loss: 0.3852 - val_acc: 0.8804 - 537ms/epoch - 537ms/step\n",
            "Epoch 60/200\n",
            "1/1 - 1s - loss: 0.3780 - acc: 0.8911 - val_loss: 0.3782 - val_acc: 0.8804 - 614ms/epoch - 614ms/step\n",
            "Epoch 61/200\n",
            "1/1 - 1s - loss: 0.3793 - acc: 0.8877 - val_loss: 0.3713 - val_acc: 0.8804 - 606ms/epoch - 606ms/step\n",
            "Epoch 62/200\n",
            "1/1 - 1s - loss: 0.3775 - acc: 0.8871 - val_loss: 0.3647 - val_acc: 0.8804 - 514ms/epoch - 514ms/step\n",
            "Epoch 63/200\n",
            "1/1 - 1s - loss: 0.3563 - acc: 0.8891 - val_loss: 0.3582 - val_acc: 0.8804 - 526ms/epoch - 526ms/step\n",
            "Epoch 64/200\n",
            "1/1 - 1s - loss: 0.3451 - acc: 0.8897 - val_loss: 0.3518 - val_acc: 0.8804 - 616ms/epoch - 616ms/step\n",
            "Epoch 65/200\n",
            "1/1 - 1s - loss: 0.3517 - acc: 0.8900 - val_loss: 0.3454 - val_acc: 0.8804 - 523ms/epoch - 523ms/step\n",
            "Epoch 66/200\n",
            "1/1 - 1s - loss: 0.3500 - acc: 0.8936 - val_loss: 0.3387 - val_acc: 0.8804 - 623ms/epoch - 623ms/step\n",
            "Epoch 67/200\n",
            "1/1 - 1s - loss: 0.3360 - acc: 0.8888 - val_loss: 0.3316 - val_acc: 0.8804 - 1s/epoch - 1s/step\n",
            "Epoch 68/200\n",
            "1/1 - 1s - loss: 0.3327 - acc: 0.8930 - val_loss: 0.3247 - val_acc: 0.8804 - 1s/epoch - 1s/step\n",
            "Epoch 69/200\n",
            "1/1 - 1s - loss: 0.3358 - acc: 0.8902 - val_loss: 0.3179 - val_acc: 0.8804 - 947ms/epoch - 947ms/step\n",
            "Epoch 70/200\n",
            "1/1 - 1s - loss: 0.3059 - acc: 0.8936 - val_loss: 0.3113 - val_acc: 0.8804 - 1s/epoch - 1s/step\n",
            "Epoch 71/200\n",
            "1/1 - 1s - loss: 0.3199 - acc: 0.8916 - val_loss: 0.3048 - val_acc: 0.8837 - 1s/epoch - 1s/step\n",
            "Epoch 72/200\n",
            "1/1 - 1s - loss: 0.3083 - acc: 0.8905 - val_loss: 0.2984 - val_acc: 0.8837 - 1s/epoch - 1s/step\n",
            "Epoch 73/200\n",
            "1/1 - 1s - loss: 0.3115 - acc: 0.8894 - val_loss: 0.2921 - val_acc: 0.8837 - 521ms/epoch - 521ms/step\n",
            "Epoch 74/200\n",
            "1/1 - 1s - loss: 0.2889 - acc: 0.8914 - val_loss: 0.2860 - val_acc: 0.8837 - 526ms/epoch - 526ms/step\n",
            "Epoch 75/200\n",
            "1/1 - 1s - loss: 0.2901 - acc: 0.8939 - val_loss: 0.2799 - val_acc: 0.8837 - 614ms/epoch - 614ms/step\n",
            "Epoch 76/200\n",
            "1/1 - 1s - loss: 0.2804 - acc: 0.8919 - val_loss: 0.2739 - val_acc: 0.8837 - 616ms/epoch - 616ms/step\n",
            "Epoch 77/200\n",
            "1/1 - 1s - loss: 0.2809 - acc: 0.8961 - val_loss: 0.2679 - val_acc: 0.8837 - 532ms/epoch - 532ms/step\n",
            "Epoch 78/200\n",
            "1/1 - 1s - loss: 0.2759 - acc: 0.8947 - val_loss: 0.2621 - val_acc: 0.8837 - 526ms/epoch - 526ms/step\n",
            "Epoch 79/200\n",
            "1/1 - 1s - loss: 0.2632 - acc: 0.8945 - val_loss: 0.2563 - val_acc: 0.8837 - 516ms/epoch - 516ms/step\n",
            "Epoch 80/200\n",
            "1/1 - 1s - loss: 0.2527 - acc: 0.9004 - val_loss: 0.2506 - val_acc: 0.8837 - 511ms/epoch - 511ms/step\n",
            "Epoch 81/200\n",
            "1/1 - 1s - loss: 0.2511 - acc: 0.8947 - val_loss: 0.2452 - val_acc: 0.8837 - 607ms/epoch - 607ms/step\n",
            "Epoch 82/200\n",
            "1/1 - 1s - loss: 0.2516 - acc: 0.8956 - val_loss: 0.2400 - val_acc: 0.8837 - 514ms/epoch - 514ms/step\n",
            "Epoch 83/200\n",
            "1/1 - 1s - loss: 0.2443 - acc: 0.8990 - val_loss: 0.2348 - val_acc: 0.8837 - 521ms/epoch - 521ms/step\n",
            "Epoch 84/200\n",
            "1/1 - 1s - loss: 0.2298 - acc: 0.9263 - val_loss: 0.2300 - val_acc: 0.9468 - 611ms/epoch - 611ms/step\n",
            "Epoch 85/200\n",
            "1/1 - 1s - loss: 0.2396 - acc: 0.9305 - val_loss: 0.2253 - val_acc: 0.9767 - 523ms/epoch - 523ms/step\n",
            "Epoch 86/200\n",
            "1/1 - 1s - loss: 0.2362 - acc: 0.9333 - val_loss: 0.2208 - val_acc: 0.9767 - 613ms/epoch - 613ms/step\n",
            "Epoch 87/200\n",
            "1/1 - 1s - loss: 0.2253 - acc: 0.9310 - val_loss: 0.2164 - val_acc: 0.9767 - 612ms/epoch - 612ms/step\n",
            "Epoch 88/200\n",
            "1/1 - 1s - loss: 0.2257 - acc: 0.9702 - val_loss: 0.2122 - val_acc: 0.9767 - 536ms/epoch - 536ms/step\n",
            "Epoch 89/200\n",
            "1/1 - 1s - loss: 0.2148 - acc: 0.9713 - val_loss: 0.2080 - val_acc: 0.9767 - 552ms/epoch - 552ms/step\n",
            "Epoch 90/200\n",
            "1/1 - 1s - loss: 0.2138 - acc: 0.9713 - val_loss: 0.2040 - val_acc: 0.9767 - 913ms/epoch - 913ms/step\n",
            "Epoch 91/200\n",
            "1/1 - 1s - loss: 0.2178 - acc: 0.9710 - val_loss: 0.2001 - val_acc: 0.9767 - 939ms/epoch - 939ms/step\n",
            "Epoch 92/200\n",
            "1/1 - 1s - loss: 0.2046 - acc: 0.9747 - val_loss: 0.1962 - val_acc: 0.9767 - 1s/epoch - 1s/step\n",
            "Epoch 93/200\n",
            "1/1 - 1s - loss: 0.2010 - acc: 0.9755 - val_loss: 0.1925 - val_acc: 0.9767 - 1s/epoch - 1s/step\n",
            "Epoch 94/200\n",
            "1/1 - 1s - loss: 0.2020 - acc: 0.9750 - val_loss: 0.1888 - val_acc: 0.9767 - 783ms/epoch - 783ms/step\n",
            "Epoch 95/200\n",
            "1/1 - 1s - loss: 0.1847 - acc: 0.9752 - val_loss: 0.1853 - val_acc: 0.9767 - 732ms/epoch - 732ms/step\n",
            "Epoch 96/200\n",
            "1/1 - 1s - loss: 0.1849 - acc: 0.9750 - val_loss: 0.1819 - val_acc: 0.9767 - 518ms/epoch - 518ms/step\n",
            "Epoch 97/200\n",
            "1/1 - 1s - loss: 0.1784 - acc: 0.9747 - val_loss: 0.1787 - val_acc: 0.9767 - 515ms/epoch - 515ms/step\n",
            "Epoch 98/200\n",
            "1/1 - 1s - loss: 0.1813 - acc: 0.9747 - val_loss: 0.1755 - val_acc: 0.9767 - 533ms/epoch - 533ms/step\n",
            "Epoch 99/200\n",
            "1/1 - 1s - loss: 0.1862 - acc: 0.9752 - val_loss: 0.1725 - val_acc: 0.9767 - 614ms/epoch - 614ms/step\n",
            "Epoch 100/200\n",
            "1/1 - 1s - loss: 0.1731 - acc: 0.9764 - val_loss: 0.1697 - val_acc: 0.9767 - 529ms/epoch - 529ms/step\n",
            "Epoch 101/200\n",
            "1/1 - 1s - loss: 0.1719 - acc: 0.9741 - val_loss: 0.1669 - val_acc: 0.9767 - 518ms/epoch - 518ms/step\n",
            "Epoch 102/200\n",
            "1/1 - 1s - loss: 0.1754 - acc: 0.9750 - val_loss: 0.1642 - val_acc: 0.9767 - 516ms/epoch - 516ms/step\n",
            "Epoch 103/200\n",
            "1/1 - 1s - loss: 0.1715 - acc: 0.9750 - val_loss: 0.1616 - val_acc: 0.9767 - 509ms/epoch - 509ms/step\n",
            "Epoch 104/200\n",
            "1/1 - 1s - loss: 0.1666 - acc: 0.9747 - val_loss: 0.1591 - val_acc: 0.9767 - 519ms/epoch - 519ms/step\n",
            "Epoch 105/200\n",
            "1/1 - 1s - loss: 0.1632 - acc: 0.9755 - val_loss: 0.1567 - val_acc: 0.9767 - 613ms/epoch - 613ms/step\n",
            "Epoch 106/200\n",
            "1/1 - 1s - loss: 0.1596 - acc: 0.9752 - val_loss: 0.1543 - val_acc: 0.9767 - 627ms/epoch - 627ms/step\n",
            "Epoch 107/200\n",
            "1/1 - 1s - loss: 0.1703 - acc: 0.9741 - val_loss: 0.1520 - val_acc: 0.9767 - 515ms/epoch - 515ms/step\n",
            "Epoch 108/200\n",
            "1/1 - 1s - loss: 0.1571 - acc: 0.9744 - val_loss: 0.1499 - val_acc: 0.9767 - 626ms/epoch - 626ms/step\n",
            "Epoch 109/200\n",
            "1/1 - 1s - loss: 0.1533 - acc: 0.9752 - val_loss: 0.1478 - val_acc: 0.9767 - 553ms/epoch - 553ms/step\n",
            "Epoch 110/200\n",
            "1/1 - 1s - loss: 0.1580 - acc: 0.9735 - val_loss: 0.1458 - val_acc: 0.9767 - 940ms/epoch - 940ms/step\n",
            "Epoch 111/200\n",
            "1/1 - 1s - loss: 0.1514 - acc: 0.9755 - val_loss: 0.1439 - val_acc: 0.9767 - 1s/epoch - 1s/step\n",
            "Epoch 112/200\n",
            "1/1 - 1s - loss: 0.1512 - acc: 0.9764 - val_loss: 0.1421 - val_acc: 0.9767 - 1s/epoch - 1s/step\n",
            "Epoch 113/200\n",
            "1/1 - 1s - loss: 0.1456 - acc: 0.9741 - val_loss: 0.1403 - val_acc: 0.9767 - 768ms/epoch - 768ms/step\n",
            "Epoch 114/200\n",
            "1/1 - 1s - loss: 0.1487 - acc: 0.9735 - val_loss: 0.1386 - val_acc: 0.9767 - 1s/epoch - 1s/step\n",
            "Epoch 115/200\n",
            "1/1 - 1s - loss: 0.1488 - acc: 0.9730 - val_loss: 0.1370 - val_acc: 0.9767 - 795ms/epoch - 795ms/step\n",
            "Epoch 116/200\n",
            "1/1 - 1s - loss: 0.1400 - acc: 0.9752 - val_loss: 0.1355 - val_acc: 0.9767 - 516ms/epoch - 516ms/step\n",
            "Epoch 117/200\n",
            "1/1 - 1s - loss: 0.1386 - acc: 0.9738 - val_loss: 0.1340 - val_acc: 0.9767 - 613ms/epoch - 613ms/step\n",
            "Epoch 118/200\n",
            "1/1 - 1s - loss: 0.1445 - acc: 0.9741 - val_loss: 0.1325 - val_acc: 0.9767 - 618ms/epoch - 618ms/step\n",
            "Epoch 119/200\n",
            "1/1 - 1s - loss: 0.1333 - acc: 0.9750 - val_loss: 0.1311 - val_acc: 0.9767 - 604ms/epoch - 604ms/step\n",
            "Epoch 120/200\n",
            "1/1 - 1s - loss: 0.1369 - acc: 0.9747 - val_loss: 0.1298 - val_acc: 0.9767 - 513ms/epoch - 513ms/step\n",
            "Epoch 121/200\n",
            "1/1 - 1s - loss: 0.1393 - acc: 0.9747 - val_loss: 0.1285 - val_acc: 0.9767 - 515ms/epoch - 515ms/step\n",
            "Epoch 122/200\n",
            "1/1 - 1s - loss: 0.1422 - acc: 0.9741 - val_loss: 0.1273 - val_acc: 0.9767 - 521ms/epoch - 521ms/step\n",
            "Epoch 123/200\n",
            "1/1 - 1s - loss: 0.1350 - acc: 0.9755 - val_loss: 0.1261 - val_acc: 0.9767 - 617ms/epoch - 617ms/step\n",
            "Epoch 124/200\n",
            "1/1 - 1s - loss: 0.1376 - acc: 0.9750 - val_loss: 0.1250 - val_acc: 0.9767 - 609ms/epoch - 609ms/step\n",
            "Epoch 125/200\n",
            "1/1 - 1s - loss: 0.1360 - acc: 0.9738 - val_loss: 0.1239 - val_acc: 0.9767 - 520ms/epoch - 520ms/step\n",
            "Epoch 126/200\n",
            "1/1 - 1s - loss: 0.1360 - acc: 0.9738 - val_loss: 0.1228 - val_acc: 0.9767 - 517ms/epoch - 517ms/step\n",
            "Epoch 127/200\n",
            "1/1 - 1s - loss: 0.1295 - acc: 0.9730 - val_loss: 0.1218 - val_acc: 0.9767 - 615ms/epoch - 615ms/step\n",
            "Epoch 128/200\n",
            "1/1 - 1s - loss: 0.1294 - acc: 0.9752 - val_loss: 0.1208 - val_acc: 0.9767 - 510ms/epoch - 510ms/step\n",
            "Epoch 129/200\n",
            "1/1 - 1s - loss: 0.1291 - acc: 0.9744 - val_loss: 0.1198 - val_acc: 0.9767 - 643ms/epoch - 643ms/step\n",
            "Epoch 130/200\n",
            "1/1 - 1s - loss: 0.1222 - acc: 0.9744 - val_loss: 0.1189 - val_acc: 0.9767 - 1s/epoch - 1s/step\n",
            "Epoch 131/200\n",
            "1/1 - 1s - loss: 0.1192 - acc: 0.9752 - val_loss: 0.1180 - val_acc: 0.9767 - 1s/epoch - 1s/step\n",
            "Epoch 132/200\n",
            "1/1 - 1s - loss: 0.1207 - acc: 0.9741 - val_loss: 0.1172 - val_acc: 0.9767 - 791ms/epoch - 791ms/step\n",
            "Epoch 133/200\n",
            "1/1 - 1s - loss: 0.1276 - acc: 0.9747 - val_loss: 0.1164 - val_acc: 0.9767 - 1s/epoch - 1s/step\n",
            "Epoch 134/200\n",
            "1/1 - 1s - loss: 0.1282 - acc: 0.9750 - val_loss: 0.1156 - val_acc: 0.9767 - 763ms/epoch - 763ms/step\n",
            "Epoch 135/200\n",
            "1/1 - 1s - loss: 0.1253 - acc: 0.9741 - val_loss: 0.1148 - val_acc: 0.9767 - 532ms/epoch - 532ms/step\n",
            "1/1 [==============================] - 0s 258ms/step - loss: 0.2229 - acc: 0.9724\n",
            "\n",
            "Test Set Metrics:\n",
            "\tloss: 0.2229\n",
            "\tacc: 0.9724\n",
            "1/1 [==============================] - 0s 398ms/step\n",
            "Using GCN (local pooling) filters...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1/1 - 2s - loss: 1.6770 - acc: 0.0059 - val_loss: 1.6658 - val_acc: 0.0100 - 2s/epoch - 2s/step\n",
            "Epoch 2/200\n",
            "1/1 - 1s - loss: 1.6568 - acc: 0.0025 - val_loss: 1.6490 - val_acc: 0.0100 - 526ms/epoch - 526ms/step\n",
            "Epoch 3/200\n",
            "1/1 - 1s - loss: 1.6466 - acc: 0.0051 - val_loss: 1.6324 - val_acc: 0.0100 - 518ms/epoch - 518ms/step\n",
            "Epoch 4/200\n",
            "1/1 - 1s - loss: 1.6324 - acc: 0.0039 - val_loss: 1.6158 - val_acc: 0.0100 - 617ms/epoch - 617ms/step\n",
            "Epoch 5/200\n",
            "1/1 - 1s - loss: 1.6176 - acc: 0.0037 - val_loss: 1.5993 - val_acc: 0.0133 - 513ms/epoch - 513ms/step\n",
            "Epoch 6/200\n",
            "1/1 - 1s - loss: 1.5981 - acc: 0.0121 - val_loss: 1.5830 - val_acc: 0.0133 - 530ms/epoch - 530ms/step\n",
            "Epoch 7/200\n",
            "1/1 - 1s - loss: 1.5835 - acc: 0.0070 - val_loss: 1.5667 - val_acc: 0.0133 - 518ms/epoch - 518ms/step\n",
            "Epoch 8/200\n",
            "1/1 - 1s - loss: 1.5670 - acc: 0.0287 - val_loss: 1.5506 - val_acc: 0.0166 - 619ms/epoch - 619ms/step\n",
            "Epoch 9/200\n",
            "1/1 - 1s - loss: 1.5553 - acc: 0.0191 - val_loss: 1.5342 - val_acc: 0.0266 - 620ms/epoch - 620ms/step\n",
            "Epoch 10/200\n",
            "1/1 - 1s - loss: 1.5441 - acc: 0.0220 - val_loss: 1.5179 - val_acc: 0.0299 - 532ms/epoch - 532ms/step\n",
            "Epoch 11/200\n",
            "1/1 - 1s - loss: 1.5235 - acc: 0.0191 - val_loss: 1.5016 - val_acc: 0.0299 - 617ms/epoch - 617ms/step\n",
            "Epoch 12/200\n",
            "1/1 - 1s - loss: 1.4904 - acc: 0.1486 - val_loss: 1.4854 - val_acc: 0.0299 - 604ms/epoch - 604ms/step\n",
            "Epoch 13/200\n",
            "1/1 - 1s - loss: 1.4786 - acc: 0.0341 - val_loss: 1.4693 - val_acc: 0.5150 - 1s/epoch - 1s/step\n",
            "Epoch 14/200\n",
            "1/1 - 1s - loss: 1.4723 - acc: 0.5888 - val_loss: 1.4531 - val_acc: 0.5249 - 948ms/epoch - 948ms/step\n",
            "Epoch 15/200\n",
            "1/1 - 1s - loss: 1.4532 - acc: 0.5246 - val_loss: 1.4369 - val_acc: 0.5482 - 940ms/epoch - 940ms/step\n",
            "Epoch 16/200\n",
            "1/1 - 1s - loss: 1.4372 - acc: 0.6631 - val_loss: 1.4208 - val_acc: 0.6279 - 1s/epoch - 1s/step\n",
            "Epoch 17/200\n",
            "1/1 - 1s - loss: 1.4243 - acc: 0.8190 - val_loss: 1.4046 - val_acc: 0.8904 - 783ms/epoch - 783ms/step\n",
            "Epoch 18/200\n",
            "1/1 - 1s - loss: 1.3990 - acc: 0.7605 - val_loss: 1.3884 - val_acc: 0.9037 - 771ms/epoch - 771ms/step\n",
            "Epoch 19/200\n",
            "1/1 - 1s - loss: 1.3844 - acc: 0.8694 - val_loss: 1.3722 - val_acc: 0.9037 - 523ms/epoch - 523ms/step\n",
            "Epoch 20/200\n",
            "1/1 - 1s - loss: 1.3667 - acc: 0.8767 - val_loss: 1.3559 - val_acc: 0.9037 - 519ms/epoch - 519ms/step\n",
            "Epoch 21/200\n",
            "1/1 - 1s - loss: 1.3597 - acc: 0.8705 - val_loss: 1.3396 - val_acc: 0.9037 - 616ms/epoch - 616ms/step\n",
            "Epoch 22/200\n",
            "1/1 - 1s - loss: 1.3428 - acc: 0.8784 - val_loss: 1.3234 - val_acc: 0.9834 - 613ms/epoch - 613ms/step\n",
            "Epoch 23/200\n",
            "1/1 - 1s - loss: 1.3263 - acc: 0.8849 - val_loss: 1.3071 - val_acc: 0.9834 - 615ms/epoch - 615ms/step\n",
            "Epoch 24/200\n",
            "1/1 - 1s - loss: 1.3144 - acc: 0.9178 - val_loss: 1.2907 - val_acc: 0.9834 - 617ms/epoch - 617ms/step\n",
            "Epoch 25/200\n",
            "1/1 - 1s - loss: 1.2966 - acc: 0.9671 - val_loss: 1.2748 - val_acc: 0.9834 - 619ms/epoch - 619ms/step\n",
            "Epoch 26/200\n",
            "1/1 - 1s - loss: 1.2775 - acc: 0.9645 - val_loss: 1.2592 - val_acc: 0.9834 - 608ms/epoch - 608ms/step\n",
            "Epoch 27/200\n",
            "1/1 - 1s - loss: 1.2600 - acc: 0.9730 - val_loss: 1.2438 - val_acc: 0.9834 - 617ms/epoch - 617ms/step\n",
            "Epoch 28/200\n",
            "1/1 - 1s - loss: 1.2541 - acc: 0.9696 - val_loss: 1.2283 - val_acc: 0.9834 - 608ms/epoch - 608ms/step\n",
            "Epoch 29/200\n",
            "1/1 - 1s - loss: 1.2294 - acc: 0.9699 - val_loss: 1.2128 - val_acc: 0.9834 - 526ms/epoch - 526ms/step\n",
            "Epoch 30/200\n",
            "1/1 - 1s - loss: 1.2165 - acc: 0.9719 - val_loss: 1.1971 - val_acc: 0.9834 - 508ms/epoch - 508ms/step\n",
            "Epoch 31/200\n",
            "1/1 - 1s - loss: 1.1966 - acc: 0.9721 - val_loss: 1.1814 - val_acc: 0.9834 - 583ms/epoch - 583ms/step\n",
            "Epoch 32/200\n",
            "1/1 - 1s - loss: 1.1862 - acc: 0.9668 - val_loss: 1.1656 - val_acc: 0.9834 - 913ms/epoch - 913ms/step\n",
            "Epoch 33/200\n",
            "1/1 - 1s - loss: 1.1645 - acc: 0.9724 - val_loss: 1.1496 - val_acc: 0.9834 - 952ms/epoch - 952ms/step\n",
            "Epoch 34/200\n",
            "1/1 - 1s - loss: 1.1576 - acc: 0.9713 - val_loss: 1.1336 - val_acc: 0.9834 - 945ms/epoch - 945ms/step\n",
            "Epoch 35/200\n",
            "1/1 - 1s - loss: 1.1374 - acc: 0.9727 - val_loss: 1.1176 - val_acc: 0.9834 - 868ms/epoch - 868ms/step\n",
            "Epoch 36/200\n",
            "1/1 - 1s - loss: 1.1283 - acc: 0.9699 - val_loss: 1.1016 - val_acc: 0.9834 - 1s/epoch - 1s/step\n",
            "Epoch 37/200\n",
            "1/1 - 1s - loss: 1.1015 - acc: 0.9685 - val_loss: 1.0854 - val_acc: 0.9834 - 788ms/epoch - 788ms/step\n",
            "Epoch 38/200\n",
            "1/1 - 1s - loss: 1.0921 - acc: 0.9733 - val_loss: 1.0692 - val_acc: 0.9834 - 768ms/epoch - 768ms/step\n",
            "Epoch 39/200\n",
            "1/1 - 1s - loss: 1.0731 - acc: 0.9724 - val_loss: 1.0528 - val_acc: 0.9834 - 522ms/epoch - 522ms/step\n",
            "Epoch 40/200\n",
            "1/1 - 1s - loss: 1.0651 - acc: 0.9696 - val_loss: 1.0363 - val_acc: 0.9834 - 515ms/epoch - 515ms/step\n",
            "Epoch 41/200\n",
            "1/1 - 1s - loss: 1.0515 - acc: 0.9704 - val_loss: 1.0196 - val_acc: 0.9834 - 624ms/epoch - 624ms/step\n",
            "Epoch 42/200\n",
            "1/1 - 1s - loss: 1.0219 - acc: 0.9730 - val_loss: 1.0028 - val_acc: 0.9834 - 607ms/epoch - 607ms/step\n",
            "Epoch 43/200\n",
            "1/1 - 1s - loss: 1.0022 - acc: 0.9735 - val_loss: 0.9858 - val_acc: 0.9834 - 527ms/epoch - 527ms/step\n",
            "Epoch 44/200\n",
            "1/1 - 1s - loss: 0.9931 - acc: 0.9738 - val_loss: 0.9687 - val_acc: 0.9834 - 630ms/epoch - 630ms/step\n",
            "Epoch 45/200\n",
            "1/1 - 1s - loss: 0.9726 - acc: 0.9735 - val_loss: 0.9514 - val_acc: 0.9834 - 515ms/epoch - 515ms/step\n",
            "Epoch 46/200\n",
            "1/1 - 1s - loss: 0.9643 - acc: 0.9710 - val_loss: 0.9339 - val_acc: 0.9834 - 513ms/epoch - 513ms/step\n",
            "Epoch 47/200\n",
            "1/1 - 1s - loss: 0.9458 - acc: 0.9716 - val_loss: 0.9162 - val_acc: 0.9834 - 612ms/epoch - 612ms/step\n",
            "Epoch 48/200\n",
            "1/1 - 1s - loss: 0.9305 - acc: 0.9735 - val_loss: 0.8978 - val_acc: 0.9834 - 515ms/epoch - 515ms/step\n",
            "Epoch 49/200\n",
            "1/1 - 1s - loss: 0.9055 - acc: 0.9735 - val_loss: 0.8793 - val_acc: 0.9834 - 518ms/epoch - 518ms/step\n",
            "Epoch 50/200\n",
            "1/1 - 1s - loss: 0.8941 - acc: 0.9735 - val_loss: 0.8606 - val_acc: 0.9834 - 613ms/epoch - 613ms/step\n",
            "Epoch 51/200\n",
            "1/1 - 1s - loss: 0.8825 - acc: 0.9738 - val_loss: 0.8418 - val_acc: 0.9834 - 626ms/epoch - 626ms/step\n",
            "Epoch 52/200\n",
            "1/1 - 1s - loss: 0.8566 - acc: 0.9735 - val_loss: 0.8222 - val_acc: 0.9834 - 518ms/epoch - 518ms/step\n",
            "Epoch 53/200\n",
            "1/1 - 1s - loss: 0.8342 - acc: 0.9738 - val_loss: 0.8025 - val_acc: 0.9834 - 527ms/epoch - 527ms/step\n",
            "Epoch 54/200\n",
            "1/1 - 1s - loss: 0.8080 - acc: 0.9730 - val_loss: 0.7827 - val_acc: 0.9834 - 746ms/epoch - 746ms/step\n",
            "Epoch 55/200\n",
            "1/1 - 1s - loss: 0.7947 - acc: 0.9733 - val_loss: 0.7629 - val_acc: 0.9834 - 943ms/epoch - 943ms/step\n",
            "Epoch 56/200\n",
            "1/1 - 1s - loss: 0.7722 - acc: 0.9716 - val_loss: 0.7432 - val_acc: 0.9834 - 939ms/epoch - 939ms/step\n",
            "Epoch 57/200\n",
            "1/1 - 1s - loss: 0.7575 - acc: 0.9733 - val_loss: 0.7236 - val_acc: 0.9834 - 1s/epoch - 1s/step\n",
            "Epoch 58/200\n",
            "1/1 - 1s - loss: 0.7382 - acc: 0.9730 - val_loss: 0.7039 - val_acc: 0.9834 - 1s/epoch - 1s/step\n",
            "Epoch 59/200\n",
            "1/1 - 1s - loss: 0.7140 - acc: 0.9735 - val_loss: 0.6843 - val_acc: 0.9834 - 798ms/epoch - 798ms/step\n",
            "Epoch 60/200\n",
            "1/1 - 1s - loss: 0.7008 - acc: 0.9735 - val_loss: 0.6647 - val_acc: 0.9834 - 518ms/epoch - 518ms/step\n",
            "Epoch 61/200\n",
            "1/1 - 1s - loss: 0.6865 - acc: 0.9733 - val_loss: 0.6450 - val_acc: 0.9834 - 618ms/epoch - 618ms/step\n",
            "Epoch 62/200\n",
            "1/1 - 1s - loss: 0.6580 - acc: 0.9735 - val_loss: 0.6254 - val_acc: 0.9834 - 514ms/epoch - 514ms/step\n",
            "Epoch 63/200\n",
            "1/1 - 1s - loss: 0.6328 - acc: 0.9735 - val_loss: 0.6061 - val_acc: 0.9834 - 516ms/epoch - 516ms/step\n",
            "Epoch 64/200\n",
            "1/1 - 1s - loss: 0.6060 - acc: 0.9735 - val_loss: 0.5868 - val_acc: 0.9834 - 522ms/epoch - 522ms/step\n",
            "Epoch 65/200\n",
            "1/1 - 1s - loss: 0.6032 - acc: 0.9733 - val_loss: 0.5677 - val_acc: 0.9834 - 532ms/epoch - 532ms/step\n",
            "Epoch 66/200\n",
            "1/1 - 1s - loss: 0.5886 - acc: 0.9735 - val_loss: 0.5488 - val_acc: 0.9834 - 614ms/epoch - 614ms/step\n",
            "Epoch 67/200\n",
            "1/1 - 1s - loss: 0.5697 - acc: 0.9721 - val_loss: 0.5303 - val_acc: 0.9834 - 521ms/epoch - 521ms/step\n",
            "Epoch 68/200\n",
            "1/1 - 1s - loss: 0.5366 - acc: 0.9735 - val_loss: 0.5121 - val_acc: 0.9834 - 612ms/epoch - 612ms/step\n",
            "Epoch 69/200\n",
            "1/1 - 1s - loss: 0.5252 - acc: 0.9741 - val_loss: 0.4943 - val_acc: 0.9834 - 609ms/epoch - 609ms/step\n",
            "Epoch 70/200\n",
            "1/1 - 1s - loss: 0.5172 - acc: 0.9741 - val_loss: 0.4769 - val_acc: 0.9834 - 610ms/epoch - 610ms/step\n",
            "Epoch 71/200\n",
            "1/1 - 1s - loss: 0.4937 - acc: 0.9738 - val_loss: 0.4599 - val_acc: 0.9834 - 524ms/epoch - 524ms/step\n",
            "Epoch 72/200\n",
            "1/1 - 1s - loss: 0.4835 - acc: 0.9741 - val_loss: 0.4434 - val_acc: 0.9834 - 516ms/epoch - 516ms/step\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 1.3252 - acc: 0.9807\n",
            "\n",
            "Test Set Metrics:\n",
            "\tloss: 1.3252\n",
            "\tacc: 0.9807\n",
            "1/1 [==============================] - 1s 573ms/step\n",
            "Using GCN (local pooling) filters...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1/1 - 2s - loss: 1.7724 - acc: 8.4436e-04 - val_loss: 1.7556 - val_acc: 0.0000e+00 - 2s/epoch - 2s/step\n",
            "Epoch 2/200\n",
            "1/1 - 1s - loss: 1.7592 - acc: 8.4436e-04 - val_loss: 1.7416 - val_acc: 0.0000e+00 - 608ms/epoch - 608ms/step\n",
            "Epoch 3/200\n",
            "1/1 - 1s - loss: 1.7378 - acc: 0.0017 - val_loss: 1.7283 - val_acc: 0.0000e+00 - 516ms/epoch - 516ms/step\n",
            "Epoch 4/200\n",
            "1/1 - 1s - loss: 1.7289 - acc: 0.0011 - val_loss: 1.7158 - val_acc: 0.0000e+00 - 619ms/epoch - 619ms/step\n",
            "Epoch 5/200\n",
            "1/1 - 1s - loss: 1.7216 - acc: 8.4436e-04 - val_loss: 1.7034 - val_acc: 0.0000e+00 - 913ms/epoch - 913ms/step\n",
            "Epoch 6/200\n",
            "1/1 - 1s - loss: 1.7102 - acc: 8.4436e-04 - val_loss: 1.6913 - val_acc: 0.0000e+00 - 1s/epoch - 1s/step\n",
            "Epoch 7/200\n",
            "1/1 - 1s - loss: 1.6884 - acc: 8.4436e-04 - val_loss: 1.6793 - val_acc: 0.0000e+00 - 936ms/epoch - 936ms/step\n",
            "Epoch 8/200\n",
            "1/1 - 1s - loss: 1.6821 - acc: 8.4436e-04 - val_loss: 1.6674 - val_acc: 0.0000e+00 - 1s/epoch - 1s/step\n",
            "Epoch 9/200\n",
            "1/1 - 1s - loss: 1.6672 - acc: 0.0011 - val_loss: 1.6556 - val_acc: 0.0000e+00 - 1s/epoch - 1s/step\n",
            "Epoch 10/200\n",
            "1/1 - 1s - loss: 1.6516 - acc: 8.4436e-04 - val_loss: 1.6439 - val_acc: 0.0000e+00 - 653ms/epoch - 653ms/step\n",
            "Epoch 11/200\n",
            "1/1 - 1s - loss: 1.6411 - acc: 8.4436e-04 - val_loss: 1.6324 - val_acc: 0.0000e+00 - 527ms/epoch - 527ms/step\n",
            "Epoch 12/200\n",
            "1/1 - 1s - loss: 1.6270 - acc: 0.0014 - val_loss: 1.6210 - val_acc: 0.0000e+00 - 515ms/epoch - 515ms/step\n",
            "Epoch 13/200\n",
            "1/1 - 1s - loss: 1.6240 - acc: 0.0011 - val_loss: 1.6104 - val_acc: 0.0000e+00 - 518ms/epoch - 518ms/step\n",
            "Epoch 14/200\n",
            "1/1 - 1s - loss: 1.6124 - acc: 8.4436e-04 - val_loss: 1.6003 - val_acc: 0.0000e+00 - 518ms/epoch - 518ms/step\n",
            "Epoch 15/200\n",
            "1/1 - 1s - loss: 1.6017 - acc: 8.4436e-04 - val_loss: 1.5904 - val_acc: 0.0000e+00 - 521ms/epoch - 521ms/step\n",
            "Epoch 16/200\n",
            "1/1 - 1s - loss: 1.5930 - acc: 8.4436e-04 - val_loss: 1.5809 - val_acc: 0.0000e+00 - 526ms/epoch - 526ms/step\n",
            "Epoch 17/200\n",
            "1/1 - 1s - loss: 1.5787 - acc: 0.0014 - val_loss: 1.5714 - val_acc: 0.0000e+00 - 602ms/epoch - 602ms/step\n",
            "Epoch 18/200\n",
            "1/1 - 1s - loss: 1.5706 - acc: 8.4436e-04 - val_loss: 1.5608 - val_acc: 0.0000e+00 - 612ms/epoch - 612ms/step\n",
            "Epoch 19/200\n",
            "1/1 - 1s - loss: 1.5573 - acc: 0.0014 - val_loss: 1.5494 - val_acc: 0.0000e+00 - 511ms/epoch - 511ms/step\n",
            "Epoch 20/200\n",
            "1/1 - 1s - loss: 1.5496 - acc: 8.4436e-04 - val_loss: 1.5380 - val_acc: 0.0000e+00 - 612ms/epoch - 612ms/step\n",
            "Epoch 21/200\n",
            "1/1 - 1s - loss: 1.5335 - acc: 0.0017 - val_loss: 1.5265 - val_acc: 0.0000e+00 - 526ms/epoch - 526ms/step\n",
            "Epoch 22/200\n",
            "1/1 - 1s - loss: 1.5321 - acc: 8.4436e-04 - val_loss: 1.5149 - val_acc: 0.0000e+00 - 511ms/epoch - 511ms/step\n",
            "Epoch 23/200\n",
            "1/1 - 1s - loss: 1.5158 - acc: 8.4436e-04 - val_loss: 1.5034 - val_acc: 0.0000e+00 - 523ms/epoch - 523ms/step\n",
            "Epoch 24/200\n",
            "1/1 - 1s - loss: 1.4968 - acc: 0.0017 - val_loss: 1.4914 - val_acc: 0.0000e+00 - 508ms/epoch - 508ms/step\n",
            "Epoch 25/200\n",
            "1/1 - 1s - loss: 1.4954 - acc: 8.4436e-04 - val_loss: 1.4790 - val_acc: 0.0000e+00 - 617ms/epoch - 617ms/step\n",
            "Epoch 26/200\n",
            "1/1 - 1s - loss: 1.4819 - acc: 8.4436e-04 - val_loss: 1.4666 - val_acc: 0.0000e+00 - 1s/epoch - 1s/step\n",
            "Epoch 27/200\n",
            "1/1 - 1s - loss: 1.4668 - acc: 0.0037 - val_loss: 1.4541 - val_acc: 0.0000e+00 - 934ms/epoch - 934ms/step\n",
            "Epoch 28/200\n",
            "1/1 - 1s - loss: 1.4558 - acc: 8.4436e-04 - val_loss: 1.4416 - val_acc: 0.0000e+00 - 1s/epoch - 1s/step\n",
            "Epoch 29/200\n",
            "1/1 - 1s - loss: 1.4377 - acc: 8.4436e-04 - val_loss: 1.4291 - val_acc: 0.0000e+00 - 1s/epoch - 1s/step\n",
            "Epoch 30/200\n",
            "1/1 - 1s - loss: 1.4297 - acc: 0.0023 - val_loss: 1.4166 - val_acc: 0.0000e+00 - 782ms/epoch - 782ms/step\n",
            "Epoch 31/200\n",
            "1/1 - 1s - loss: 1.4170 - acc: 0.0039 - val_loss: 1.4041 - val_acc: 0.0000e+00 - 1s/epoch - 1s/step\n",
            "Epoch 32/200\n",
            "1/1 - 1s - loss: 1.4059 - acc: 0.0045 - val_loss: 1.3915 - val_acc: 0.0000e+00 - 538ms/epoch - 538ms/step\n",
            "Epoch 33/200\n",
            "1/1 - 1s - loss: 1.3982 - acc: 0.0070 - val_loss: 1.3790 - val_acc: 0.0000e+00 - 520ms/epoch - 520ms/step\n",
            "Epoch 34/200\n",
            "1/1 - 1s - loss: 1.3784 - acc: 0.0084 - val_loss: 1.3664 - val_acc: 0.0000e+00 - 613ms/epoch - 613ms/step\n",
            "Epoch 35/200\n",
            "1/1 - 1s - loss: 1.3730 - acc: 0.0034 - val_loss: 1.3538 - val_acc: 0.0000e+00 - 514ms/epoch - 514ms/step\n",
            "Epoch 36/200\n",
            "1/1 - 1s - loss: 1.3515 - acc: 0.0118 - val_loss: 1.3411 - val_acc: 0.0000e+00 - 624ms/epoch - 624ms/step\n",
            "Epoch 37/200\n",
            "1/1 - 1s - loss: 1.3467 - acc: 0.0146 - val_loss: 1.3283 - val_acc: 0.0000e+00 - 517ms/epoch - 517ms/step\n",
            "Epoch 38/200\n",
            "1/1 - 1s - loss: 1.3230 - acc: 0.0335 - val_loss: 1.3156 - val_acc: 0.0066 - 521ms/epoch - 521ms/step\n",
            "Epoch 39/200\n",
            "1/1 - 1s - loss: 1.3185 - acc: 0.0287 - val_loss: 1.3028 - val_acc: 0.5581 - 619ms/epoch - 619ms/step\n",
            "Epoch 40/200\n",
            "1/1 - 1s - loss: 1.3045 - acc: 0.4002 - val_loss: 1.2901 - val_acc: 0.5615 - 526ms/epoch - 526ms/step\n",
            "Epoch 41/200\n",
            "1/1 - 1s - loss: 1.2983 - acc: 0.6279 - val_loss: 1.2775 - val_acc: 0.5648 - 509ms/epoch - 509ms/step\n",
            "Epoch 42/200\n",
            "1/1 - 1s - loss: 1.2901 - acc: 0.5550 - val_loss: 1.2649 - val_acc: 0.6545 - 513ms/epoch - 513ms/step\n",
            "Epoch 43/200\n",
            "1/1 - 1s - loss: 1.2644 - acc: 0.6200 - val_loss: 1.2523 - val_acc: 0.6611 - 527ms/epoch - 527ms/step\n",
            "Epoch 44/200\n",
            "1/1 - 1s - loss: 1.2618 - acc: 0.6274 - val_loss: 1.2397 - val_acc: 0.7442 - 516ms/epoch - 516ms/step\n",
            "Epoch 45/200\n",
            "1/1 - 1s - loss: 1.2452 - acc: 0.6741 - val_loss: 1.2271 - val_acc: 0.7475 - 526ms/epoch - 526ms/step\n",
            "Epoch 46/200\n",
            "1/1 - 1s - loss: 1.2280 - acc: 0.6789 - val_loss: 1.2145 - val_acc: 0.7542 - 611ms/epoch - 611ms/step\n",
            "Epoch 47/200\n",
            "1/1 - 1s - loss: 1.2141 - acc: 0.7095 - val_loss: 1.2019 - val_acc: 0.7542 - 514ms/epoch - 514ms/step\n",
            "Epoch 48/200\n",
            "1/1 - 1s - loss: 1.1964 - acc: 0.7284 - val_loss: 1.1893 - val_acc: 0.7674 - 609ms/epoch - 609ms/step\n",
            "Epoch 49/200\n",
            "1/1 - 1s - loss: 1.1850 - acc: 0.7608 - val_loss: 1.1767 - val_acc: 0.8771 - 964ms/epoch - 964ms/step\n",
            "Epoch 50/200\n",
            "1/1 - 1s - loss: 1.1778 - acc: 0.8728 - val_loss: 1.1640 - val_acc: 0.8771 - 1s/epoch - 1s/step\n",
            "Epoch 51/200\n",
            "1/1 - 1s - loss: 1.1670 - acc: 0.8728 - val_loss: 1.1513 - val_acc: 0.8771 - 838ms/epoch - 838ms/step\n",
            "Epoch 52/200\n",
            "1/1 - 1s - loss: 1.1528 - acc: 0.8728 - val_loss: 1.1387 - val_acc: 0.8771 - 769ms/epoch - 769ms/step\n",
            "Epoch 53/200\n",
            "1/1 - 1s - loss: 1.1411 - acc: 0.8717 - val_loss: 1.1259 - val_acc: 0.8771 - 769ms/epoch - 769ms/step\n",
            "Epoch 54/200\n",
            "1/1 - 1s - loss: 1.1273 - acc: 0.8838 - val_loss: 1.1131 - val_acc: 0.8771 - 560ms/epoch - 560ms/step\n",
            "Epoch 55/200\n",
            "1/1 - 1s - loss: 1.1147 - acc: 0.8852 - val_loss: 1.1004 - val_acc: 0.8771 - 609ms/epoch - 609ms/step\n",
            "Epoch 56/200\n",
            "1/1 - 1s - loss: 1.1066 - acc: 0.8818 - val_loss: 1.0876 - val_acc: 0.8771 - 515ms/epoch - 515ms/step\n",
            "Epoch 57/200\n",
            "1/1 - 1s - loss: 1.0935 - acc: 0.8742 - val_loss: 1.0748 - val_acc: 0.8771 - 627ms/epoch - 627ms/step\n",
            "Epoch 58/200\n",
            "1/1 - 1s - loss: 1.0771 - acc: 0.8753 - val_loss: 1.0620 - val_acc: 0.8771 - 611ms/epoch - 611ms/step\n",
            "Epoch 59/200\n",
            "1/1 - 1s - loss: 1.0637 - acc: 0.8778 - val_loss: 1.0492 - val_acc: 0.8771 - 509ms/epoch - 509ms/step\n",
            "Epoch 60/200\n",
            "1/1 - 1s - loss: 1.0456 - acc: 0.8770 - val_loss: 1.0363 - val_acc: 0.8771 - 519ms/epoch - 519ms/step\n",
            "Epoch 61/200\n",
            "1/1 - 1s - loss: 1.0414 - acc: 0.8733 - val_loss: 1.0236 - val_acc: 0.8771 - 535ms/epoch - 535ms/step\n",
            "Epoch 62/200\n",
            "1/1 - 1s - loss: 1.0259 - acc: 0.8854 - val_loss: 1.0111 - val_acc: 0.8771 - 519ms/epoch - 519ms/step\n",
            "Epoch 63/200\n",
            "1/1 - 1s - loss: 1.0262 - acc: 0.8773 - val_loss: 0.9988 - val_acc: 0.8771 - 507ms/epoch - 507ms/step\n",
            "Epoch 64/200\n",
            "1/1 - 1s - loss: 0.9999 - acc: 0.8849 - val_loss: 0.9865 - val_acc: 0.8771 - 622ms/epoch - 622ms/step\n",
            "Epoch 65/200\n",
            "1/1 - 1s - loss: 0.9882 - acc: 0.8840 - val_loss: 0.9745 - val_acc: 0.8771 - 654ms/epoch - 654ms/step\n",
            "Epoch 66/200\n",
            "1/1 - 1s - loss: 0.9771 - acc: 0.8852 - val_loss: 0.9626 - val_acc: 0.8771 - 512ms/epoch - 512ms/step\n",
            "Epoch 67/200\n",
            "1/1 - 1s - loss: 0.9666 - acc: 0.8756 - val_loss: 0.9508 - val_acc: 0.8771 - 514ms/epoch - 514ms/step\n",
            "Epoch 68/200\n",
            "1/1 - 1s - loss: 0.9560 - acc: 0.8812 - val_loss: 0.9390 - val_acc: 0.8771 - 610ms/epoch - 610ms/step\n",
            "Epoch 69/200\n",
            "1/1 - 1s - loss: 0.9437 - acc: 0.8824 - val_loss: 0.9273 - val_acc: 0.8771 - 1s/epoch - 1s/step\n",
            "Epoch 70/200\n",
            "1/1 - 1s - loss: 0.9314 - acc: 0.8835 - val_loss: 0.9157 - val_acc: 0.8771 - 1s/epoch - 1s/step\n",
            "Epoch 71/200\n",
            "1/1 - 1s - loss: 0.9157 - acc: 0.8733 - val_loss: 0.9042 - val_acc: 0.8738 - 1s/epoch - 1s/step\n",
            "Epoch 72/200\n",
            "1/1 - 1s - loss: 0.9146 - acc: 0.8745 - val_loss: 0.8928 - val_acc: 0.8738 - 1s/epoch - 1s/step\n",
            "Epoch 73/200\n",
            "1/1 - 1s - loss: 0.8897 - acc: 0.8764 - val_loss: 0.8814 - val_acc: 0.8738 - 781ms/epoch - 781ms/step\n",
            "Epoch 74/200\n",
            "1/1 - 1s - loss: 0.8818 - acc: 0.8759 - val_loss: 0.8700 - val_acc: 0.8738 - 597ms/epoch - 597ms/step\n",
            "Epoch 75/200\n",
            "1/1 - 1s - loss: 0.8691 - acc: 0.8857 - val_loss: 0.8587 - val_acc: 0.8738 - 513ms/epoch - 513ms/step\n",
            "Epoch 76/200\n",
            "1/1 - 1s - loss: 0.8579 - acc: 0.8793 - val_loss: 0.8474 - val_acc: 0.8738 - 611ms/epoch - 611ms/step\n",
            "Epoch 77/200\n",
            "1/1 - 1s - loss: 0.8530 - acc: 0.8759 - val_loss: 0.8362 - val_acc: 0.8738 - 517ms/epoch - 517ms/step\n",
            "Epoch 78/200\n",
            "1/1 - 1s - loss: 0.8376 - acc: 0.8852 - val_loss: 0.8250 - val_acc: 0.8738 - 539ms/epoch - 539ms/step\n",
            "Epoch 79/200\n",
            "1/1 - 1s - loss: 0.8296 - acc: 0.8781 - val_loss: 0.8139 - val_acc: 0.8738 - 512ms/epoch - 512ms/step\n",
            "Epoch 80/200\n",
            "1/1 - 1s - loss: 0.8186 - acc: 0.8863 - val_loss: 0.8029 - val_acc: 0.8738 - 616ms/epoch - 616ms/step\n",
            "Epoch 81/200\n",
            "1/1 - 1s - loss: 0.8034 - acc: 0.8880 - val_loss: 0.7918 - val_acc: 0.8738 - 615ms/epoch - 615ms/step\n",
            "Epoch 82/200\n",
            "1/1 - 1s - loss: 0.8015 - acc: 0.8776 - val_loss: 0.7808 - val_acc: 0.8738 - 507ms/epoch - 507ms/step\n",
            "Epoch 83/200\n",
            "1/1 - 1s - loss: 0.7801 - acc: 0.8812 - val_loss: 0.7699 - val_acc: 0.8771 - 614ms/epoch - 614ms/step\n",
            "Epoch 84/200\n",
            "1/1 - 1s - loss: 0.7699 - acc: 0.8773 - val_loss: 0.7591 - val_acc: 0.8771 - 629ms/epoch - 629ms/step\n",
            "Epoch 85/200\n",
            "1/1 - 1s - loss: 0.7670 - acc: 0.8891 - val_loss: 0.7484 - val_acc: 0.8837 - 527ms/epoch - 527ms/step\n",
            "Epoch 86/200\n",
            "1/1 - 1s - loss: 0.7352 - acc: 0.8874 - val_loss: 0.7377 - val_acc: 0.8837 - 602ms/epoch - 602ms/step\n",
            "Epoch 87/200\n",
            "1/1 - 1s - loss: 0.7443 - acc: 0.8762 - val_loss: 0.7272 - val_acc: 0.8837 - 600ms/epoch - 600ms/step\n",
            "Epoch 88/200\n",
            "1/1 - 1s - loss: 0.7295 - acc: 0.8905 - val_loss: 0.7167 - val_acc: 0.8837 - 1s/epoch - 1s/step\n",
            "Epoch 89/200\n",
            "1/1 - 1s - loss: 0.7187 - acc: 0.8863 - val_loss: 0.7064 - val_acc: 0.8837 - 1s/epoch - 1s/step\n",
            "Epoch 90/200\n",
            "1/1 - 1s - loss: 0.7098 - acc: 0.8866 - val_loss: 0.6961 - val_acc: 0.8837 - 1s/epoch - 1s/step\n",
            "Epoch 91/200\n",
            "1/1 - 1s - loss: 0.7033 - acc: 0.8885 - val_loss: 0.6860 - val_acc: 0.8837 - 1s/epoch - 1s/step\n",
            "Epoch 92/200\n",
            "1/1 - 1s - loss: 0.6908 - acc: 0.8776 - val_loss: 0.6759 - val_acc: 0.8837 - 1s/epoch - 1s/step\n",
            "Epoch 93/200\n",
            "1/1 - 1s - loss: 0.6742 - acc: 0.8874 - val_loss: 0.6659 - val_acc: 0.8837 - 735ms/epoch - 735ms/step\n",
            "Epoch 94/200\n",
            "1/1 - 1s - loss: 0.6601 - acc: 0.8798 - val_loss: 0.6555 - val_acc: 0.8837 - 523ms/epoch - 523ms/step\n",
            "Epoch 95/200\n",
            "1/1 - 1s - loss: 0.6609 - acc: 0.8891 - val_loss: 0.6452 - val_acc: 0.8837 - 511ms/epoch - 511ms/step\n",
            "Epoch 96/200\n",
            "1/1 - 1s - loss: 0.6465 - acc: 0.8770 - val_loss: 0.6350 - val_acc: 0.8837 - 617ms/epoch - 617ms/step\n",
            "Epoch 97/200\n",
            "1/1 - 1s - loss: 0.6455 - acc: 0.8897 - val_loss: 0.6248 - val_acc: 0.8837 - 608ms/epoch - 608ms/step\n",
            "Epoch 98/200\n",
            "1/1 - 1s - loss: 0.6389 - acc: 0.8863 - val_loss: 0.6146 - val_acc: 0.8837 - 511ms/epoch - 511ms/step\n",
            "Epoch 99/200\n",
            "1/1 - 1s - loss: 0.6223 - acc: 0.8807 - val_loss: 0.6045 - val_acc: 0.8837 - 518ms/epoch - 518ms/step\n",
            "Epoch 100/200\n",
            "1/1 - 1s - loss: 0.6043 - acc: 0.8902 - val_loss: 0.5945 - val_acc: 0.8837 - 518ms/epoch - 518ms/step\n",
            "Epoch 101/200\n",
            "1/1 - 1s - loss: 0.5947 - acc: 0.8793 - val_loss: 0.5847 - val_acc: 0.8837 - 521ms/epoch - 521ms/step\n",
            "Epoch 102/200\n",
            "1/1 - 1s - loss: 0.5894 - acc: 0.8928 - val_loss: 0.5749 - val_acc: 0.8837 - 512ms/epoch - 512ms/step\n",
            "Epoch 103/200\n",
            "1/1 - 1s - loss: 0.5824 - acc: 0.8894 - val_loss: 0.5653 - val_acc: 0.8837 - 618ms/epoch - 618ms/step\n",
            "Epoch 104/200\n",
            "1/1 - 1s - loss: 0.5711 - acc: 0.8804 - val_loss: 0.5559 - val_acc: 0.8837 - 601ms/epoch - 601ms/step\n",
            "Epoch 105/200\n",
            "1/1 - 1s - loss: 0.5538 - acc: 0.8928 - val_loss: 0.5466 - val_acc: 0.8837 - 512ms/epoch - 512ms/step\n",
            "Epoch 106/200\n",
            "1/1 - 1s - loss: 0.5492 - acc: 0.8897 - val_loss: 0.5374 - val_acc: 0.8837 - 606ms/epoch - 606ms/step\n",
            "Epoch 107/200\n",
            "1/1 - 1s - loss: 0.5327 - acc: 0.8919 - val_loss: 0.5285 - val_acc: 0.8837 - 521ms/epoch - 521ms/step\n",
            "Epoch 108/200\n",
            "1/1 - 1s - loss: 0.5369 - acc: 0.8860 - val_loss: 0.5198 - val_acc: 0.8837 - 613ms/epoch - 613ms/step\n",
            "Epoch 109/200\n",
            "1/1 - 1s - loss: 0.5217 - acc: 0.8930 - val_loss: 0.5113 - val_acc: 0.8837 - 606ms/epoch - 606ms/step\n",
            "Epoch 110/200\n",
            "1/1 - 1s - loss: 0.5089 - acc: 0.8914 - val_loss: 0.5029 - val_acc: 0.8837 - 975ms/epoch - 975ms/step\n",
            "Epoch 111/200\n",
            "1/1 - 1s - loss: 0.5034 - acc: 0.8942 - val_loss: 0.4947 - val_acc: 0.8837 - 934ms/epoch - 934ms/step\n",
            "Epoch 112/200\n",
            "1/1 - 1s - loss: 0.4906 - acc: 0.8914 - val_loss: 0.4867 - val_acc: 0.8837 - 946ms/epoch - 946ms/step\n",
            "Epoch 113/200\n",
            "1/1 - 1s - loss: 0.4837 - acc: 0.8953 - val_loss: 0.4788 - val_acc: 0.8837 - 1s/epoch - 1s/step\n",
            "Epoch 114/200\n",
            "1/1 - 1s - loss: 0.4839 - acc: 0.8894 - val_loss: 0.4711 - val_acc: 0.8837 - 771ms/epoch - 771ms/step\n",
            "Epoch 115/200\n",
            "1/1 - 1s - loss: 0.4719 - acc: 0.8945 - val_loss: 0.4635 - val_acc: 0.8837 - 790ms/epoch - 790ms/step\n",
            "Epoch 116/200\n",
            "1/1 - 1s - loss: 0.4604 - acc: 0.8930 - val_loss: 0.4561 - val_acc: 0.8837 - 537ms/epoch - 537ms/step\n",
            "Epoch 117/200\n",
            "1/1 - 1s - loss: 0.4547 - acc: 0.8840 - val_loss: 0.4488 - val_acc: 0.8837 - 606ms/epoch - 606ms/step\n",
            "Epoch 118/200\n",
            "1/1 - 1s - loss: 0.4540 - acc: 0.8947 - val_loss: 0.4417 - val_acc: 0.8837 - 536ms/epoch - 536ms/step\n",
            "Epoch 119/200\n",
            "1/1 - 1s - loss: 0.4522 - acc: 0.8885 - val_loss: 0.4348 - val_acc: 0.8837 - 514ms/epoch - 514ms/step\n",
            "Epoch 120/200\n",
            "1/1 - 1s - loss: 0.4406 - acc: 0.8936 - val_loss: 0.4279 - val_acc: 0.8837 - 520ms/epoch - 520ms/step\n",
            "Epoch 121/200\n",
            "1/1 - 1s - loss: 0.4296 - acc: 0.8930 - val_loss: 0.4213 - val_acc: 0.8837 - 606ms/epoch - 606ms/step\n",
            "Epoch 122/200\n",
            "1/1 - 1s - loss: 0.4153 - acc: 0.8956 - val_loss: 0.4148 - val_acc: 0.8837 - 515ms/epoch - 515ms/step\n",
            "Epoch 123/200\n",
            "1/1 - 1s - loss: 0.4200 - acc: 0.8953 - val_loss: 0.4084 - val_acc: 0.8837 - 522ms/epoch - 522ms/step\n",
            "Epoch 124/200\n",
            "1/1 - 1s - loss: 0.4141 - acc: 0.8953 - val_loss: 0.4022 - val_acc: 0.8837 - 613ms/epoch - 613ms/step\n",
            "Epoch 125/200\n",
            "1/1 - 1s - loss: 0.4022 - acc: 0.8939 - val_loss: 0.3960 - val_acc: 0.8837 - 617ms/epoch - 617ms/step\n",
            "Epoch 126/200\n",
            "1/1 - 1s - loss: 0.3972 - acc: 0.8933 - val_loss: 0.3901 - val_acc: 0.8837 - 603ms/epoch - 603ms/step\n",
            "Epoch 127/200\n",
            "1/1 - 1s - loss: 0.3879 - acc: 0.8970 - val_loss: 0.3842 - val_acc: 0.8837 - 616ms/epoch - 616ms/step\n",
            "Epoch 128/200\n",
            "1/1 - 1s - loss: 0.3832 - acc: 0.8939 - val_loss: 0.3785 - val_acc: 0.8837 - 622ms/epoch - 622ms/step\n",
            "Epoch 129/200\n",
            "1/1 - 1s - loss: 0.3838 - acc: 0.8959 - val_loss: 0.3729 - val_acc: 0.8837 - 517ms/epoch - 517ms/step\n",
            "Epoch 130/200\n",
            "1/1 - 1s - loss: 0.3805 - acc: 0.8953 - val_loss: 0.3674 - val_acc: 0.8837 - 566ms/epoch - 566ms/step\n",
            "Epoch 131/200\n",
            "1/1 - 1s - loss: 0.3724 - acc: 0.8964 - val_loss: 0.3620 - val_acc: 0.8837 - 925ms/epoch - 925ms/step\n",
            "Epoch 132/200\n",
            "1/1 - 1s - loss: 0.3654 - acc: 0.8961 - val_loss: 0.3568 - val_acc: 0.8837 - 952ms/epoch - 952ms/step\n",
            "Epoch 133/200\n",
            "1/1 - 1s - loss: 0.3627 - acc: 0.8961 - val_loss: 0.3516 - val_acc: 0.8837 - 932ms/epoch - 932ms/step\n",
            "Epoch 134/200\n",
            "1/1 - 1s - loss: 0.3526 - acc: 0.8945 - val_loss: 0.3466 - val_acc: 0.8837 - 858ms/epoch - 858ms/step\n",
            "Epoch 135/200\n",
            "1/1 - 1s - loss: 0.3469 - acc: 0.8925 - val_loss: 0.3416 - val_acc: 0.8837 - 1s/epoch - 1s/step\n",
            "1/1 [==============================] - 0s 350ms/step - loss: 0.7524 - acc: 0.8978\n",
            "\n",
            "Test Set Metrics:\n",
            "\tloss: 0.7524\n",
            "\tacc: 0.8978\n",
            "1/1 [==============================] - 0s 354ms/step\n",
            "Using GCN (local pooling) filters...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1/1 - 3s - loss: 1.7217 - acc: 0.0737 - val_loss: 1.6994 - val_acc: 0.0963 - 3s/epoch - 3s/step\n",
            "Epoch 2/200\n",
            "1/1 - 1s - loss: 1.7026 - acc: 0.1058 - val_loss: 1.6773 - val_acc: 0.1030 - 1s/epoch - 1s/step\n",
            "Epoch 3/200\n",
            "1/1 - 1s - loss: 1.6707 - acc: 0.1053 - val_loss: 1.6556 - val_acc: 0.1096 - 774ms/epoch - 774ms/step\n",
            "Epoch 4/200\n",
            "1/1 - 1s - loss: 1.6531 - acc: 0.1022 - val_loss: 1.6342 - val_acc: 0.1096 - 788ms/epoch - 788ms/step\n",
            "Epoch 5/200\n",
            "1/1 - 1s - loss: 1.6291 - acc: 0.1055 - val_loss: 1.6131 - val_acc: 0.1096 - 786ms/epoch - 786ms/step\n",
            "Epoch 6/200\n",
            "1/1 - 1s - loss: 1.6096 - acc: 0.1061 - val_loss: 1.5930 - val_acc: 0.1096 - 509ms/epoch - 509ms/step\n",
            "Epoch 7/200\n",
            "1/1 - 1s - loss: 1.5907 - acc: 0.1041 - val_loss: 1.5738 - val_acc: 0.1096 - 610ms/epoch - 610ms/step\n",
            "Epoch 8/200\n",
            "1/1 - 1s - loss: 1.5742 - acc: 0.1070 - val_loss: 1.5552 - val_acc: 0.1096 - 519ms/epoch - 519ms/step\n",
            "Epoch 9/200\n",
            "1/1 - 1s - loss: 1.5538 - acc: 0.1033 - val_loss: 1.5368 - val_acc: 0.1096 - 506ms/epoch - 506ms/step\n",
            "Epoch 10/200\n",
            "1/1 - 1s - loss: 1.5304 - acc: 0.1075 - val_loss: 1.5187 - val_acc: 0.1096 - 628ms/epoch - 628ms/step\n",
            "Epoch 11/200\n",
            "1/1 - 1s - loss: 1.5206 - acc: 0.1075 - val_loss: 1.5008 - val_acc: 0.1096 - 522ms/epoch - 522ms/step\n",
            "Epoch 12/200\n",
            "1/1 - 1s - loss: 1.5003 - acc: 0.1075 - val_loss: 1.4831 - val_acc: 0.1096 - 515ms/epoch - 515ms/step\n",
            "Epoch 13/200\n",
            "1/1 - 1s - loss: 1.4895 - acc: 0.1075 - val_loss: 1.4655 - val_acc: 0.1096 - 517ms/epoch - 517ms/step\n",
            "Epoch 14/200\n",
            "1/1 - 1s - loss: 1.4730 - acc: 0.1089 - val_loss: 1.4481 - val_acc: 0.5914 - 613ms/epoch - 613ms/step\n",
            "Epoch 15/200\n",
            "1/1 - 1s - loss: 1.4499 - acc: 0.5404 - val_loss: 1.4310 - val_acc: 0.6179 - 606ms/epoch - 606ms/step\n",
            "Epoch 16/200\n",
            "1/1 - 1s - loss: 1.4335 - acc: 0.6566 - val_loss: 1.4139 - val_acc: 0.6910 - 521ms/epoch - 521ms/step\n",
            "Epoch 17/200\n",
            "1/1 - 1s - loss: 1.4181 - acc: 0.7672 - val_loss: 1.3975 - val_acc: 0.9701 - 521ms/epoch - 521ms/step\n",
            "Epoch 18/200\n",
            "1/1 - 1s - loss: 1.4014 - acc: 0.9074 - val_loss: 1.3816 - val_acc: 0.9701 - 609ms/epoch - 609ms/step\n",
            "Epoch 19/200\n",
            "1/1 - 1s - loss: 1.3813 - acc: 0.9277 - val_loss: 1.3662 - val_acc: 0.9701 - 522ms/epoch - 522ms/step\n",
            "Epoch 20/200\n",
            "1/1 - 1s - loss: 1.3704 - acc: 0.9443 - val_loss: 1.3509 - val_acc: 0.9701 - 510ms/epoch - 510ms/step\n",
            "Epoch 21/200\n",
            "1/1 - 1s - loss: 1.3550 - acc: 0.9499 - val_loss: 1.3356 - val_acc: 0.9701 - 640ms/epoch - 640ms/step\n",
            "Epoch 22/200\n",
            "1/1 - 1s - loss: 1.3393 - acc: 0.9485 - val_loss: 1.3203 - val_acc: 0.9701 - 965ms/epoch - 965ms/step\n",
            "Epoch 23/200\n",
            "1/1 - 1s - loss: 1.3348 - acc: 0.9581 - val_loss: 1.3050 - val_acc: 0.9701 - 953ms/epoch - 953ms/step\n",
            "Epoch 24/200\n",
            "1/1 - 1s - loss: 1.3117 - acc: 0.9522 - val_loss: 1.2897 - val_acc: 0.9668 - 989ms/epoch - 989ms/step\n",
            "Epoch 25/200\n",
            "1/1 - 1s - loss: 1.2909 - acc: 0.9544 - val_loss: 1.2743 - val_acc: 0.9668 - 1s/epoch - 1s/step\n",
            "Epoch 26/200\n",
            "1/1 - 1s - loss: 1.2799 - acc: 0.9716 - val_loss: 1.2588 - val_acc: 0.9601 - 1s/epoch - 1s/step\n",
            "Epoch 27/200\n",
            "1/1 - 1s - loss: 1.2618 - acc: 0.9578 - val_loss: 1.2432 - val_acc: 0.9601 - 1s/epoch - 1s/step\n",
            "Epoch 28/200\n",
            "1/1 - 1s - loss: 1.2493 - acc: 0.9643 - val_loss: 1.2276 - val_acc: 0.9601 - 618ms/epoch - 618ms/step\n",
            "Epoch 29/200\n",
            "1/1 - 1s - loss: 1.2411 - acc: 0.9716 - val_loss: 1.2122 - val_acc: 0.9601 - 619ms/epoch - 619ms/step\n",
            "Epoch 30/200\n",
            "1/1 - 1s - loss: 1.2140 - acc: 0.9679 - val_loss: 1.1966 - val_acc: 0.9568 - 615ms/epoch - 615ms/step\n",
            "Epoch 31/200\n",
            "1/1 - 1s - loss: 1.1999 - acc: 0.9620 - val_loss: 1.1809 - val_acc: 0.9568 - 629ms/epoch - 629ms/step\n",
            "Epoch 32/200\n",
            "1/1 - 1s - loss: 1.1800 - acc: 0.9645 - val_loss: 1.1650 - val_acc: 0.9568 - 604ms/epoch - 604ms/step\n",
            "Epoch 33/200\n",
            "1/1 - 1s - loss: 1.1592 - acc: 0.9598 - val_loss: 1.1490 - val_acc: 0.9668 - 523ms/epoch - 523ms/step\n",
            "Epoch 34/200\n",
            "1/1 - 1s - loss: 1.1477 - acc: 0.9682 - val_loss: 1.1329 - val_acc: 0.9668 - 524ms/epoch - 524ms/step\n",
            "Epoch 35/200\n",
            "1/1 - 1s - loss: 1.1338 - acc: 0.9704 - val_loss: 1.1167 - val_acc: 0.9668 - 523ms/epoch - 523ms/step\n",
            "Epoch 36/200\n",
            "1/1 - 1s - loss: 1.1211 - acc: 0.9704 - val_loss: 1.1004 - val_acc: 0.9701 - 516ms/epoch - 516ms/step\n",
            "Epoch 37/200\n",
            "1/1 - 1s - loss: 1.1054 - acc: 0.9648 - val_loss: 1.0837 - val_acc: 0.9701 - 621ms/epoch - 621ms/step\n",
            "Epoch 38/200\n",
            "1/1 - 1s - loss: 1.0825 - acc: 0.9747 - val_loss: 1.0667 - val_acc: 0.9701 - 521ms/epoch - 521ms/step\n",
            "Epoch 39/200\n",
            "1/1 - 1s - loss: 1.0721 - acc: 0.9741 - val_loss: 1.0496 - val_acc: 0.9701 - 617ms/epoch - 617ms/step\n",
            "Epoch 40/200\n",
            "1/1 - 1s - loss: 1.0428 - acc: 0.9721 - val_loss: 1.0324 - val_acc: 0.9701 - 932ms/epoch - 932ms/step\n",
            "Epoch 41/200\n",
            "1/1 - 1s - loss: 1.0326 - acc: 0.9606 - val_loss: 1.0150 - val_acc: 0.9701 - 1s/epoch - 1s/step\n",
            "Epoch 42/200\n",
            "1/1 - 1s - loss: 1.0222 - acc: 0.9730 - val_loss: 0.9975 - val_acc: 0.9701 - 1s/epoch - 1s/step\n",
            "Epoch 43/200\n",
            "1/1 - 1s - loss: 1.0027 - acc: 0.9699 - val_loss: 0.9799 - val_acc: 0.9701 - 1s/epoch - 1s/step\n",
            "Epoch 44/200\n",
            "1/1 - 1s - loss: 0.9945 - acc: 0.9741 - val_loss: 0.9620 - val_acc: 0.9701 - 781ms/epoch - 781ms/step\n",
            "Epoch 45/200\n",
            "1/1 - 1s - loss: 0.9697 - acc: 0.9738 - val_loss: 0.9440 - val_acc: 0.9701 - 605ms/epoch - 605ms/step\n",
            "Epoch 46/200\n",
            "1/1 - 1s - loss: 0.9393 - acc: 0.9637 - val_loss: 0.9260 - val_acc: 0.9701 - 522ms/epoch - 522ms/step\n",
            "Epoch 47/200\n",
            "1/1 - 1s - loss: 0.9272 - acc: 0.9674 - val_loss: 0.9081 - val_acc: 0.9701 - 608ms/epoch - 608ms/step\n",
            "Epoch 48/200\n",
            "1/1 - 1s - loss: 0.9234 - acc: 0.9752 - val_loss: 0.8904 - val_acc: 0.9701 - 618ms/epoch - 618ms/step\n",
            "Epoch 49/200\n",
            "1/1 - 1s - loss: 0.8965 - acc: 0.9738 - val_loss: 0.8725 - val_acc: 0.9701 - 517ms/epoch - 517ms/step\n",
            "Epoch 50/200\n",
            "1/1 - 1s - loss: 0.8752 - acc: 0.9758 - val_loss: 0.8547 - val_acc: 0.9701 - 741ms/epoch - 741ms/step\n",
            "Epoch 51/200\n",
            "1/1 - 1s - loss: 0.8599 - acc: 0.9755 - val_loss: 0.8369 - val_acc: 0.9701 - 881ms/epoch - 881ms/step\n",
            "Epoch 52/200\n",
            "1/1 - 1s - loss: 0.8428 - acc: 0.9738 - val_loss: 0.8190 - val_acc: 0.9701 - 823ms/epoch - 823ms/step\n",
            "Epoch 53/200\n",
            "1/1 - 1s - loss: 0.8232 - acc: 0.9648 - val_loss: 0.8011 - val_acc: 0.9701 - 753ms/epoch - 753ms/step\n",
            "Epoch 54/200\n",
            "1/1 - 1s - loss: 0.8056 - acc: 0.9730 - val_loss: 0.7833 - val_acc: 0.9701 - 1s/epoch - 1s/step\n",
            "Epoch 55/200\n",
            "1/1 - 1s - loss: 0.8109 - acc: 0.9707 - val_loss: 0.7656 - val_acc: 0.9701 - 769ms/epoch - 769ms/step\n",
            "Epoch 56/200\n",
            "1/1 - 1s - loss: 0.7761 - acc: 0.9758 - val_loss: 0.7480 - val_acc: 0.9701 - 1s/epoch - 1s/step\n",
            "Epoch 57/200\n",
            "1/1 - 1s - loss: 0.7537 - acc: 0.9758 - val_loss: 0.7307 - val_acc: 0.9701 - 1s/epoch - 1s/step\n",
            "Epoch 58/200\n",
            "1/1 - 1s - loss: 0.7363 - acc: 0.9758 - val_loss: 0.7134 - val_acc: 0.9701 - 1s/epoch - 1s/step\n",
            "Epoch 59/200\n",
            "1/1 - 1s - loss: 0.7089 - acc: 0.9750 - val_loss: 0.6964 - val_acc: 0.9701 - 1s/epoch - 1s/step\n",
            "Epoch 60/200\n",
            "1/1 - 1s - loss: 0.6980 - acc: 0.9741 - val_loss: 0.6795 - val_acc: 0.9701 - 815ms/epoch - 815ms/step\n",
            "Epoch 61/200\n",
            "1/1 - 1s - loss: 0.6804 - acc: 0.9733 - val_loss: 0.6628 - val_acc: 0.9701 - 1s/epoch - 1s/step\n",
            "Epoch 62/200\n",
            "1/1 - 1s - loss: 0.6799 - acc: 0.9744 - val_loss: 0.6463 - val_acc: 0.9701 - 792ms/epoch - 792ms/step\n",
            "Epoch 63/200\n",
            "1/1 - 1s - loss: 0.6547 - acc: 0.9744 - val_loss: 0.6301 - val_acc: 0.9701 - 689ms/epoch - 689ms/step\n",
            "Epoch 64/200\n",
            "1/1 - 1s - loss: 0.6284 - acc: 0.9758 - val_loss: 0.6140 - val_acc: 0.9701 - 535ms/epoch - 535ms/step\n",
            "Epoch 65/200\n",
            "1/1 - 1s - loss: 0.6120 - acc: 0.9758 - val_loss: 0.5975 - val_acc: 0.9701 - 529ms/epoch - 529ms/step\n",
            "Epoch 66/200\n",
            "1/1 - 1s - loss: 0.6061 - acc: 0.9735 - val_loss: 0.5812 - val_acc: 0.9701 - 623ms/epoch - 623ms/step\n",
            "Epoch 67/200\n",
            "1/1 - 1s - loss: 0.5815 - acc: 0.9750 - val_loss: 0.5653 - val_acc: 0.9701 - 534ms/epoch - 534ms/step\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 1.3970 - acc: 0.9696\n",
            "\n",
            "Test Set Metrics:\n",
            "\tloss: 1.3970\n",
            "\tacc: 0.9696\n",
            "1/1 [==============================] - 0s 358ms/step\n",
            "Using GCN (local pooling) filters...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1/1 - 2s - loss: 1.4656 - acc: 0.0169 - val_loss: 1.4383 - val_acc: 0.0033 - 2s/epoch - 2s/step\n",
            "Epoch 2/200\n",
            "1/1 - 1s - loss: 1.4453 - acc: 0.0121 - val_loss: 1.4175 - val_acc: 0.0033 - 609ms/epoch - 609ms/step\n",
            "Epoch 3/200\n",
            "1/1 - 1s - loss: 1.4250 - acc: 0.0343 - val_loss: 1.3967 - val_acc: 0.5349 - 523ms/epoch - 523ms/step\n",
            "Epoch 4/200\n",
            "1/1 - 1s - loss: 1.4035 - acc: 0.5471 - val_loss: 1.3760 - val_acc: 0.5415 - 524ms/epoch - 524ms/step\n",
            "Epoch 5/200\n",
            "1/1 - 1s - loss: 1.3729 - acc: 0.6744 - val_loss: 1.3556 - val_acc: 0.6346 - 613ms/epoch - 613ms/step\n",
            "Epoch 6/200\n",
            "1/1 - 1s - loss: 1.3672 - acc: 0.5913 - val_loss: 1.3356 - val_acc: 0.7674 - 614ms/epoch - 614ms/step\n",
            "Epoch 7/200\n",
            "1/1 - 1s - loss: 1.3318 - acc: 0.7115 - val_loss: 1.3159 - val_acc: 0.8704 - 610ms/epoch - 610ms/step\n",
            "Epoch 8/200\n",
            "1/1 - 1s - loss: 1.3177 - acc: 0.7537 - val_loss: 1.2957 - val_acc: 0.8738 - 611ms/epoch - 611ms/step\n",
            "Epoch 9/200\n",
            "1/1 - 1s - loss: 1.3017 - acc: 0.8410 - val_loss: 1.2757 - val_acc: 0.8738 - 609ms/epoch - 609ms/step\n",
            "Epoch 10/200\n",
            "1/1 - 1s - loss: 1.2788 - acc: 0.8672 - val_loss: 1.2558 - val_acc: 0.8738 - 505ms/epoch - 505ms/step\n",
            "Epoch 11/200\n",
            "1/1 - 1s - loss: 1.2669 - acc: 0.8570 - val_loss: 1.2362 - val_acc: 0.8738 - 513ms/epoch - 513ms/step\n",
            "Epoch 12/200\n",
            "1/1 - 1s - loss: 1.2415 - acc: 0.8688 - val_loss: 1.2167 - val_acc: 0.8738 - 511ms/epoch - 511ms/step\n",
            "Epoch 13/200\n",
            "1/1 - 1s - loss: 1.2366 - acc: 0.8643 - val_loss: 1.1974 - val_acc: 0.8738 - 1s/epoch - 1s/step\n",
            "Epoch 14/200\n",
            "1/1 - 1s - loss: 1.1970 - acc: 0.8663 - val_loss: 1.1784 - val_acc: 0.8738 - 1s/epoch - 1s/step\n",
            "Epoch 15/200\n",
            "1/1 - 1s - loss: 1.1910 - acc: 0.8567 - val_loss: 1.1598 - val_acc: 0.8738 - 923ms/epoch - 923ms/step\n",
            "Epoch 16/200\n",
            "1/1 - 1s - loss: 1.1748 - acc: 0.8674 - val_loss: 1.1415 - val_acc: 0.8738 - 835ms/epoch - 835ms/step\n",
            "Epoch 17/200\n",
            "1/1 - 1s - loss: 1.1548 - acc: 0.8711 - val_loss: 1.1234 - val_acc: 0.8738 - 775ms/epoch - 775ms/step\n",
            "Epoch 18/200\n",
            "1/1 - 1s - loss: 1.1418 - acc: 0.8672 - val_loss: 1.1054 - val_acc: 0.8738 - 780ms/epoch - 780ms/step\n",
            "Epoch 19/200\n",
            "1/1 - 1s - loss: 1.1214 - acc: 0.8711 - val_loss: 1.0875 - val_acc: 0.8738 - 682ms/epoch - 682ms/step\n",
            "Epoch 20/200\n",
            "1/1 - 1s - loss: 1.1027 - acc: 0.8691 - val_loss: 1.0697 - val_acc: 0.8738 - 607ms/epoch - 607ms/step\n",
            "Epoch 21/200\n",
            "1/1 - 1s - loss: 1.0758 - acc: 0.8722 - val_loss: 1.0521 - val_acc: 0.8738 - 522ms/epoch - 522ms/step\n",
            "Epoch 22/200\n",
            "1/1 - 1s - loss: 1.0630 - acc: 0.8728 - val_loss: 1.0346 - val_acc: 0.8738 - 615ms/epoch - 615ms/step\n",
            "Epoch 23/200\n",
            "1/1 - 1s - loss: 1.0468 - acc: 0.8711 - val_loss: 1.0173 - val_acc: 0.8738 - 503ms/epoch - 503ms/step\n",
            "Epoch 24/200\n",
            "1/1 - 1s - loss: 1.0221 - acc: 0.8725 - val_loss: 1.0001 - val_acc: 0.8738 - 510ms/epoch - 510ms/step\n",
            "Epoch 25/200\n",
            "1/1 - 1s - loss: 1.0217 - acc: 0.8711 - val_loss: 0.9833 - val_acc: 0.8738 - 600ms/epoch - 600ms/step\n",
            "Epoch 26/200\n",
            "1/1 - 1s - loss: 0.9887 - acc: 0.8708 - val_loss: 0.9666 - val_acc: 0.8738 - 613ms/epoch - 613ms/step\n",
            "Epoch 27/200\n",
            "1/1 - 1s - loss: 0.9808 - acc: 0.8728 - val_loss: 0.9500 - val_acc: 0.8738 - 602ms/epoch - 602ms/step\n",
            "Epoch 28/200\n",
            "1/1 - 1s - loss: 0.9671 - acc: 0.8714 - val_loss: 0.9335 - val_acc: 0.8738 - 514ms/epoch - 514ms/step\n",
            "Epoch 29/200\n",
            "1/1 - 1s - loss: 0.9527 - acc: 0.8725 - val_loss: 0.9172 - val_acc: 0.8738 - 611ms/epoch - 611ms/step\n",
            "Epoch 30/200\n",
            "1/1 - 1s - loss: 0.9274 - acc: 0.8714 - val_loss: 0.9009 - val_acc: 0.8738 - 502ms/epoch - 502ms/step\n",
            "Epoch 31/200\n",
            "1/1 - 1s - loss: 0.9102 - acc: 0.8725 - val_loss: 0.8847 - val_acc: 0.8738 - 621ms/epoch - 621ms/step\n",
            "Epoch 32/200\n",
            "1/1 - 1s - loss: 0.9022 - acc: 0.8728 - val_loss: 0.8687 - val_acc: 0.8738 - 517ms/epoch - 517ms/step\n",
            "Epoch 33/200\n",
            "1/1 - 1s - loss: 0.8913 - acc: 0.8728 - val_loss: 0.8527 - val_acc: 0.8738 - 605ms/epoch - 605ms/step\n",
            "Epoch 34/200\n",
            "1/1 - 1s - loss: 0.8693 - acc: 0.8722 - val_loss: 0.8369 - val_acc: 0.8738 - 704ms/epoch - 704ms/step\n",
            "Epoch 35/200\n",
            "1/1 - 1s - loss: 0.8496 - acc: 0.8725 - val_loss: 0.8212 - val_acc: 0.8738 - 921ms/epoch - 921ms/step\n",
            "Epoch 36/200\n",
            "1/1 - 1s - loss: 0.8350 - acc: 0.8725 - val_loss: 0.8058 - val_acc: 0.8738 - 1s/epoch - 1s/step\n",
            "Epoch 37/200\n",
            "1/1 - 1s - loss: 0.8073 - acc: 0.8733 - val_loss: 0.7909 - val_acc: 0.8738 - 815ms/epoch - 815ms/step\n",
            "Epoch 38/200\n",
            "1/1 - 1s - loss: 0.8014 - acc: 0.8725 - val_loss: 0.7766 - val_acc: 0.8738 - 1s/epoch - 1s/step\n",
            "Epoch 39/200\n",
            "1/1 - 1s - loss: 0.7858 - acc: 0.8728 - val_loss: 0.7623 - val_acc: 0.8738 - 718ms/epoch - 718ms/step\n",
            "Epoch 40/200\n",
            "1/1 - 1s - loss: 0.7737 - acc: 0.8725 - val_loss: 0.7480 - val_acc: 0.8738 - 527ms/epoch - 527ms/step\n",
            "Epoch 41/200\n",
            "1/1 - 1s - loss: 0.7521 - acc: 0.8722 - val_loss: 0.7338 - val_acc: 0.8738 - 602ms/epoch - 602ms/step\n",
            "Epoch 42/200\n",
            "1/1 - 1s - loss: 0.7521 - acc: 0.8725 - val_loss: 0.7196 - val_acc: 0.8738 - 616ms/epoch - 616ms/step\n",
            "Epoch 43/200\n",
            "1/1 - 1s - loss: 0.7110 - acc: 0.8798 - val_loss: 0.7054 - val_acc: 0.8738 - 605ms/epoch - 605ms/step\n",
            "Epoch 44/200\n",
            "1/1 - 1s - loss: 0.7194 - acc: 0.8725 - val_loss: 0.6915 - val_acc: 0.8738 - 613ms/epoch - 613ms/step\n",
            "Epoch 45/200\n",
            "1/1 - 1s - loss: 0.6950 - acc: 0.8725 - val_loss: 0.6776 - val_acc: 0.8738 - 604ms/epoch - 604ms/step\n",
            "Epoch 46/200\n",
            "1/1 - 1s - loss: 0.6996 - acc: 0.8739 - val_loss: 0.6639 - val_acc: 0.8738 - 532ms/epoch - 532ms/step\n",
            "Epoch 47/200\n",
            "1/1 - 1s - loss: 0.6785 - acc: 0.8725 - val_loss: 0.6503 - val_acc: 0.8738 - 617ms/epoch - 617ms/step\n",
            "Epoch 48/200\n",
            "1/1 - 1s - loss: 0.6669 - acc: 0.8731 - val_loss: 0.6368 - val_acc: 0.8738 - 613ms/epoch - 613ms/step\n",
            "Epoch 49/200\n",
            "1/1 - 1s - loss: 0.6526 - acc: 0.8728 - val_loss: 0.6235 - val_acc: 0.8738 - 516ms/epoch - 516ms/step\n",
            "Epoch 50/200\n",
            "1/1 - 1s - loss: 0.6487 - acc: 0.8728 - val_loss: 0.6105 - val_acc: 0.8738 - 602ms/epoch - 602ms/step\n",
            "Epoch 51/200\n",
            "1/1 - 1s - loss: 0.6226 - acc: 0.8739 - val_loss: 0.5976 - val_acc: 0.8738 - 611ms/epoch - 611ms/step\n",
            "Epoch 52/200\n",
            "1/1 - 1s - loss: 0.6143 - acc: 0.8722 - val_loss: 0.5848 - val_acc: 0.8738 - 515ms/epoch - 515ms/step\n",
            "Epoch 53/200\n",
            "1/1 - 1s - loss: 0.5973 - acc: 0.8728 - val_loss: 0.5721 - val_acc: 0.8738 - 517ms/epoch - 517ms/step\n",
            "Epoch 54/200\n",
            "1/1 - 1s - loss: 0.5876 - acc: 0.8731 - val_loss: 0.5596 - val_acc: 0.8738 - 994ms/epoch - 994ms/step\n",
            "Epoch 55/200\n",
            "1/1 - 1s - loss: 0.5731 - acc: 0.8745 - val_loss: 0.5474 - val_acc: 0.8738 - 1s/epoch - 1s/step\n",
            "Epoch 56/200\n",
            "1/1 - 1s - loss: 0.5709 - acc: 0.8739 - val_loss: 0.5353 - val_acc: 0.8738 - 939ms/epoch - 939ms/step\n",
            "Epoch 57/200\n",
            "1/1 - 1s - loss: 0.5523 - acc: 0.8728 - val_loss: 0.5235 - val_acc: 0.8738 - 1s/epoch - 1s/step\n",
            "Epoch 58/200\n",
            "1/1 - 1s - loss: 0.5511 - acc: 0.8728 - val_loss: 0.5119 - val_acc: 0.8738 - 1s/epoch - 1s/step\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 1.2974 - acc: 0.8729\n",
            "\n",
            "Test Set Metrics:\n",
            "\tloss: 1.2974\n",
            "\tacc: 0.8729\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f323dfa6050> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 349ms/step\n",
            "Using GCN (local pooling) filters...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1/1 - 3s - loss: 1.8521 - acc: 0.0042 - val_loss: 1.8303 - val_acc: 0.0033 - 3s/epoch - 3s/step\n",
            "Epoch 2/200\n",
            "1/1 - 1s - loss: 1.8276 - acc: 0.0023 - val_loss: 1.8154 - val_acc: 0.0033 - 772ms/epoch - 772ms/step\n",
            "Epoch 3/200\n",
            "1/1 - 1s - loss: 1.8133 - acc: 0.0037 - val_loss: 1.8016 - val_acc: 0.0033 - 780ms/epoch - 780ms/step\n",
            "Epoch 4/200\n",
            "1/1 - 1s - loss: 1.8062 - acc: 0.0037 - val_loss: 1.7872 - val_acc: 0.0033 - 686ms/epoch - 686ms/step\n",
            "Epoch 5/200\n",
            "1/1 - 1s - loss: 1.7941 - acc: 0.0045 - val_loss: 1.7727 - val_acc: 0.0033 - 609ms/epoch - 609ms/step\n",
            "Epoch 6/200\n",
            "1/1 - 1s - loss: 1.7784 - acc: 0.0051 - val_loss: 1.7582 - val_acc: 0.0033 - 506ms/epoch - 506ms/step\n",
            "Epoch 7/200\n",
            "1/1 - 1s - loss: 1.7577 - acc: 0.0056 - val_loss: 1.7438 - val_acc: 0.0033 - 524ms/epoch - 524ms/step\n",
            "Epoch 8/200\n",
            "1/1 - 1s - loss: 1.7437 - acc: 0.0039 - val_loss: 1.7295 - val_acc: 0.0033 - 611ms/epoch - 611ms/step\n",
            "Epoch 9/200\n",
            "1/1 - 1s - loss: 1.7300 - acc: 0.0065 - val_loss: 1.7149 - val_acc: 0.0033 - 620ms/epoch - 620ms/step\n",
            "Epoch 10/200\n",
            "1/1 - 1s - loss: 1.7128 - acc: 0.0070 - val_loss: 1.7000 - val_acc: 0.0033 - 611ms/epoch - 611ms/step\n",
            "Epoch 11/200\n",
            "1/1 - 1s - loss: 1.7037 - acc: 0.0068 - val_loss: 1.6851 - val_acc: 0.0133 - 521ms/epoch - 521ms/step\n",
            "Epoch 12/200\n",
            "1/1 - 1s - loss: 1.6888 - acc: 0.0056 - val_loss: 1.6704 - val_acc: 0.0133 - 515ms/epoch - 515ms/step\n",
            "Epoch 13/200\n",
            "1/1 - 1s - loss: 1.6678 - acc: 0.0132 - val_loss: 1.6555 - val_acc: 0.0266 - 608ms/epoch - 608ms/step\n",
            "Epoch 14/200\n",
            "1/1 - 1s - loss: 1.6612 - acc: 0.0248 - val_loss: 1.6405 - val_acc: 0.0266 - 617ms/epoch - 617ms/step\n",
            "Epoch 15/200\n",
            "1/1 - 1s - loss: 1.6457 - acc: 0.0175 - val_loss: 1.6254 - val_acc: 0.0266 - 614ms/epoch - 614ms/step\n",
            "Epoch 16/200\n",
            "1/1 - 1s - loss: 1.6133 - acc: 0.0158 - val_loss: 1.6104 - val_acc: 0.0266 - 610ms/epoch - 610ms/step\n",
            "Epoch 17/200\n",
            "1/1 - 1s - loss: 1.6047 - acc: 0.0158 - val_loss: 1.5955 - val_acc: 0.0266 - 897ms/epoch - 897ms/step\n",
            "Epoch 18/200\n",
            "1/1 - 1s - loss: 1.6014 - acc: 0.0129 - val_loss: 1.5810 - val_acc: 0.0266 - 1s/epoch - 1s/step\n",
            "Epoch 19/200\n",
            "1/1 - 1s - loss: 1.5859 - acc: 0.0104 - val_loss: 1.5662 - val_acc: 0.0266 - 1s/epoch - 1s/step\n",
            "Epoch 20/200\n",
            "1/1 - 1s - loss: 1.5625 - acc: 0.0276 - val_loss: 1.5515 - val_acc: 0.0332 - 1s/epoch - 1s/step\n",
            "Epoch 21/200\n",
            "1/1 - 1s - loss: 1.5535 - acc: 0.0208 - val_loss: 1.5367 - val_acc: 0.0365 - 789ms/epoch - 789ms/step\n",
            "Epoch 22/200\n",
            "1/1 - 1s - loss: 1.5347 - acc: 0.1452 - val_loss: 1.5220 - val_acc: 0.5183 - 799ms/epoch - 799ms/step\n",
            "Epoch 23/200\n",
            "1/1 - 1s - loss: 1.5291 - acc: 0.5531 - val_loss: 1.5075 - val_acc: 0.5316 - 512ms/epoch - 512ms/step\n",
            "Epoch 24/200\n",
            "1/1 - 1s - loss: 1.5122 - acc: 0.5660 - val_loss: 1.4927 - val_acc: 0.5316 - 613ms/epoch - 613ms/step\n",
            "Epoch 25/200\n",
            "1/1 - 1s - loss: 1.4921 - acc: 0.5846 - val_loss: 1.4779 - val_acc: 0.5382 - 519ms/epoch - 519ms/step\n",
            "Epoch 26/200\n",
            "1/1 - 1s - loss: 1.4855 - acc: 0.5457 - val_loss: 1.4627 - val_acc: 0.5415 - 608ms/epoch - 608ms/step\n",
            "Epoch 27/200\n",
            "1/1 - 1s - loss: 1.4633 - acc: 0.6136 - val_loss: 1.4475 - val_acc: 0.6578 - 520ms/epoch - 520ms/step\n",
            "Epoch 28/200\n",
            "1/1 - 1s - loss: 1.4485 - acc: 0.6693 - val_loss: 1.4325 - val_acc: 0.6578 - 616ms/epoch - 616ms/step\n",
            "Epoch 29/200\n",
            "1/1 - 1s - loss: 1.4311 - acc: 0.6814 - val_loss: 1.4176 - val_acc: 0.7708 - 617ms/epoch - 617ms/step\n",
            "Epoch 30/200\n",
            "1/1 - 1s - loss: 1.4186 - acc: 0.8337 - val_loss: 1.4027 - val_acc: 0.7708 - 511ms/epoch - 511ms/step\n",
            "Epoch 31/200\n",
            "1/1 - 1s - loss: 1.3984 - acc: 0.7999 - val_loss: 1.3877 - val_acc: 0.7708 - 519ms/epoch - 519ms/step\n",
            "Epoch 32/200\n",
            "1/1 - 0s - loss: 1.3834 - acc: 0.8190 - val_loss: 1.3725 - val_acc: 0.8970 - 490ms/epoch - 490ms/step\n",
            "Epoch 33/200\n",
            "1/1 - 1s - loss: 1.3774 - acc: 0.8308 - val_loss: 1.3572 - val_acc: 0.8970 - 604ms/epoch - 604ms/step\n",
            "Epoch 34/200\n",
            "1/1 - 1s - loss: 1.3518 - acc: 0.8300 - val_loss: 1.3419 - val_acc: 0.8970 - 505ms/epoch - 505ms/step\n",
            "Epoch 35/200\n",
            "1/1 - 1s - loss: 1.3404 - acc: 0.8697 - val_loss: 1.3265 - val_acc: 0.8970 - 610ms/epoch - 610ms/step\n",
            "Epoch 36/200\n",
            "1/1 - 1s - loss: 1.3264 - acc: 0.8742 - val_loss: 1.3109 - val_acc: 0.8937 - 507ms/epoch - 507ms/step\n",
            "Epoch 37/200\n",
            "1/1 - 1s - loss: 1.3193 - acc: 0.8672 - val_loss: 1.2953 - val_acc: 0.8937 - 744ms/epoch - 744ms/step\n",
            "Epoch 38/200\n",
            "1/1 - 1s - loss: 1.2928 - acc: 0.8629 - val_loss: 1.2797 - val_acc: 0.8937 - 946ms/epoch - 946ms/step\n",
            "Epoch 39/200\n",
            "1/1 - 1s - loss: 1.2880 - acc: 0.8700 - val_loss: 1.2640 - val_acc: 0.8937 - 1s/epoch - 1s/step\n",
            "Epoch 40/200\n",
            "1/1 - 1s - loss: 1.2713 - acc: 0.8826 - val_loss: 1.2483 - val_acc: 0.8937 - 1s/epoch - 1s/step\n",
            "Epoch 41/200\n",
            "1/1 - 1s - loss: 1.2626 - acc: 0.8798 - val_loss: 1.2324 - val_acc: 0.8937 - 760ms/epoch - 760ms/step\n",
            "Epoch 42/200\n",
            "1/1 - 1s - loss: 1.2357 - acc: 0.8897 - val_loss: 1.2166 - val_acc: 0.8937 - 1s/epoch - 1s/step\n",
            "Epoch 43/200\n",
            "1/1 - 1s - loss: 1.2119 - acc: 0.8795 - val_loss: 1.2008 - val_acc: 0.8937 - 644ms/epoch - 644ms/step\n",
            "Epoch 44/200\n",
            "1/1 - 1s - loss: 1.2120 - acc: 0.8815 - val_loss: 1.1845 - val_acc: 0.8937 - 606ms/epoch - 606ms/step\n",
            "Epoch 45/200\n",
            "1/1 - 1s - loss: 1.1796 - acc: 0.8733 - val_loss: 1.1680 - val_acc: 0.8937 - 520ms/epoch - 520ms/step\n",
            "Epoch 46/200\n",
            "1/1 - 1s - loss: 1.1681 - acc: 0.8843 - val_loss: 1.1510 - val_acc: 0.8970 - 611ms/epoch - 611ms/step\n",
            "Epoch 47/200\n",
            "1/1 - 1s - loss: 1.1519 - acc: 0.8742 - val_loss: 1.1340 - val_acc: 0.8970 - 513ms/epoch - 513ms/step\n",
            "Epoch 48/200\n",
            "1/1 - 1s - loss: 1.1254 - acc: 0.8798 - val_loss: 1.1173 - val_acc: 0.8970 - 600ms/epoch - 600ms/step\n",
            "Epoch 49/200\n",
            "1/1 - 1s - loss: 1.1164 - acc: 0.8919 - val_loss: 1.1012 - val_acc: 0.8970 - 618ms/epoch - 618ms/step\n",
            "Epoch 50/200\n",
            "1/1 - 1s - loss: 1.1053 - acc: 0.8846 - val_loss: 1.0838 - val_acc: 0.8970 - 526ms/epoch - 526ms/step\n",
            "Epoch 51/200\n",
            "1/1 - 1s - loss: 1.0916 - acc: 0.8849 - val_loss: 1.0647 - val_acc: 0.8970 - 516ms/epoch - 516ms/step\n",
            "Epoch 52/200\n",
            "1/1 - 1s - loss: 1.0661 - acc: 0.8824 - val_loss: 1.0455 - val_acc: 0.8970 - 614ms/epoch - 614ms/step\n",
            "Epoch 53/200\n",
            "1/1 - 1s - loss: 1.0516 - acc: 0.8759 - val_loss: 1.0260 - val_acc: 0.8970 - 606ms/epoch - 606ms/step\n",
            "Epoch 54/200\n",
            "1/1 - 1s - loss: 1.0342 - acc: 0.8860 - val_loss: 1.0064 - val_acc: 0.8970 - 519ms/epoch - 519ms/step\n",
            "Epoch 55/200\n",
            "1/1 - 1s - loss: 1.0050 - acc: 0.8869 - val_loss: 0.9865 - val_acc: 0.8970 - 504ms/epoch - 504ms/step\n",
            "Epoch 56/200\n",
            "1/1 - 0s - loss: 0.9948 - acc: 0.8770 - val_loss: 0.9666 - val_acc: 0.8970 - 487ms/epoch - 487ms/step\n",
            "Epoch 57/200\n",
            "1/1 - 1s - loss: 0.9621 - acc: 0.8832 - val_loss: 0.9466 - val_acc: 0.8970 - 505ms/epoch - 505ms/step\n",
            "Epoch 58/200\n",
            "1/1 - 1s - loss: 0.9600 - acc: 0.8883 - val_loss: 0.9265 - val_acc: 0.8970 - 982ms/epoch - 982ms/step\n",
            "Epoch 59/200\n",
            "1/1 - 1s - loss: 0.9311 - acc: 0.8877 - val_loss: 0.9064 - val_acc: 0.8970 - 895ms/epoch - 895ms/step\n",
            "Epoch 60/200\n",
            "1/1 - 1s - loss: 0.9218 - acc: 0.8897 - val_loss: 0.8864 - val_acc: 0.8970 - 959ms/epoch - 959ms/step\n",
            "Epoch 61/200\n",
            "1/1 - 1s - loss: 0.8980 - acc: 0.8767 - val_loss: 0.8664 - val_acc: 0.8970 - 966ms/epoch - 966ms/step\n",
            "Epoch 62/200\n",
            "1/1 - 1s - loss: 0.8732 - acc: 0.8883 - val_loss: 0.8465 - val_acc: 0.8970 - 811ms/epoch - 811ms/step\n",
            "Epoch 63/200\n",
            "1/1 - 1s - loss: 0.8552 - acc: 0.8863 - val_loss: 0.8268 - val_acc: 0.8970 - 1s/epoch - 1s/step\n",
            "Epoch 64/200\n",
            "1/1 - 1s - loss: 0.8317 - acc: 0.8942 - val_loss: 0.8071 - val_acc: 0.8970 - 780ms/epoch - 780ms/step\n",
            "Epoch 65/200\n",
            "1/1 - 1s - loss: 0.8142 - acc: 0.9389 - val_loss: 0.7876 - val_acc: 0.8970 - 706ms/epoch - 706ms/step\n",
            "Epoch 66/200\n",
            "1/1 - 1s - loss: 0.7938 - acc: 0.8900 - val_loss: 0.7682 - val_acc: 0.8970 - 609ms/epoch - 609ms/step\n",
            "Epoch 67/200\n",
            "1/1 - 1s - loss: 0.7674 - acc: 0.8888 - val_loss: 0.7490 - val_acc: 0.8970 - 603ms/epoch - 603ms/step\n",
            "Epoch 68/200\n",
            "1/1 - 1s - loss: 0.7561 - acc: 0.8880 - val_loss: 0.7300 - val_acc: 0.9003 - 613ms/epoch - 613ms/step\n",
            "Epoch 69/200\n",
            "1/1 - 1s - loss: 0.7350 - acc: 0.8922 - val_loss: 0.7113 - val_acc: 0.9003 - 517ms/epoch - 517ms/step\n",
            "Epoch 70/200\n",
            "1/1 - 1s - loss: 0.7323 - acc: 0.8871 - val_loss: 0.6929 - val_acc: 0.9003 - 607ms/epoch - 607ms/step\n",
            "Epoch 71/200\n",
            "1/1 - 1s - loss: 0.6997 - acc: 0.9105 - val_loss: 0.6747 - val_acc: 0.9003 - 605ms/epoch - 605ms/step\n",
            "Epoch 72/200\n",
            "1/1 - 1s - loss: 0.6838 - acc: 0.9102 - val_loss: 0.6569 - val_acc: 0.9003 - 583ms/epoch - 583ms/step\n",
            "Epoch 73/200\n",
            "1/1 - 1s - loss: 0.6681 - acc: 0.9370 - val_loss: 0.6393 - val_acc: 0.9003 - 616ms/epoch - 616ms/step\n",
            "Epoch 74/200\n",
            "1/1 - 1s - loss: 0.6399 - acc: 0.9147 - val_loss: 0.6219 - val_acc: 0.9402 - 508ms/epoch - 508ms/step\n",
            "Epoch 75/200\n",
            "1/1 - 1s - loss: 0.6273 - acc: 0.8981 - val_loss: 0.6048 - val_acc: 0.9767 - 518ms/epoch - 518ms/step\n",
            "Epoch 76/200\n",
            "1/1 - 1s - loss: 0.6118 - acc: 0.9547 - val_loss: 0.5880 - val_acc: 0.9767 - 525ms/epoch - 525ms/step\n",
            "Epoch 77/200\n",
            "1/1 - 1s - loss: 0.6022 - acc: 0.9772 - val_loss: 0.5715 - val_acc: 0.9767 - 508ms/epoch - 508ms/step\n",
            "Epoch 78/200\n",
            "1/1 - 1s - loss: 0.5772 - acc: 0.9690 - val_loss: 0.5553 - val_acc: 0.9767 - 617ms/epoch - 617ms/step\n",
            "Epoch 79/200\n",
            "1/1 - 1s - loss: 0.5582 - acc: 0.9685 - val_loss: 0.5395 - val_acc: 0.9767 - 534ms/epoch - 534ms/step\n",
            "Epoch 80/200\n",
            "1/1 - 1s - loss: 0.5484 - acc: 0.9519 - val_loss: 0.5240 - val_acc: 0.9834 - 1s/epoch - 1s/step\n",
            "Epoch 81/200\n",
            "1/1 - 1s - loss: 0.5461 - acc: 0.9462 - val_loss: 0.5089 - val_acc: 0.9834 - 935ms/epoch - 935ms/step\n",
            "Epoch 82/200\n",
            "1/1 - 1s - loss: 0.5258 - acc: 0.9733 - val_loss: 0.4942 - val_acc: 0.9834 - 931ms/epoch - 931ms/step\n",
            "Epoch 83/200\n",
            "1/1 - 1s - loss: 0.5123 - acc: 0.9479 - val_loss: 0.4799 - val_acc: 0.9834 - 1s/epoch - 1s/step\n",
            "Epoch 84/200\n",
            "1/1 - 1s - loss: 0.5048 - acc: 0.9769 - val_loss: 0.4659 - val_acc: 0.9834 - 781ms/epoch - 781ms/step\n",
            "Epoch 85/200\n",
            "1/1 - 1s - loss: 0.4788 - acc: 0.9758 - val_loss: 0.4523 - val_acc: 0.9834 - 763ms/epoch - 763ms/step\n",
            "Epoch 86/200\n",
            "1/1 - 1s - loss: 0.4600 - acc: 0.9772 - val_loss: 0.4390 - val_acc: 0.9834 - 510ms/epoch - 510ms/step\n",
            "Epoch 87/200\n",
            "1/1 - 1s - loss: 0.4485 - acc: 0.9769 - val_loss: 0.4262 - val_acc: 0.9834 - 514ms/epoch - 514ms/step\n",
            "Epoch 88/200\n",
            "1/1 - 1s - loss: 0.4486 - acc: 0.9764 - val_loss: 0.4137 - val_acc: 0.9834 - 629ms/epoch - 629ms/step\n",
            "Epoch 89/200\n",
            "1/1 - 1s - loss: 0.4191 - acc: 0.9766 - val_loss: 0.4016 - val_acc: 0.9834 - 605ms/epoch - 605ms/step\n",
            "Epoch 90/200\n",
            "1/1 - 1s - loss: 0.4065 - acc: 0.9761 - val_loss: 0.3898 - val_acc: 0.9834 - 609ms/epoch - 609ms/step\n",
            "Epoch 91/200\n",
            "1/1 - 1s - loss: 0.4122 - acc: 0.9769 - val_loss: 0.3782 - val_acc: 0.9834 - 620ms/epoch - 620ms/step\n",
            "Epoch 92/200\n",
            "1/1 - 1s - loss: 0.3910 - acc: 0.9769 - val_loss: 0.3669 - val_acc: 0.9834 - 523ms/epoch - 523ms/step\n",
            "Epoch 93/200\n",
            "1/1 - 1s - loss: 0.3747 - acc: 0.9769 - val_loss: 0.3559 - val_acc: 0.9834 - 514ms/epoch - 514ms/step\n",
            "Epoch 94/200\n",
            "1/1 - 1s - loss: 0.3694 - acc: 0.9744 - val_loss: 0.3452 - val_acc: 0.9834 - 527ms/epoch - 527ms/step\n",
            "Epoch 95/200\n",
            "1/1 - 1s - loss: 0.3592 - acc: 0.9766 - val_loss: 0.3349 - val_acc: 0.9834 - 513ms/epoch - 513ms/step\n",
            "Epoch 96/200\n",
            "1/1 - 1s - loss: 0.3431 - acc: 0.9766 - val_loss: 0.3249 - val_acc: 0.9834 - 612ms/epoch - 612ms/step\n",
            "Epoch 97/200\n",
            "1/1 - 1s - loss: 0.3414 - acc: 0.9758 - val_loss: 0.3153 - val_acc: 0.9834 - 604ms/epoch - 604ms/step\n",
            "Epoch 98/200\n",
            "1/1 - 1s - loss: 0.3362 - acc: 0.9769 - val_loss: 0.3059 - val_acc: 0.9834 - 524ms/epoch - 524ms/step\n",
            "Epoch 99/200\n",
            "1/1 - 1s - loss: 0.3246 - acc: 0.9769 - val_loss: 0.2969 - val_acc: 0.9834 - 519ms/epoch - 519ms/step\n",
            "Epoch 100/200\n",
            "1/1 - 1s - loss: 0.3148 - acc: 0.9772 - val_loss: 0.2881 - val_acc: 0.9834 - 507ms/epoch - 507ms/step\n",
            "Epoch 101/200\n",
            "1/1 - 1s - loss: 0.2926 - acc: 0.9769 - val_loss: 0.2797 - val_acc: 0.9834 - 1s/epoch - 1s/step\n",
            "Epoch 102/200\n",
            "1/1 - 1s - loss: 0.2825 - acc: 0.9769 - val_loss: 0.2716 - val_acc: 0.9834 - 1s/epoch - 1s/step\n",
            "Epoch 103/200\n",
            "1/1 - 1s - loss: 0.2969 - acc: 0.9766 - val_loss: 0.2638 - val_acc: 0.9834 - 1s/epoch - 1s/step\n",
            "Epoch 104/200\n",
            "1/1 - 1s - loss: 0.2711 - acc: 0.9766 - val_loss: 0.2563 - val_acc: 0.9834 - 783ms/epoch - 783ms/step\n",
            "Epoch 105/200\n",
            "1/1 - 1s - loss: 0.2765 - acc: 0.9764 - val_loss: 0.2491 - val_acc: 0.9834 - 769ms/epoch - 769ms/step\n",
            "Epoch 106/200\n",
            "1/1 - 1s - loss: 0.2654 - acc: 0.9769 - val_loss: 0.2421 - val_acc: 0.9834 - 782ms/epoch - 782ms/step\n",
            "Epoch 107/200\n",
            "1/1 - 1s - loss: 0.2615 - acc: 0.9769 - val_loss: 0.2354 - val_acc: 0.9834 - 591ms/epoch - 591ms/step\n",
            "Epoch 108/200\n",
            "1/1 - 1s - loss: 0.2572 - acc: 0.9775 - val_loss: 0.2289 - val_acc: 0.9834 - 517ms/epoch - 517ms/step\n",
            "Epoch 109/200\n",
            "1/1 - 1s - loss: 0.2402 - acc: 0.9769 - val_loss: 0.2227 - val_acc: 0.9834 - 608ms/epoch - 608ms/step\n",
            "Epoch 110/200\n",
            "1/1 - 1s - loss: 0.2356 - acc: 0.9769 - val_loss: 0.2168 - val_acc: 0.9834 - 511ms/epoch - 511ms/step\n",
            "Epoch 111/200\n",
            "1/1 - 1s - loss: 0.2359 - acc: 0.9769 - val_loss: 0.2111 - val_acc: 0.9834 - 525ms/epoch - 525ms/step\n",
            "Epoch 112/200\n",
            "1/1 - 1s - loss: 0.2247 - acc: 0.9769 - val_loss: 0.2056 - val_acc: 0.9834 - 505ms/epoch - 505ms/step\n",
            "Epoch 113/200\n",
            "1/1 - 1s - loss: 0.2250 - acc: 0.9766 - val_loss: 0.2003 - val_acc: 0.9834 - 508ms/epoch - 508ms/step\n",
            "Epoch 114/200\n",
            "1/1 - 1s - loss: 0.2198 - acc: 0.9764 - val_loss: 0.1953 - val_acc: 0.9834 - 606ms/epoch - 606ms/step\n",
            "Epoch 115/200\n",
            "1/1 - 1s - loss: 0.2079 - acc: 0.9764 - val_loss: 0.1904 - val_acc: 0.9834 - 618ms/epoch - 618ms/step\n",
            "Epoch 116/200\n",
            "1/1 - 1s - loss: 0.2139 - acc: 0.9766 - val_loss: 0.1858 - val_acc: 0.9834 - 605ms/epoch - 605ms/step\n",
            "Epoch 117/200\n",
            "1/1 - 1s - loss: 0.2155 - acc: 0.9769 - val_loss: 0.1814 - val_acc: 0.9834 - 516ms/epoch - 516ms/step\n",
            "Epoch 118/200\n",
            "1/1 - 1s - loss: 0.2041 - acc: 0.9766 - val_loss: 0.1771 - val_acc: 0.9834 - 523ms/epoch - 523ms/step\n",
            "Epoch 119/200\n",
            "1/1 - 1s - loss: 0.1931 - acc: 0.9769 - val_loss: 0.1730 - val_acc: 0.9834 - 619ms/epoch - 619ms/step\n",
            "Epoch 120/200\n",
            "1/1 - 1s - loss: 0.1864 - acc: 0.9769 - val_loss: 0.1691 - val_acc: 0.9834 - 514ms/epoch - 514ms/step\n",
            "Epoch 121/200\n",
            "1/1 - 1s - loss: 0.1946 - acc: 0.9769 - val_loss: 0.1653 - val_acc: 0.9834 - 613ms/epoch - 613ms/step\n",
            "Epoch 122/200\n",
            "1/1 - 1s - loss: 0.1905 - acc: 0.9769 - val_loss: 0.1616 - val_acc: 0.9834 - 1s/epoch - 1s/step\n",
            "Epoch 123/200\n",
            "1/1 - 1s - loss: 0.1831 - acc: 0.9769 - val_loss: 0.1581 - val_acc: 0.9834 - 1s/epoch - 1s/step\n",
            "Epoch 124/200\n",
            "1/1 - 1s - loss: 0.1843 - acc: 0.9766 - val_loss: 0.1547 - val_acc: 0.9834 - 941ms/epoch - 941ms/step\n",
            "Epoch 125/200\n",
            "1/1 - 1s - loss: 0.1806 - acc: 0.9769 - val_loss: 0.1515 - val_acc: 0.9834 - 784ms/epoch - 784ms/step\n",
            "Epoch 126/200\n",
            "1/1 - 1s - loss: 0.1659 - acc: 0.9769 - val_loss: 0.1484 - val_acc: 0.9834 - 1s/epoch - 1s/step\n",
            "Epoch 127/200\n",
            "1/1 - 1s - loss: 0.1655 - acc: 0.9766 - val_loss: 0.1454 - val_acc: 0.9834 - 755ms/epoch - 755ms/step\n",
            "Epoch 128/200\n",
            "1/1 - 1s - loss: 0.1701 - acc: 0.9769 - val_loss: 0.1425 - val_acc: 0.9834 - 610ms/epoch - 610ms/step\n",
            "Epoch 129/200\n",
            "1/1 - 1s - loss: 0.1624 - acc: 0.9769 - val_loss: 0.1398 - val_acc: 0.9834 - 611ms/epoch - 611ms/step\n",
            "Epoch 130/200\n",
            "1/1 - 1s - loss: 0.1618 - acc: 0.9766 - val_loss: 0.1372 - val_acc: 0.9834 - 622ms/epoch - 622ms/step\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.5286 - acc: 0.9834\n",
            "\n",
            "Test Set Metrics:\n",
            "\tloss: 0.5286\n",
            "\tacc: 0.9834\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f32303c8e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 364ms/step\n",
            "Using GCN (local pooling) filters...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1/1 - 2s - loss: 1.5387 - acc: 0.0115 - val_loss: 1.5240 - val_acc: 0.0066 - 2s/epoch - 2s/step\n",
            "Epoch 2/200\n",
            "1/1 - 1s - loss: 1.5155 - acc: 0.0250 - val_loss: 1.4980 - val_acc: 0.0066 - 513ms/epoch - 513ms/step\n",
            "Epoch 3/200\n",
            "1/1 - 1s - loss: 1.4834 - acc: 0.0129 - val_loss: 1.4723 - val_acc: 0.0066 - 509ms/epoch - 509ms/step\n",
            "Epoch 4/200\n",
            "1/1 - 1s - loss: 1.4518 - acc: 0.0183 - val_loss: 1.4465 - val_acc: 0.0066 - 509ms/epoch - 509ms/step\n",
            "Epoch 5/200\n",
            "1/1 - 1s - loss: 1.4221 - acc: 0.0132 - val_loss: 1.4200 - val_acc: 0.0066 - 614ms/epoch - 614ms/step\n",
            "Epoch 6/200\n",
            "1/1 - 1s - loss: 1.4124 - acc: 0.0231 - val_loss: 1.3937 - val_acc: 0.0066 - 608ms/epoch - 608ms/step\n",
            "Epoch 7/200\n",
            "1/1 - 1s - loss: 1.3790 - acc: 0.0169 - val_loss: 1.3669 - val_acc: 0.0066 - 615ms/epoch - 615ms/step\n",
            "Epoch 8/200\n",
            "1/1 - 1s - loss: 1.3631 - acc: 0.0239 - val_loss: 1.3402 - val_acc: 0.0066 - 602ms/epoch - 602ms/step\n",
            "Epoch 9/200\n",
            "1/1 - 1s - loss: 1.3265 - acc: 0.0200 - val_loss: 1.3134 - val_acc: 0.0066 - 511ms/epoch - 511ms/step\n",
            "Epoch 10/200\n",
            "1/1 - 1s - loss: 1.2840 - acc: 0.4576 - val_loss: 1.2869 - val_acc: 0.4684 - 518ms/epoch - 518ms/step\n",
            "Epoch 11/200\n",
            "1/1 - 1s - loss: 1.2742 - acc: 0.5699 - val_loss: 1.2605 - val_acc: 0.4684 - 516ms/epoch - 516ms/step\n",
            "Epoch 12/200\n",
            "1/1 - 0s - loss: 1.2509 - acc: 0.5446 - val_loss: 1.2342 - val_acc: 0.4817 - 480ms/epoch - 480ms/step\n",
            "Epoch 13/200\n",
            "1/1 - 1s - loss: 1.2344 - acc: 0.5590 - val_loss: 1.2082 - val_acc: 0.4850 - 622ms/epoch - 622ms/step\n",
            "Epoch 14/200\n",
            "1/1 - 1s - loss: 1.1938 - acc: 0.5975 - val_loss: 1.1823 - val_acc: 0.4850 - 608ms/epoch - 608ms/step\n",
            "Epoch 15/200\n",
            "1/1 - 1s - loss: 1.1567 - acc: 0.6493 - val_loss: 1.1566 - val_acc: 0.5914 - 1s/epoch - 1s/step\n",
            "Epoch 16/200\n",
            "1/1 - 1s - loss: 1.1480 - acc: 0.6834 - val_loss: 1.1310 - val_acc: 0.6047 - 932ms/epoch - 932ms/step\n",
            "Epoch 17/200\n",
            "1/1 - 1s - loss: 1.1065 - acc: 0.7430 - val_loss: 1.1056 - val_acc: 0.7342 - 1s/epoch - 1s/step\n",
            "Epoch 18/200\n",
            "1/1 - 1s - loss: 1.1057 - acc: 0.7402 - val_loss: 1.0805 - val_acc: 0.7375 - 831ms/epoch - 831ms/step\n",
            "Epoch 19/200\n",
            "1/1 - 1s - loss: 1.0666 - acc: 0.7087 - val_loss: 1.0556 - val_acc: 0.7409 - 764ms/epoch - 764ms/step\n",
            "Epoch 20/200\n",
            "1/1 - 1s - loss: 1.0363 - acc: 0.8573 - val_loss: 1.0310 - val_acc: 0.8738 - 783ms/epoch - 783ms/step\n",
            "Epoch 21/200\n",
            "1/1 - 1s - loss: 1.0049 - acc: 0.8750 - val_loss: 1.0066 - val_acc: 0.8738 - 616ms/epoch - 616ms/step\n",
            "Epoch 22/200\n",
            "1/1 - 1s - loss: 1.0005 - acc: 0.8641 - val_loss: 0.9823 - val_acc: 0.8738 - 612ms/epoch - 612ms/step\n",
            "Epoch 23/200\n",
            "1/1 - 1s - loss: 0.9693 - acc: 0.8753 - val_loss: 0.9583 - val_acc: 0.8738 - 516ms/epoch - 516ms/step\n",
            "Epoch 24/200\n",
            "1/1 - 1s - loss: 0.9358 - acc: 0.8672 - val_loss: 0.9345 - val_acc: 0.8738 - 513ms/epoch - 513ms/step\n",
            "Epoch 25/200\n",
            "1/1 - 1s - loss: 0.9224 - acc: 0.8666 - val_loss: 0.9108 - val_acc: 0.8738 - 608ms/epoch - 608ms/step\n",
            "Epoch 26/200\n",
            "1/1 - 1s - loss: 0.8888 - acc: 0.8733 - val_loss: 0.8873 - val_acc: 0.8738 - 607ms/epoch - 607ms/step\n",
            "Epoch 27/200\n",
            "1/1 - 1s - loss: 0.8749 - acc: 0.8781 - val_loss: 0.8640 - val_acc: 0.8738 - 619ms/epoch - 619ms/step\n",
            "Epoch 28/200\n",
            "1/1 - 1s - loss: 0.8566 - acc: 0.8843 - val_loss: 0.8406 - val_acc: 0.8738 - 518ms/epoch - 518ms/step\n",
            "Epoch 29/200\n",
            "1/1 - 1s - loss: 0.8266 - acc: 0.8739 - val_loss: 0.8174 - val_acc: 0.8738 - 508ms/epoch - 508ms/step\n",
            "Epoch 30/200\n",
            "1/1 - 1s - loss: 0.8034 - acc: 0.8731 - val_loss: 0.7946 - val_acc: 0.8738 - 603ms/epoch - 603ms/step\n",
            "Epoch 31/200\n",
            "1/1 - 1s - loss: 0.7787 - acc: 0.8745 - val_loss: 0.7720 - val_acc: 0.8738 - 517ms/epoch - 517ms/step\n",
            "Epoch 32/200\n",
            "1/1 - 1s - loss: 0.7703 - acc: 0.8736 - val_loss: 0.7499 - val_acc: 0.8738 - 520ms/epoch - 520ms/step\n",
            "Epoch 33/200\n",
            "1/1 - 1s - loss: 0.7380 - acc: 0.8742 - val_loss: 0.7281 - val_acc: 0.8738 - 519ms/epoch - 519ms/step\n",
            "Epoch 34/200\n",
            "1/1 - 1s - loss: 0.7122 - acc: 0.8708 - val_loss: 0.7069 - val_acc: 0.8738 - 607ms/epoch - 607ms/step\n",
            "Epoch 35/200\n",
            "1/1 - 1s - loss: 0.6876 - acc: 0.8756 - val_loss: 0.6862 - val_acc: 0.8738 - 603ms/epoch - 603ms/step\n",
            "Epoch 36/200\n",
            "1/1 - 1s - loss: 0.6820 - acc: 0.8733 - val_loss: 0.6659 - val_acc: 0.8738 - 617ms/epoch - 617ms/step\n",
            "Epoch 37/200\n",
            "1/1 - 1s - loss: 0.6646 - acc: 0.8753 - val_loss: 0.6462 - val_acc: 0.8738 - 1s/epoch - 1s/step\n",
            "Epoch 38/200\n",
            "1/1 - 1s - loss: 0.6419 - acc: 0.8753 - val_loss: 0.6270 - val_acc: 0.8738 - 1s/epoch - 1s/step\n",
            "Epoch 39/200\n",
            "1/1 - 1s - loss: 0.6120 - acc: 0.8722 - val_loss: 0.6084 - val_acc: 0.8738 - 952ms/epoch - 952ms/step\n",
            "Epoch 40/200\n",
            "1/1 - 1s - loss: 0.6061 - acc: 0.8764 - val_loss: 0.5902 - val_acc: 0.8738 - 761ms/epoch - 761ms/step\n",
            "Epoch 41/200\n",
            "1/1 - 1s - loss: 0.5764 - acc: 0.8739 - val_loss: 0.5727 - val_acc: 0.8738 - 770ms/epoch - 770ms/step\n",
            "Epoch 42/200\n",
            "1/1 - 1s - loss: 0.5607 - acc: 0.8742 - val_loss: 0.5556 - val_acc: 0.8738 - 714ms/epoch - 714ms/step\n",
            "Epoch 43/200\n",
            "1/1 - 1s - loss: 0.5437 - acc: 0.8762 - val_loss: 0.5391 - val_acc: 0.8738 - 606ms/epoch - 606ms/step\n",
            "Epoch 44/200\n",
            "1/1 - 1s - loss: 0.5380 - acc: 0.8719 - val_loss: 0.5232 - val_acc: 0.8738 - 574ms/epoch - 574ms/step\n",
            "Epoch 45/200\n",
            "1/1 - 1s - loss: 0.5184 - acc: 0.8728 - val_loss: 0.5078 - val_acc: 0.8738 - 606ms/epoch - 606ms/step\n",
            "Epoch 46/200\n",
            "1/1 - 1s - loss: 0.5064 - acc: 0.8725 - val_loss: 0.4929 - val_acc: 0.8738 - 508ms/epoch - 508ms/step\n",
            "Epoch 47/200\n",
            "1/1 - 1s - loss: 0.4860 - acc: 0.8857 - val_loss: 0.4786 - val_acc: 0.8738 - 529ms/epoch - 529ms/step\n",
            "Epoch 48/200\n",
            "1/1 - 1s - loss: 0.4732 - acc: 0.8728 - val_loss: 0.4649 - val_acc: 0.8738 - 605ms/epoch - 605ms/step\n",
            "Epoch 49/200\n",
            "1/1 - 1s - loss: 0.4529 - acc: 0.8731 - val_loss: 0.4517 - val_acc: 0.8738 - 513ms/epoch - 513ms/step\n",
            "Epoch 50/200\n",
            "1/1 - 1s - loss: 0.4435 - acc: 0.8753 - val_loss: 0.4391 - val_acc: 0.8738 - 605ms/epoch - 605ms/step\n",
            "Epoch 51/200\n",
            "1/1 - 1s - loss: 0.4346 - acc: 0.8739 - val_loss: 0.4271 - val_acc: 0.8738 - 506ms/epoch - 506ms/step\n",
            "Epoch 52/200\n",
            "1/1 - 0s - loss: 0.4177 - acc: 0.8843 - val_loss: 0.4155 - val_acc: 0.8738 - 480ms/epoch - 480ms/step\n",
            "Epoch 53/200\n",
            "1/1 - 1s - loss: 0.4037 - acc: 0.8759 - val_loss: 0.4045 - val_acc: 0.8738 - 610ms/epoch - 610ms/step\n",
            "Epoch 54/200\n",
            "1/1 - 1s - loss: 0.4012 - acc: 0.8756 - val_loss: 0.3939 - val_acc: 0.8738 - 530ms/epoch - 530ms/step\n",
            "Epoch 55/200\n",
            "1/1 - 1s - loss: 0.3913 - acc: 0.8750 - val_loss: 0.3837 - val_acc: 0.8738 - 508ms/epoch - 508ms/step\n",
            "Epoch 56/200\n",
            "1/1 - 1s - loss: 0.3663 - acc: 0.9004 - val_loss: 0.3740 - val_acc: 0.8738 - 600ms/epoch - 600ms/step\n",
            "Epoch 57/200\n",
            "1/1 - 1s - loss: 0.3678 - acc: 0.8742 - val_loss: 0.3647 - val_acc: 0.8738 - 603ms/epoch - 603ms/step\n",
            "Epoch 58/200\n",
            "1/1 - 1s - loss: 0.3565 - acc: 0.8877 - val_loss: 0.3559 - val_acc: 0.8837 - 1s/epoch - 1s/step\n",
            "Epoch 59/200\n",
            "1/1 - 1s - loss: 0.3493 - acc: 0.8840 - val_loss: 0.3474 - val_acc: 0.8837 - 927ms/epoch - 927ms/step\n",
            "Epoch 60/200\n",
            "1/1 - 1s - loss: 0.3366 - acc: 0.9108 - val_loss: 0.3392 - val_acc: 0.8837 - 908ms/epoch - 908ms/step\n",
            "Epoch 61/200\n",
            "1/1 - 1s - loss: 0.3272 - acc: 0.8857 - val_loss: 0.3314 - val_acc: 0.8904 - 788ms/epoch - 788ms/step\n",
            "Epoch 62/200\n",
            "1/1 - 1s - loss: 0.3195 - acc: 0.8748 - val_loss: 0.3238 - val_acc: 0.8904 - 761ms/epoch - 761ms/step\n",
            "Epoch 63/200\n",
            "1/1 - 1s - loss: 0.3134 - acc: 0.8950 - val_loss: 0.3166 - val_acc: 0.9402 - 635ms/epoch - 635ms/step\n",
            "Epoch 64/200\n",
            "1/1 - 1s - loss: 0.3095 - acc: 0.9316 - val_loss: 0.3096 - val_acc: 0.9668 - 508ms/epoch - 508ms/step\n",
            "Epoch 65/200\n",
            "1/1 - 1s - loss: 0.2943 - acc: 0.9170 - val_loss: 0.3029 - val_acc: 0.9668 - 517ms/epoch - 517ms/step\n",
            "Epoch 66/200\n",
            "1/1 - 1s - loss: 0.2880 - acc: 0.9567 - val_loss: 0.2965 - val_acc: 0.9668 - 611ms/epoch - 611ms/step\n",
            "Epoch 67/200\n",
            "1/1 - 1s - loss: 0.2895 - acc: 0.9330 - val_loss: 0.2902 - val_acc: 0.9668 - 506ms/epoch - 506ms/step\n",
            "Epoch 68/200\n",
            "1/1 - 1s - loss: 0.2767 - acc: 0.9643 - val_loss: 0.2843 - val_acc: 0.9668 - 505ms/epoch - 505ms/step\n",
            "Epoch 69/200\n",
            "1/1 - 1s - loss: 0.2800 - acc: 0.9631 - val_loss: 0.2785 - val_acc: 0.9668 - 531ms/epoch - 531ms/step\n",
            "Epoch 70/200\n",
            "1/1 - 1s - loss: 0.2697 - acc: 0.9420 - val_loss: 0.2730 - val_acc: 0.9668 - 503ms/epoch - 503ms/step\n",
            "Epoch 71/200\n",
            "1/1 - 1s - loss: 0.2559 - acc: 0.9679 - val_loss: 0.2676 - val_acc: 0.9668 - 514ms/epoch - 514ms/step\n",
            "Epoch 72/200\n",
            "1/1 - 0s - loss: 0.2564 - acc: 0.9674 - val_loss: 0.2624 - val_acc: 0.9668 - 476ms/epoch - 476ms/step\n",
            "Epoch 73/200\n",
            "1/1 - 1s - loss: 0.2512 - acc: 0.9268 - val_loss: 0.2574 - val_acc: 0.9668 - 617ms/epoch - 617ms/step\n",
            "Epoch 74/200\n",
            "1/1 - 1s - loss: 0.2432 - acc: 0.9460 - val_loss: 0.2526 - val_acc: 0.9701 - 522ms/epoch - 522ms/step\n",
            "Epoch 75/200\n",
            "1/1 - 1s - loss: 0.2486 - acc: 0.9676 - val_loss: 0.2479 - val_acc: 0.9701 - 507ms/epoch - 507ms/step\n",
            "Epoch 76/200\n",
            "1/1 - 1s - loss: 0.2362 - acc: 0.9676 - val_loss: 0.2433 - val_acc: 0.9701 - 520ms/epoch - 520ms/step\n",
            "Epoch 77/200\n",
            "1/1 - 1s - loss: 0.2329 - acc: 0.9761 - val_loss: 0.2389 - val_acc: 0.9701 - 514ms/epoch - 514ms/step\n",
            "Epoch 78/200\n",
            "1/1 - 1s - loss: 0.2327 - acc: 0.9682 - val_loss: 0.2345 - val_acc: 0.9701 - 519ms/epoch - 519ms/step\n",
            "Epoch 79/200\n",
            "1/1 - 1s - loss: 0.2278 - acc: 0.9730 - val_loss: 0.2304 - val_acc: 0.9734 - 608ms/epoch - 608ms/step\n",
            "Epoch 80/200\n",
            "1/1 - 1s - loss: 0.2222 - acc: 0.9699 - val_loss: 0.2263 - val_acc: 0.9734 - 989ms/epoch - 989ms/step\n",
            "Epoch 81/200\n",
            "1/1 - 1s - loss: 0.2163 - acc: 0.9690 - val_loss: 0.2223 - val_acc: 0.9734 - 899ms/epoch - 899ms/step\n",
            "Epoch 82/200\n",
            "1/1 - 1s - loss: 0.2192 - acc: 0.9797 - val_loss: 0.2183 - val_acc: 0.9734 - 1s/epoch - 1s/step\n",
            "Epoch 83/200\n",
            "1/1 - 1s - loss: 0.2098 - acc: 0.9690 - val_loss: 0.2143 - val_acc: 0.9734 - 912ms/epoch - 912ms/step\n",
            "Epoch 84/200\n",
            "1/1 - 1s - loss: 0.2035 - acc: 0.9783 - val_loss: 0.2104 - val_acc: 0.9734 - 1s/epoch - 1s/step\n",
            "Epoch 85/200\n",
            "1/1 - 1s - loss: 0.2038 - acc: 0.9803 - val_loss: 0.2066 - val_acc: 0.9734 - 778ms/epoch - 778ms/step\n",
            "Epoch 86/200\n",
            "1/1 - 1s - loss: 0.1983 - acc: 0.9786 - val_loss: 0.2028 - val_acc: 0.9734 - 610ms/epoch - 610ms/step\n",
            "Epoch 87/200\n",
            "1/1 - 0s - loss: 0.1929 - acc: 0.9786 - val_loss: 0.1990 - val_acc: 0.9734 - 487ms/epoch - 487ms/step\n",
            "Epoch 88/200\n",
            "1/1 - 1s - loss: 0.1880 - acc: 0.9710 - val_loss: 0.1953 - val_acc: 0.9734 - 525ms/epoch - 525ms/step\n",
            "Epoch 89/200\n",
            "1/1 - 1s - loss: 0.1875 - acc: 0.9778 - val_loss: 0.1916 - val_acc: 0.9734 - 1s/epoch - 1s/step\n",
            "Epoch 90/200\n",
            "1/1 - 1s - loss: 0.1799 - acc: 0.9786 - val_loss: 0.1881 - val_acc: 0.9734 - 1s/epoch - 1s/step\n",
            "Epoch 91/200\n",
            "1/1 - 1s - loss: 0.1782 - acc: 0.9764 - val_loss: 0.1846 - val_acc: 0.9734 - 935ms/epoch - 935ms/step\n",
            "Epoch 92/200\n",
            "1/1 - 1s - loss: 0.1727 - acc: 0.9783 - val_loss: 0.1813 - val_acc: 0.9734 - 521ms/epoch - 521ms/step\n",
            "Epoch 93/200\n",
            "1/1 - 1s - loss: 0.1708 - acc: 0.9786 - val_loss: 0.1780 - val_acc: 0.9734 - 618ms/epoch - 618ms/step\n",
            "Epoch 94/200\n",
            "1/1 - 1s - loss: 0.1658 - acc: 0.9769 - val_loss: 0.1748 - val_acc: 0.9734 - 507ms/epoch - 507ms/step\n",
            "Epoch 95/200\n",
            "1/1 - 1s - loss: 0.1681 - acc: 0.9783 - val_loss: 0.1718 - val_acc: 0.9734 - 615ms/epoch - 615ms/step\n",
            "Epoch 96/200\n",
            "1/1 - 1s - loss: 0.1597 - acc: 0.9789 - val_loss: 0.1688 - val_acc: 0.9734 - 510ms/epoch - 510ms/step\n",
            "Epoch 97/200\n",
            "1/1 - 1s - loss: 0.1568 - acc: 0.9797 - val_loss: 0.1659 - val_acc: 0.9734 - 614ms/epoch - 614ms/step\n",
            "Epoch 98/200\n",
            "1/1 - 1s - loss: 0.1532 - acc: 0.9789 - val_loss: 0.1631 - val_acc: 0.9734 - 733ms/epoch - 733ms/step\n",
            "Epoch 99/200\n",
            "1/1 - 1s - loss: 0.1519 - acc: 0.9789 - val_loss: 0.1604 - val_acc: 0.9734 - 1s/epoch - 1s/step\n",
            "Epoch 100/200\n",
            "1/1 - 1s - loss: 0.1516 - acc: 0.9789 - val_loss: 0.1578 - val_acc: 0.9734 - 1s/epoch - 1s/step\n",
            "Epoch 101/200\n",
            "1/1 - 1s - loss: 0.1469 - acc: 0.9789 - val_loss: 0.1552 - val_acc: 0.9734 - 938ms/epoch - 938ms/step\n",
            "Epoch 102/200\n",
            "1/1 - 1s - loss: 0.1459 - acc: 0.9783 - val_loss: 0.1528 - val_acc: 0.9734 - 771ms/epoch - 771ms/step\n",
            "Epoch 103/200\n",
            "1/1 - 1s - loss: 0.1446 - acc: 0.9792 - val_loss: 0.1504 - val_acc: 0.9734 - 768ms/epoch - 768ms/step\n",
            "Epoch 104/200\n",
            "1/1 - 1s - loss: 0.1347 - acc: 0.9786 - val_loss: 0.1481 - val_acc: 0.9734 - 706ms/epoch - 706ms/step\n",
            "Epoch 105/200\n",
            "1/1 - 1s - loss: 0.1381 - acc: 0.9792 - val_loss: 0.1459 - val_acc: 0.9734 - 608ms/epoch - 608ms/step\n",
            "Epoch 106/200\n",
            "1/1 - 1s - loss: 0.1406 - acc: 0.9792 - val_loss: 0.1438 - val_acc: 0.9734 - 524ms/epoch - 524ms/step\n",
            "Epoch 107/200\n",
            "1/1 - 1s - loss: 0.1366 - acc: 0.9792 - val_loss: 0.1417 - val_acc: 0.9734 - 521ms/epoch - 521ms/step\n",
            "Epoch 108/200\n",
            "1/1 - 1s - loss: 0.1333 - acc: 0.9789 - val_loss: 0.1397 - val_acc: 0.9734 - 610ms/epoch - 610ms/step\n",
            "Epoch 109/200\n",
            "1/1 - 1s - loss: 0.1374 - acc: 0.9783 - val_loss: 0.1378 - val_acc: 0.9734 - 516ms/epoch - 516ms/step\n",
            "Epoch 110/200\n",
            "1/1 - 1s - loss: 0.1284 - acc: 0.9789 - val_loss: 0.1360 - val_acc: 0.9734 - 525ms/epoch - 525ms/step\n",
            "Epoch 111/200\n",
            "1/1 - 1s - loss: 0.1299 - acc: 0.9789 - val_loss: 0.1342 - val_acc: 0.9734 - 518ms/epoch - 518ms/step\n",
            "Epoch 112/200\n",
            "1/1 - 1s - loss: 0.1250 - acc: 0.9789 - val_loss: 0.1325 - val_acc: 0.9734 - 516ms/epoch - 516ms/step\n",
            "Epoch 113/200\n",
            "1/1 - 1s - loss: 0.1250 - acc: 0.9792 - val_loss: 0.1308 - val_acc: 0.9734 - 617ms/epoch - 617ms/step\n",
            "Epoch 114/200\n",
            "1/1 - 0s - loss: 0.1218 - acc: 0.9780 - val_loss: 0.1293 - val_acc: 0.9734 - 488ms/epoch - 488ms/step\n",
            "Epoch 115/200\n",
            "1/1 - 1s - loss: 0.1191 - acc: 0.9789 - val_loss: 0.1277 - val_acc: 0.9734 - 609ms/epoch - 609ms/step\n",
            "Epoch 116/200\n",
            "1/1 - 1s - loss: 0.1194 - acc: 0.9803 - val_loss: 0.1263 - val_acc: 0.9734 - 625ms/epoch - 625ms/step\n",
            "Epoch 117/200\n",
            "1/1 - 1s - loss: 0.1175 - acc: 0.9780 - val_loss: 0.1248 - val_acc: 0.9734 - 514ms/epoch - 514ms/step\n",
            "Epoch 118/200\n",
            "1/1 - 1s - loss: 0.1213 - acc: 0.9786 - val_loss: 0.1235 - val_acc: 0.9734 - 770ms/epoch - 770ms/step\n",
            "Epoch 119/200\n",
            "1/1 - 1s - loss: 0.1131 - acc: 0.9789 - val_loss: 0.1222 - val_acc: 0.9734 - 1s/epoch - 1s/step\n",
            "Epoch 120/200\n",
            "1/1 - 1s - loss: 0.1159 - acc: 0.9786 - val_loss: 0.1209 - val_acc: 0.9734 - 946ms/epoch - 946ms/step\n",
            "Epoch 121/200\n",
            "1/1 - 1s - loss: 0.1122 - acc: 0.9783 - val_loss: 0.1197 - val_acc: 0.9734 - 1s/epoch - 1s/step\n",
            "Epoch 122/200\n",
            "1/1 - 1s - loss: 0.1071 - acc: 0.9800 - val_loss: 0.1185 - val_acc: 0.9734 - 722ms/epoch - 722ms/step\n",
            "Epoch 123/200\n",
            "1/1 - 1s - loss: 0.1130 - acc: 0.9780 - val_loss: 0.1174 - val_acc: 0.9734 - 786ms/epoch - 786ms/step\n",
            "Epoch 124/200\n",
            "1/1 - 1s - loss: 0.1074 - acc: 0.9789 - val_loss: 0.1163 - val_acc: 0.9734 - 615ms/epoch - 615ms/step\n",
            "Epoch 125/200\n",
            "1/1 - 1s - loss: 0.1078 - acc: 0.9789 - val_loss: 0.1152 - val_acc: 0.9734 - 526ms/epoch - 526ms/step\n",
            "Epoch 126/200\n",
            "1/1 - 1s - loss: 0.1029 - acc: 0.9789 - val_loss: 0.1142 - val_acc: 0.9734 - 537ms/epoch - 537ms/step\n",
            "Epoch 127/200\n",
            "1/1 - 1s - loss: 0.1027 - acc: 0.9789 - val_loss: 0.1132 - val_acc: 0.9734 - 508ms/epoch - 508ms/step\n",
            "Epoch 128/200\n",
            "1/1 - 1s - loss: 0.1064 - acc: 0.9783 - val_loss: 0.1122 - val_acc: 0.9734 - 517ms/epoch - 517ms/step\n",
            "Epoch 129/200\n",
            "1/1 - 1s - loss: 0.1054 - acc: 0.9786 - val_loss: 0.1113 - val_acc: 0.9734 - 512ms/epoch - 512ms/step\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 0.2425 - acc: 0.9724\n",
            "\n",
            "Test Set Metrics:\n",
            "\tloss: 0.2425\n",
            "\tacc: 0.9724\n",
            "1/1 [==============================] - 0s 338ms/step\n",
            "Using GCN (local pooling) filters...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1/1 - 2s - loss: 1.4524 - acc: 0.0070 - val_loss: 1.4338 - val_acc: 0.0100 - 2s/epoch - 2s/step\n",
            "Epoch 2/200\n",
            "1/1 - 0s - loss: 1.4269 - acc: 0.0051 - val_loss: 1.4132 - val_acc: 0.0100 - 470ms/epoch - 470ms/step\n",
            "Epoch 3/200\n",
            "1/1 - 0s - loss: 1.4160 - acc: 0.0172 - val_loss: 1.3928 - val_acc: 0.0299 - 471ms/epoch - 471ms/step\n",
            "Epoch 4/200\n",
            "1/1 - 1s - loss: 1.4033 - acc: 0.1323 - val_loss: 1.3725 - val_acc: 0.1196 - 563ms/epoch - 563ms/step\n",
            "Epoch 5/200\n",
            "1/1 - 1s - loss: 1.3780 - acc: 0.1179 - val_loss: 1.3523 - val_acc: 0.6611 - 572ms/epoch - 572ms/step\n",
            "Epoch 6/200\n",
            "1/1 - 1s - loss: 1.3565 - acc: 0.5376 - val_loss: 1.3322 - val_acc: 0.6844 - 571ms/epoch - 571ms/step\n",
            "Epoch 7/200\n",
            "1/1 - 1s - loss: 1.3400 - acc: 0.7087 - val_loss: 1.3122 - val_acc: 0.8106 - 564ms/epoch - 564ms/step\n",
            "Epoch 8/200\n",
            "1/1 - 0s - loss: 1.3215 - acc: 0.8401 - val_loss: 1.2918 - val_acc: 0.8738 - 481ms/epoch - 481ms/step\n",
            "Epoch 9/200\n",
            "1/1 - 1s - loss: 1.3035 - acc: 0.8033 - val_loss: 1.2722 - val_acc: 0.8738 - 557ms/epoch - 557ms/step\n",
            "Epoch 10/200\n",
            "1/1 - 0s - loss: 1.2786 - acc: 0.8635 - val_loss: 1.2528 - val_acc: 0.8738 - 438ms/epoch - 438ms/step\n",
            "Epoch 11/200\n",
            "1/1 - 0s - loss: 1.2567 - acc: 0.8672 - val_loss: 1.2335 - val_acc: 0.8738 - 468ms/epoch - 468ms/step\n",
            "Epoch 12/200\n",
            "1/1 - 0s - loss: 1.2238 - acc: 0.8545 - val_loss: 1.2144 - val_acc: 0.8738 - 459ms/epoch - 459ms/step\n",
            "Epoch 13/200\n",
            "1/1 - 0s - loss: 1.2250 - acc: 0.8674 - val_loss: 1.1953 - val_acc: 0.8738 - 471ms/epoch - 471ms/step\n",
            "Epoch 14/200\n",
            "1/1 - 1s - loss: 1.1989 - acc: 0.8657 - val_loss: 1.1765 - val_acc: 0.8738 - 565ms/epoch - 565ms/step\n",
            "Epoch 15/200\n",
            "1/1 - 1s - loss: 1.1795 - acc: 0.8711 - val_loss: 1.1581 - val_acc: 0.8738 - 1s/epoch - 1s/step\n",
            "Epoch 16/200\n",
            "1/1 - 1s - loss: 1.1566 - acc: 0.8705 - val_loss: 1.1402 - val_acc: 0.8738 - 1s/epoch - 1s/step\n",
            "Epoch 17/200\n",
            "1/1 - 1s - loss: 1.1600 - acc: 0.8725 - val_loss: 1.1225 - val_acc: 0.8738 - 900ms/epoch - 900ms/step\n",
            "Epoch 18/200\n",
            "1/1 - 1s - loss: 1.1191 - acc: 0.8725 - val_loss: 1.1049 - val_acc: 0.8738 - 735ms/epoch - 735ms/step\n",
            "Epoch 19/200\n",
            "1/1 - 1s - loss: 1.1085 - acc: 0.8725 - val_loss: 1.0873 - val_acc: 0.8738 - 733ms/epoch - 733ms/step\n",
            "Epoch 20/200\n",
            "1/1 - 1s - loss: 1.0888 - acc: 0.8722 - val_loss: 1.0697 - val_acc: 0.8738 - 713ms/epoch - 713ms/step\n",
            "Epoch 21/200\n",
            "1/1 - 1s - loss: 1.0813 - acc: 0.8722 - val_loss: 1.0520 - val_acc: 0.8738 - 725ms/epoch - 725ms/step\n",
            "Epoch 22/200\n",
            "1/1 - 1s - loss: 1.0582 - acc: 0.8725 - val_loss: 1.0343 - val_acc: 0.8738 - 560ms/epoch - 560ms/step\n",
            "Epoch 23/200\n",
            "1/1 - 0s - loss: 1.0313 - acc: 0.8725 - val_loss: 1.0177 - val_acc: 0.8738 - 463ms/epoch - 463ms/step\n",
            "Epoch 24/200\n",
            "1/1 - 1s - loss: 1.0284 - acc: 0.8731 - val_loss: 1.0022 - val_acc: 0.8738 - 568ms/epoch - 568ms/step\n",
            "Epoch 25/200\n",
            "1/1 - 1s - loss: 1.0020 - acc: 0.8725 - val_loss: 0.9869 - val_acc: 0.8738 - 568ms/epoch - 568ms/step\n",
            "Epoch 26/200\n",
            "1/1 - 0s - loss: 0.9889 - acc: 0.8725 - val_loss: 0.9717 - val_acc: 0.8738 - 443ms/epoch - 443ms/step\n",
            "Epoch 27/200\n",
            "1/1 - 1s - loss: 0.9797 - acc: 0.8725 - val_loss: 0.9566 - val_acc: 0.8738 - 563ms/epoch - 563ms/step\n",
            "Epoch 28/200\n",
            "1/1 - 0s - loss: 0.9545 - acc: 0.8731 - val_loss: 0.9419 - val_acc: 0.8738 - 482ms/epoch - 482ms/step\n",
            "Epoch 29/200\n",
            "1/1 - 0s - loss: 0.9430 - acc: 0.8753 - val_loss: 0.9273 - val_acc: 0.8738 - 465ms/epoch - 465ms/step\n",
            "Epoch 30/200\n",
            "1/1 - 1s - loss: 0.9325 - acc: 0.8728 - val_loss: 0.9126 - val_acc: 0.8738 - 576ms/epoch - 576ms/step\n",
            "Epoch 31/200\n",
            "1/1 - 1s - loss: 0.9117 - acc: 0.8725 - val_loss: 0.8979 - val_acc: 0.8738 - 558ms/epoch - 558ms/step\n",
            "Epoch 32/200\n",
            "1/1 - 1s - loss: 0.9098 - acc: 0.8725 - val_loss: 0.8832 - val_acc: 0.8738 - 560ms/epoch - 560ms/step\n",
            "Epoch 33/200\n",
            "1/1 - 0s - loss: 0.8824 - acc: 0.8725 - val_loss: 0.8684 - val_acc: 0.8738 - 465ms/epoch - 465ms/step\n",
            "Epoch 34/200\n",
            "1/1 - 1s - loss: 0.8651 - acc: 0.8725 - val_loss: 0.8536 - val_acc: 0.8738 - 573ms/epoch - 573ms/step\n",
            "Epoch 35/200\n",
            "1/1 - 0s - loss: 0.8548 - acc: 0.8739 - val_loss: 0.8388 - val_acc: 0.8738 - 481ms/epoch - 481ms/step\n",
            "Epoch 36/200\n",
            "1/1 - 1s - loss: 0.8404 - acc: 0.8914 - val_loss: 0.8239 - val_acc: 0.8738 - 558ms/epoch - 558ms/step\n",
            "Epoch 37/200\n",
            "1/1 - 1s - loss: 0.8214 - acc: 0.8798 - val_loss: 0.8090 - val_acc: 0.8738 - 567ms/epoch - 567ms/step\n",
            "Epoch 38/200\n",
            "1/1 - 0s - loss: 0.8026 - acc: 0.8973 - val_loss: 0.7941 - val_acc: 0.8738 - 460ms/epoch - 460ms/step\n",
            "Epoch 39/200\n",
            "1/1 - 1s - loss: 0.7861 - acc: 0.8798 - val_loss: 0.7791 - val_acc: 0.8738 - 709ms/epoch - 709ms/step\n",
            "Epoch 40/200\n",
            "1/1 - 1s - loss: 0.7907 - acc: 0.8801 - val_loss: 0.7638 - val_acc: 0.8738 - 1s/epoch - 1s/step\n",
            "Epoch 41/200\n",
            "1/1 - 1s - loss: 0.7633 - acc: 0.8973 - val_loss: 0.7485 - val_acc: 0.8738 - 1s/epoch - 1s/step\n",
            "Epoch 42/200\n",
            "1/1 - 1s - loss: 0.7494 - acc: 0.8784 - val_loss: 0.7333 - val_acc: 0.8738 - 1s/epoch - 1s/step\n",
            "Epoch 43/200\n",
            "1/1 - 1s - loss: 0.7452 - acc: 0.8725 - val_loss: 0.7181 - val_acc: 0.8738 - 718ms/epoch - 718ms/step\n",
            "Epoch 44/200\n",
            "1/1 - 1s - loss: 0.7297 - acc: 0.8731 - val_loss: 0.7030 - val_acc: 0.8738 - 712ms/epoch - 712ms/step\n",
            "Epoch 45/200\n",
            "1/1 - 1s - loss: 0.7012 - acc: 0.8798 - val_loss: 0.6879 - val_acc: 0.8738 - 635ms/epoch - 635ms/step\n",
            "Epoch 46/200\n",
            "1/1 - 0s - loss: 0.6992 - acc: 0.8733 - val_loss: 0.6730 - val_acc: 0.8738 - 471ms/epoch - 471ms/step\n",
            "Epoch 47/200\n",
            "1/1 - 1s - loss: 0.6752 - acc: 0.8728 - val_loss: 0.6582 - val_acc: 0.8738 - 566ms/epoch - 566ms/step\n",
            "Epoch 48/200\n",
            "1/1 - 0s - loss: 0.6640 - acc: 0.9046 - val_loss: 0.6435 - val_acc: 0.8738 - 476ms/epoch - 476ms/step\n",
            "Epoch 49/200\n",
            "1/1 - 0s - loss: 0.6579 - acc: 0.8725 - val_loss: 0.6290 - val_acc: 0.8738 - 456ms/epoch - 456ms/step\n",
            "Epoch 50/200\n",
            "1/1 - 0s - loss: 0.6244 - acc: 0.8961 - val_loss: 0.6147 - val_acc: 0.9468 - 475ms/epoch - 475ms/step\n",
            "Epoch 51/200\n",
            "1/1 - 0s - loss: 0.6190 - acc: 0.8731 - val_loss: 0.6006 - val_acc: 0.9468 - 472ms/epoch - 472ms/step\n",
            "Epoch 52/200\n",
            "1/1 - 0s - loss: 0.6182 - acc: 0.8973 - val_loss: 0.5865 - val_acc: 0.9535 - 473ms/epoch - 473ms/step\n",
            "Epoch 53/200\n",
            "1/1 - 1s - loss: 0.5917 - acc: 0.9527 - val_loss: 0.5720 - val_acc: 0.9535 - 561ms/epoch - 561ms/step\n",
            "Epoch 54/200\n",
            "1/1 - 1s - loss: 0.5660 - acc: 0.9046 - val_loss: 0.5577 - val_acc: 0.9535 - 567ms/epoch - 567ms/step\n",
            "Epoch 55/200\n",
            "1/1 - 0s - loss: 0.5687 - acc: 0.8978 - val_loss: 0.5437 - val_acc: 0.9535 - 486ms/epoch - 486ms/step\n",
            "Epoch 56/200\n",
            "1/1 - 1s - loss: 0.5449 - acc: 0.9451 - val_loss: 0.5298 - val_acc: 0.9535 - 563ms/epoch - 563ms/step\n",
            "Epoch 57/200\n",
            "1/1 - 0s - loss: 0.5416 - acc: 0.9220 - val_loss: 0.5159 - val_acc: 0.9535 - 475ms/epoch - 475ms/step\n",
            "Epoch 58/200\n",
            "1/1 - 1s - loss: 0.5278 - acc: 0.9451 - val_loss: 0.5021 - val_acc: 0.9535 - 569ms/epoch - 569ms/step\n",
            "Epoch 59/200\n",
            "1/1 - 1s - loss: 0.5060 - acc: 0.9524 - val_loss: 0.4886 - val_acc: 0.9535 - 560ms/epoch - 560ms/step\n",
            "Epoch 60/200\n",
            "1/1 - 1s - loss: 0.5027 - acc: 0.9203 - val_loss: 0.4753 - val_acc: 0.9535 - 567ms/epoch - 567ms/step\n",
            "Epoch 61/200\n",
            "1/1 - 0s - loss: 0.4773 - acc: 0.9617 - val_loss: 0.4623 - val_acc: 0.9535 - 470ms/epoch - 470ms/step\n",
            "Epoch 62/200\n",
            "1/1 - 1s - loss: 0.4597 - acc: 0.9533 - val_loss: 0.4496 - val_acc: 0.9535 - 573ms/epoch - 573ms/step\n",
            "Epoch 63/200\n",
            "1/1 - 1s - loss: 0.4549 - acc: 0.9527 - val_loss: 0.4371 - val_acc: 0.9535 - 851ms/epoch - 851ms/step\n",
            "Epoch 64/200\n",
            "1/1 - 1s - loss: 0.4416 - acc: 0.9524 - val_loss: 0.4248 - val_acc: 0.9535 - 1s/epoch - 1s/step\n",
            "Epoch 65/200\n",
            "1/1 - 1s - loss: 0.4384 - acc: 0.9527 - val_loss: 0.4129 - val_acc: 0.9535 - 876ms/epoch - 876ms/step\n",
            "Epoch 66/200\n",
            "1/1 - 1s - loss: 0.4258 - acc: 0.9291 - val_loss: 0.4007 - val_acc: 0.9535 - 1s/epoch - 1s/step\n",
            "Epoch 67/200\n",
            "1/1 - 1s - loss: 0.4120 - acc: 0.9476 - val_loss: 0.3882 - val_acc: 0.9535 - 716ms/epoch - 716ms/step\n",
            "Epoch 68/200\n",
            "1/1 - 1s - loss: 0.3995 - acc: 0.9631 - val_loss: 0.3761 - val_acc: 0.9535 - 558ms/epoch - 558ms/step\n",
            "Epoch 69/200\n",
            "1/1 - 0s - loss: 0.3842 - acc: 0.9474 - val_loss: 0.3645 - val_acc: 0.9535 - 474ms/epoch - 474ms/step\n",
            "Epoch 70/200\n",
            "1/1 - 0s - loss: 0.3663 - acc: 0.9628 - val_loss: 0.3533 - val_acc: 0.9535 - 476ms/epoch - 476ms/step\n",
            "Epoch 71/200\n",
            "1/1 - 1s - loss: 0.3613 - acc: 0.9530 - val_loss: 0.3425 - val_acc: 0.9535 - 575ms/epoch - 575ms/step\n",
            "Epoch 72/200\n",
            "1/1 - 0s - loss: 0.3566 - acc: 0.9533 - val_loss: 0.3321 - val_acc: 0.9535 - 475ms/epoch - 475ms/step\n",
            "Epoch 73/200\n",
            "1/1 - 1s - loss: 0.3445 - acc: 0.9550 - val_loss: 0.3221 - val_acc: 0.9601 - 557ms/epoch - 557ms/step\n",
            "Epoch 74/200\n",
            "1/1 - 1s - loss: 0.3277 - acc: 0.9617 - val_loss: 0.3123 - val_acc: 0.9601 - 558ms/epoch - 558ms/step\n",
            "Epoch 75/200\n",
            "1/1 - 1s - loss: 0.3189 - acc: 0.9479 - val_loss: 0.3029 - val_acc: 0.9601 - 568ms/epoch - 568ms/step\n",
            "Epoch 76/200\n",
            "1/1 - 0s - loss: 0.3107 - acc: 0.9527 - val_loss: 0.2938 - val_acc: 0.9668 - 470ms/epoch - 470ms/step\n",
            "Epoch 77/200\n",
            "1/1 - 0s - loss: 0.3088 - acc: 0.9482 - val_loss: 0.2851 - val_acc: 0.9668 - 479ms/epoch - 479ms/step\n",
            "Epoch 78/200\n",
            "1/1 - 1s - loss: 0.2959 - acc: 0.9609 - val_loss: 0.2767 - val_acc: 0.9668 - 568ms/epoch - 568ms/step\n",
            "Epoch 79/200\n",
            "1/1 - 1s - loss: 0.2875 - acc: 0.9713 - val_loss: 0.2688 - val_acc: 0.9668 - 576ms/epoch - 576ms/step\n",
            "Epoch 80/200\n",
            "1/1 - 0s - loss: 0.2813 - acc: 0.9651 - val_loss: 0.2613 - val_acc: 0.9767 - 428ms/epoch - 428ms/step\n",
            "Epoch 81/200\n",
            "1/1 - 0s - loss: 0.2777 - acc: 0.9614 - val_loss: 0.2541 - val_acc: 0.9767 - 472ms/epoch - 472ms/step\n",
            "Epoch 82/200\n",
            "1/1 - 0s - loss: 0.2631 - acc: 0.9648 - val_loss: 0.2472 - val_acc: 0.9767 - 478ms/epoch - 478ms/step\n",
            "Epoch 83/200\n",
            "1/1 - 1s - loss: 0.2604 - acc: 0.9651 - val_loss: 0.2406 - val_acc: 0.9767 - 565ms/epoch - 565ms/step\n",
            "Epoch 84/200\n",
            "1/1 - 1s - loss: 0.2549 - acc: 0.9676 - val_loss: 0.2343 - val_acc: 0.9767 - 568ms/epoch - 568ms/step\n",
            "Epoch 85/200\n",
            "1/1 - 1s - loss: 0.2456 - acc: 0.9657 - val_loss: 0.2283 - val_acc: 0.9767 - 822ms/epoch - 822ms/step\n",
            "Epoch 86/200\n",
            "1/1 - 1s - loss: 0.2315 - acc: 0.9721 - val_loss: 0.2226 - val_acc: 0.9767 - 1s/epoch - 1s/step\n",
            "Epoch 87/200\n",
            "1/1 - 1s - loss: 0.2347 - acc: 0.9769 - val_loss: 0.2172 - val_acc: 0.9767 - 905ms/epoch - 905ms/step\n",
            "Epoch 88/200\n",
            "1/1 - 1s - loss: 0.2234 - acc: 0.9775 - val_loss: 0.2120 - val_acc: 0.9801 - 1s/epoch - 1s/step\n",
            "Epoch 89/200\n",
            "1/1 - 1s - loss: 0.2205 - acc: 0.9761 - val_loss: 0.2070 - val_acc: 0.9801 - 714ms/epoch - 714ms/step\n",
            "Epoch 90/200\n",
            "1/1 - 1s - loss: 0.2139 - acc: 0.9741 - val_loss: 0.2022 - val_acc: 0.9801 - 724ms/epoch - 724ms/step\n",
            "Epoch 91/200\n",
            "1/1 - 1s - loss: 0.2147 - acc: 0.9747 - val_loss: 0.1977 - val_acc: 0.9801 - 729ms/epoch - 729ms/step\n",
            "Epoch 92/200\n",
            "1/1 - 1s - loss: 0.2127 - acc: 0.9752 - val_loss: 0.1933 - val_acc: 0.9801 - 646ms/epoch - 646ms/step\n",
            "Epoch 93/200\n",
            "1/1 - 0s - loss: 0.1977 - acc: 0.9769 - val_loss: 0.1892 - val_acc: 0.9801 - 462ms/epoch - 462ms/step\n",
            "Epoch 94/200\n",
            "1/1 - 0s - loss: 0.2004 - acc: 0.9772 - val_loss: 0.1852 - val_acc: 0.9801 - 478ms/epoch - 478ms/step\n",
            "Epoch 95/200\n",
            "1/1 - 1s - loss: 0.2006 - acc: 0.9769 - val_loss: 0.1814 - val_acc: 0.9801 - 560ms/epoch - 560ms/step\n",
            "Epoch 96/200\n",
            "1/1 - 0s - loss: 0.1979 - acc: 0.9772 - val_loss: 0.1777 - val_acc: 0.9801 - 451ms/epoch - 451ms/step\n",
            "Epoch 97/200\n",
            "1/1 - 0s - loss: 0.1863 - acc: 0.9764 - val_loss: 0.1742 - val_acc: 0.9801 - 468ms/epoch - 468ms/step\n",
            "Epoch 98/200\n",
            "1/1 - 0s - loss: 0.1824 - acc: 0.9775 - val_loss: 0.1709 - val_acc: 0.9801 - 483ms/epoch - 483ms/step\n",
            "Epoch 99/200\n",
            "1/1 - 0s - loss: 0.1795 - acc: 0.9775 - val_loss: 0.1677 - val_acc: 0.9801 - 470ms/epoch - 470ms/step\n",
            "Epoch 100/200\n",
            "1/1 - 1s - loss: 0.1869 - acc: 0.9685 - val_loss: 0.1646 - val_acc: 0.9801 - 565ms/epoch - 565ms/step\n",
            "Epoch 101/200\n",
            "1/1 - 0s - loss: 0.1780 - acc: 0.9778 - val_loss: 0.1616 - val_acc: 0.9801 - 480ms/epoch - 480ms/step\n",
            "Epoch 102/200\n",
            "1/1 - 0s - loss: 0.1744 - acc: 0.9775 - val_loss: 0.1588 - val_acc: 0.9801 - 468ms/epoch - 468ms/step\n",
            "Epoch 103/200\n",
            "1/1 - 0s - loss: 0.1736 - acc: 0.9769 - val_loss: 0.1560 - val_acc: 0.9801 - 467ms/epoch - 467ms/step\n",
            "Epoch 104/200\n",
            "1/1 - 0s - loss: 0.1677 - acc: 0.9775 - val_loss: 0.1533 - val_acc: 0.9801 - 458ms/epoch - 458ms/step\n",
            "Epoch 105/200\n",
            "1/1 - 0s - loss: 0.1692 - acc: 0.9766 - val_loss: 0.1507 - val_acc: 0.9801 - 475ms/epoch - 475ms/step\n",
            "Epoch 106/200\n",
            "1/1 - 0s - loss: 0.1684 - acc: 0.9769 - val_loss: 0.1482 - val_acc: 0.9801 - 468ms/epoch - 468ms/step\n",
            "Epoch 107/200\n",
            "1/1 - 1s - loss: 0.1560 - acc: 0.9772 - val_loss: 0.1458 - val_acc: 0.9801 - 568ms/epoch - 568ms/step\n",
            "Epoch 108/200\n",
            "1/1 - 1s - loss: 0.1604 - acc: 0.9766 - val_loss: 0.1435 - val_acc: 0.9801 - 578ms/epoch - 578ms/step\n",
            "Epoch 109/200\n",
            "1/1 - 0s - loss: 0.1537 - acc: 0.9772 - val_loss: 0.1413 - val_acc: 0.9801 - 470ms/epoch - 470ms/step\n",
            "Epoch 110/200\n",
            "1/1 - 1s - loss: 0.1538 - acc: 0.9775 - val_loss: 0.1392 - val_acc: 0.9801 - 802ms/epoch - 802ms/step\n",
            "Epoch 111/200\n",
            "1/1 - 1s - loss: 0.1484 - acc: 0.9772 - val_loss: 0.1372 - val_acc: 0.9801 - 1s/epoch - 1s/step\n",
            "Epoch 112/200\n",
            "1/1 - 1s - loss: 0.1525 - acc: 0.9775 - val_loss: 0.1352 - val_acc: 0.9801 - 849ms/epoch - 849ms/step\n",
            "Epoch 113/200\n",
            "1/1 - 1s - loss: 0.1502 - acc: 0.9769 - val_loss: 0.1333 - val_acc: 0.9801 - 1s/epoch - 1s/step\n",
            "Epoch 114/200\n",
            "1/1 - 1s - loss: 0.1378 - acc: 0.9778 - val_loss: 0.1315 - val_acc: 0.9801 - 1s/epoch - 1s/step\n",
            "Epoch 115/200\n",
            "1/1 - 1s - loss: 0.1453 - acc: 0.9778 - val_loss: 0.1298 - val_acc: 0.9801 - 680ms/epoch - 680ms/step\n",
            "Epoch 116/200\n",
            "1/1 - 0s - loss: 0.1374 - acc: 0.9775 - val_loss: 0.1281 - val_acc: 0.9801 - 469ms/epoch - 469ms/step\n",
            "Epoch 117/200\n",
            "1/1 - 1s - loss: 0.1365 - acc: 0.9769 - val_loss: 0.1265 - val_acc: 0.9801 - 582ms/epoch - 582ms/step\n",
            "Epoch 118/200\n",
            "1/1 - 1s - loss: 0.1404 - acc: 0.9778 - val_loss: 0.1250 - val_acc: 0.9801 - 562ms/epoch - 562ms/step\n",
            "Epoch 119/200\n",
            "1/1 - 0s - loss: 0.1384 - acc: 0.9778 - val_loss: 0.1235 - val_acc: 0.9801 - 485ms/epoch - 485ms/step\n",
            "Epoch 120/200\n",
            "1/1 - 0s - loss: 0.1362 - acc: 0.9778 - val_loss: 0.1220 - val_acc: 0.9801 - 449ms/epoch - 449ms/step\n",
            "Epoch 121/200\n",
            "1/1 - 0s - loss: 0.1316 - acc: 0.9772 - val_loss: 0.1204 - val_acc: 0.9801 - 471ms/epoch - 471ms/step\n",
            "Epoch 122/200\n",
            "1/1 - 1s - loss: 0.1339 - acc: 0.9775 - val_loss: 0.1189 - val_acc: 0.9801 - 568ms/epoch - 568ms/step\n",
            "Epoch 123/200\n",
            "1/1 - 1s - loss: 0.1296 - acc: 0.9775 - val_loss: 0.1175 - val_acc: 0.9801 - 563ms/epoch - 563ms/step\n",
            "Epoch 124/200\n",
            "1/1 - 1s - loss: 0.1282 - acc: 0.9772 - val_loss: 0.1161 - val_acc: 0.9801 - 578ms/epoch - 578ms/step\n",
            "Epoch 125/200\n",
            "1/1 - 1s - loss: 0.1237 - acc: 0.9764 - val_loss: 0.1148 - val_acc: 0.9801 - 568ms/epoch - 568ms/step\n",
            "Epoch 126/200\n",
            "1/1 - 0s - loss: 0.1240 - acc: 0.9769 - val_loss: 0.1136 - val_acc: 0.9801 - 478ms/epoch - 478ms/step\n",
            "Epoch 127/200\n",
            "1/1 - 1s - loss: 0.1269 - acc: 0.9775 - val_loss: 0.1124 - val_acc: 0.9801 - 570ms/epoch - 570ms/step\n",
            "Epoch 128/200\n",
            "1/1 - 0s - loss: 0.1153 - acc: 0.9778 - val_loss: 0.1113 - val_acc: 0.9801 - 474ms/epoch - 474ms/step\n",
            "Epoch 129/200\n",
            "1/1 - 0s - loss: 0.1226 - acc: 0.9764 - val_loss: 0.1102 - val_acc: 0.9801 - 482ms/epoch - 482ms/step\n",
            "Epoch 130/200\n",
            "1/1 - 1s - loss: 0.1202 - acc: 0.9780 - val_loss: 0.1092 - val_acc: 0.9801 - 569ms/epoch - 569ms/step\n",
            "Epoch 131/200\n",
            "1/1 - 0s - loss: 0.1189 - acc: 0.9775 - val_loss: 0.1082 - val_acc: 0.9801 - 467ms/epoch - 467ms/step\n",
            "Epoch 132/200\n",
            "1/1 - 1s - loss: 0.1186 - acc: 0.9772 - val_loss: 0.1073 - val_acc: 0.9801 - 753ms/epoch - 753ms/step\n",
            "Epoch 133/200\n",
            "1/1 - 1s - loss: 0.1170 - acc: 0.9778 - val_loss: 0.1065 - val_acc: 0.9801 - 896ms/epoch - 896ms/step\n",
            "Epoch 134/200\n",
            "1/1 - 1s - loss: 0.1116 - acc: 0.9775 - val_loss: 0.1056 - val_acc: 0.9801 - 878ms/epoch - 878ms/step\n",
            "Epoch 135/200\n",
            "1/1 - 1s - loss: 0.1136 - acc: 0.9769 - val_loss: 0.1048 - val_acc: 0.9801 - 1s/epoch - 1s/step\n",
            "Epoch 136/200\n",
            "1/1 - 1s - loss: 0.1187 - acc: 0.9778 - val_loss: 0.1041 - val_acc: 0.9801 - 783ms/epoch - 783ms/step\n",
            "Epoch 137/200\n",
            "1/1 - 1s - loss: 0.1166 - acc: 0.9775 - val_loss: 0.1034 - val_acc: 0.9801 - 1s/epoch - 1s/step\n",
            "Epoch 138/200\n",
            "1/1 - 1s - loss: 0.1139 - acc: 0.9778 - val_loss: 0.1027 - val_acc: 0.9801 - 727ms/epoch - 727ms/step\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.2230 - acc: 0.9779\n",
            "\n",
            "Test Set Metrics:\n",
            "\tloss: 0.2230\n",
            "\tacc: 0.9779\n",
            "1/1 [==============================] - 0s 341ms/step\n",
            "Using GCN (local pooling) filters...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1/1 - 3s - loss: 1.7325 - acc: 0.0059 - val_loss: 1.7032 - val_acc: 0.0000e+00 - 3s/epoch - 3s/step\n",
            "Epoch 2/200\n",
            "1/1 - 1s - loss: 1.7064 - acc: 0.0039 - val_loss: 1.6824 - val_acc: 0.0000e+00 - 749ms/epoch - 749ms/step\n",
            "Epoch 3/200\n",
            "1/1 - 0s - loss: 1.6813 - acc: 0.0056 - val_loss: 1.6620 - val_acc: 0.0000e+00 - 496ms/epoch - 496ms/step\n",
            "Epoch 4/200\n",
            "1/1 - 0s - loss: 1.6692 - acc: 0.0042 - val_loss: 1.6438 - val_acc: 0.0000e+00 - 496ms/epoch - 496ms/step\n",
            "Epoch 5/200\n",
            "1/1 - 1s - loss: 1.6616 - acc: 0.0042 - val_loss: 1.6261 - val_acc: 0.0000e+00 - 607ms/epoch - 607ms/step\n",
            "Epoch 6/200\n",
            "1/1 - 1s - loss: 1.6378 - acc: 0.0189 - val_loss: 1.6088 - val_acc: 0.0000e+00 - 600ms/epoch - 600ms/step\n",
            "Epoch 7/200\n",
            "1/1 - 1s - loss: 1.6158 - acc: 0.0180 - val_loss: 1.5922 - val_acc: 0.0000e+00 - 602ms/epoch - 602ms/step\n",
            "Epoch 8/200\n",
            "1/1 - 1s - loss: 1.6046 - acc: 0.0090 - val_loss: 1.5760 - val_acc: 0.0299 - 500ms/epoch - 500ms/step\n",
            "Epoch 9/200\n",
            "1/1 - 1s - loss: 1.5760 - acc: 0.1497 - val_loss: 1.5598 - val_acc: 0.0299 - 603ms/epoch - 603ms/step\n",
            "Epoch 10/200\n",
            "1/1 - 1s - loss: 1.5692 - acc: 0.1464 - val_loss: 1.5440 - val_acc: 0.0997 - 507ms/epoch - 507ms/step\n",
            "Epoch 11/200\n",
            "1/1 - 0s - loss: 1.5493 - acc: 0.1874 - val_loss: 1.5285 - val_acc: 0.2990 - 492ms/epoch - 492ms/step\n",
            "Epoch 12/200\n",
            "1/1 - 1s - loss: 1.5355 - acc: 0.1326 - val_loss: 1.5132 - val_acc: 0.2990 - 605ms/epoch - 605ms/step\n",
            "Epoch 13/200\n",
            "1/1 - 1s - loss: 1.5182 - acc: 0.2834 - val_loss: 1.4982 - val_acc: 0.3090 - 608ms/epoch - 608ms/step\n",
            "Epoch 14/200\n",
            "1/1 - 0s - loss: 1.5081 - acc: 0.2857 - val_loss: 1.4832 - val_acc: 0.3090 - 491ms/epoch - 491ms/step\n",
            "Epoch 15/200\n",
            "1/1 - 1s - loss: 1.4868 - acc: 0.3107 - val_loss: 1.4684 - val_acc: 0.3090 - 608ms/epoch - 608ms/step\n",
            "Epoch 16/200\n",
            "1/1 - 0s - loss: 1.4728 - acc: 0.3304 - val_loss: 1.4539 - val_acc: 0.8704 - 493ms/epoch - 493ms/step\n",
            "Epoch 17/200\n",
            "1/1 - 1s - loss: 1.4663 - acc: 0.4481 - val_loss: 1.4397 - val_acc: 0.8704 - 618ms/epoch - 618ms/step\n",
            "Epoch 18/200\n",
            "1/1 - 1s - loss: 1.4366 - acc: 0.8505 - val_loss: 1.4261 - val_acc: 0.8738 - 602ms/epoch - 602ms/step\n",
            "Epoch 19/200\n",
            "1/1 - 1s - loss: 1.4417 - acc: 0.8289 - val_loss: 1.4125 - val_acc: 0.8738 - 933ms/epoch - 933ms/step\n",
            "Epoch 20/200\n",
            "1/1 - 1s - loss: 1.4204 - acc: 0.8607 - val_loss: 1.3988 - val_acc: 0.8738 - 894ms/epoch - 894ms/step\n",
            "Epoch 21/200\n",
            "1/1 - 1s - loss: 1.3980 - acc: 0.8621 - val_loss: 1.3851 - val_acc: 0.8738 - 926ms/epoch - 926ms/step\n",
            "Epoch 22/200\n",
            "1/1 - 1s - loss: 1.3874 - acc: 0.8624 - val_loss: 1.3715 - val_acc: 0.8738 - 1s/epoch - 1s/step\n",
            "Epoch 23/200\n",
            "1/1 - 1s - loss: 1.3809 - acc: 0.8587 - val_loss: 1.3580 - val_acc: 0.8738 - 761ms/epoch - 761ms/step\n",
            "Epoch 24/200\n",
            "1/1 - 1s - loss: 1.3676 - acc: 0.8660 - val_loss: 1.3447 - val_acc: 0.8738 - 765ms/epoch - 765ms/step\n",
            "Epoch 25/200\n",
            "1/1 - 1s - loss: 1.3507 - acc: 0.8680 - val_loss: 1.3317 - val_acc: 0.8738 - 679ms/epoch - 679ms/step\n",
            "Epoch 26/200\n",
            "1/1 - 1s - loss: 1.3481 - acc: 0.8708 - val_loss: 1.3188 - val_acc: 0.8738 - 610ms/epoch - 610ms/step\n",
            "Epoch 27/200\n",
            "1/1 - 1s - loss: 1.3277 - acc: 0.8652 - val_loss: 1.3061 - val_acc: 0.8738 - 512ms/epoch - 512ms/step\n",
            "Epoch 28/200\n",
            "1/1 - 1s - loss: 1.3140 - acc: 0.8660 - val_loss: 1.2936 - val_acc: 0.8738 - 609ms/epoch - 609ms/step\n",
            "Epoch 29/200\n",
            "1/1 - 0s - loss: 1.3032 - acc: 0.8714 - val_loss: 1.2811 - val_acc: 0.8738 - 493ms/epoch - 493ms/step\n",
            "Epoch 30/200\n",
            "1/1 - 1s - loss: 1.2905 - acc: 0.8669 - val_loss: 1.2686 - val_acc: 0.8738 - 609ms/epoch - 609ms/step\n",
            "Epoch 31/200\n",
            "1/1 - 1s - loss: 1.2778 - acc: 0.8688 - val_loss: 1.2563 - val_acc: 0.8738 - 598ms/epoch - 598ms/step\n",
            "Epoch 32/200\n",
            "1/1 - 1s - loss: 1.2612 - acc: 0.8694 - val_loss: 1.2439 - val_acc: 0.8738 - 508ms/epoch - 508ms/step\n",
            "Epoch 33/200\n",
            "1/1 - 1s - loss: 1.2516 - acc: 0.8714 - val_loss: 1.2316 - val_acc: 0.8738 - 614ms/epoch - 614ms/step\n",
            "Epoch 34/200\n",
            "1/1 - 1s - loss: 1.2400 - acc: 0.8711 - val_loss: 1.2192 - val_acc: 0.8738 - 515ms/epoch - 515ms/step\n",
            "Epoch 35/200\n",
            "1/1 - 1s - loss: 1.2168 - acc: 0.8725 - val_loss: 1.2065 - val_acc: 0.8738 - 599ms/epoch - 599ms/step\n",
            "Epoch 36/200\n",
            "1/1 - 1s - loss: 1.2074 - acc: 0.8714 - val_loss: 1.1937 - val_acc: 0.8738 - 608ms/epoch - 608ms/step\n",
            "Epoch 37/200\n",
            "1/1 - 1s - loss: 1.1955 - acc: 0.8725 - val_loss: 1.1813 - val_acc: 0.8738 - 598ms/epoch - 598ms/step\n",
            "Epoch 38/200\n",
            "1/1 - 0s - loss: 1.1875 - acc: 0.8725 - val_loss: 1.1695 - val_acc: 0.8738 - 492ms/epoch - 492ms/step\n",
            "Epoch 39/200\n",
            "1/1 - 1s - loss: 1.1723 - acc: 0.8722 - val_loss: 1.1575 - val_acc: 0.8738 - 1s/epoch - 1s/step\n",
            "Epoch 40/200\n",
            "1/1 - 1s - loss: 1.1652 - acc: 0.8722 - val_loss: 1.1454 - val_acc: 0.8738 - 904ms/epoch - 904ms/step\n",
            "Epoch 41/200\n",
            "1/1 - 1s - loss: 1.1427 - acc: 0.8725 - val_loss: 1.1332 - val_acc: 0.8738 - 1s/epoch - 1s/step\n",
            "Epoch 42/200\n",
            "1/1 - 1s - loss: 1.1355 - acc: 0.8725 - val_loss: 1.1211 - val_acc: 0.8738 - 809ms/epoch - 809ms/step\n",
            "Epoch 43/200\n",
            "1/1 - 1s - loss: 1.1226 - acc: 0.8725 - val_loss: 1.1088 - val_acc: 0.8738 - 770ms/epoch - 770ms/step\n",
            "Epoch 44/200\n",
            "1/1 - 1s - loss: 1.1061 - acc: 0.8722 - val_loss: 1.0965 - val_acc: 0.8738 - 766ms/epoch - 766ms/step\n",
            "Epoch 45/200\n",
            "1/1 - 1s - loss: 1.1084 - acc: 0.8683 - val_loss: 1.0843 - val_acc: 0.8738 - 534ms/epoch - 534ms/step\n",
            "Epoch 46/200\n",
            "1/1 - 1s - loss: 1.0978 - acc: 0.8719 - val_loss: 1.0723 - val_acc: 0.8738 - 603ms/epoch - 603ms/step\n",
            "Epoch 47/200\n",
            "1/1 - 0s - loss: 1.0815 - acc: 0.8725 - val_loss: 1.0602 - val_acc: 0.8738 - 498ms/epoch - 498ms/step\n",
            "Epoch 48/200\n",
            "1/1 - 1s - loss: 1.0638 - acc: 0.8725 - val_loss: 1.0482 - val_acc: 0.8738 - 510ms/epoch - 510ms/step\n",
            "Epoch 49/200\n",
            "1/1 - 0s - loss: 1.0506 - acc: 0.8725 - val_loss: 1.0363 - val_acc: 0.8738 - 499ms/epoch - 499ms/step\n",
            "Epoch 50/200\n",
            "1/1 - 0s - loss: 1.0419 - acc: 0.8717 - val_loss: 1.0244 - val_acc: 0.8738 - 498ms/epoch - 498ms/step\n",
            "Epoch 51/200\n",
            "1/1 - 0s - loss: 1.0238 - acc: 0.8725 - val_loss: 1.0125 - val_acc: 0.8738 - 498ms/epoch - 498ms/step\n",
            "Epoch 52/200\n",
            "1/1 - 0s - loss: 1.0136 - acc: 0.8728 - val_loss: 1.0005 - val_acc: 0.8738 - 500ms/epoch - 500ms/step\n",
            "Epoch 53/200\n",
            "1/1 - 0s - loss: 1.0035 - acc: 0.8725 - val_loss: 0.9885 - val_acc: 0.8738 - 499ms/epoch - 499ms/step\n",
            "Epoch 54/200\n",
            "1/1 - 1s - loss: 1.0017 - acc: 0.8725 - val_loss: 0.9763 - val_acc: 0.8738 - 609ms/epoch - 609ms/step\n",
            "Epoch 55/200\n",
            "1/1 - 1s - loss: 0.9821 - acc: 0.8725 - val_loss: 0.9618 - val_acc: 0.8738 - 604ms/epoch - 604ms/step\n",
            "Epoch 56/200\n",
            "1/1 - 0s - loss: 0.9618 - acc: 0.8725 - val_loss: 0.9469 - val_acc: 0.8738 - 490ms/epoch - 490ms/step\n",
            "Epoch 57/200\n",
            "1/1 - 0s - loss: 0.9486 - acc: 0.8725 - val_loss: 0.9315 - val_acc: 0.8738 - 496ms/epoch - 496ms/step\n",
            "Epoch 58/200\n",
            "1/1 - 1s - loss: 0.9292 - acc: 0.8728 - val_loss: 0.9159 - val_acc: 0.8738 - 604ms/epoch - 604ms/step\n",
            "Epoch 59/200\n",
            "1/1 - 1s - loss: 0.9272 - acc: 0.8722 - val_loss: 0.9001 - val_acc: 0.8738 - 605ms/epoch - 605ms/step\n",
            "Epoch 60/200\n",
            "1/1 - 1s - loss: 0.9019 - acc: 0.8725 - val_loss: 0.8842 - val_acc: 0.8738 - 600ms/epoch - 600ms/step\n",
            "Epoch 61/200\n",
            "1/1 - 0s - loss: 0.8925 - acc: 0.8725 - val_loss: 0.8682 - val_acc: 0.8738 - 492ms/epoch - 492ms/step\n",
            "Epoch 62/200\n",
            "1/1 - 1s - loss: 0.8765 - acc: 0.8725 - val_loss: 0.8522 - val_acc: 0.8738 - 597ms/epoch - 597ms/step\n",
            "Epoch 63/200\n",
            "1/1 - 1s - loss: 0.8622 - acc: 0.8725 - val_loss: 0.8362 - val_acc: 0.8738 - 1s/epoch - 1s/step\n",
            "Epoch 64/200\n",
            "1/1 - 1s - loss: 0.8373 - acc: 0.8722 - val_loss: 0.8202 - val_acc: 0.8738 - 1s/epoch - 1s/step\n",
            "Epoch 65/200\n",
            "1/1 - 1s - loss: 0.8211 - acc: 0.8725 - val_loss: 0.8039 - val_acc: 0.8738 - 1s/epoch - 1s/step\n",
            "Epoch 66/200\n",
            "1/1 - 1s - loss: 0.8125 - acc: 0.8725 - val_loss: 0.7873 - val_acc: 0.8738 - 747ms/epoch - 747ms/step\n",
            "Epoch 67/200\n",
            "1/1 - 1s - loss: 0.7863 - acc: 0.8725 - val_loss: 0.7707 - val_acc: 0.8738 - 754ms/epoch - 754ms/step\n",
            "Epoch 68/200\n",
            "1/1 - 1s - loss: 0.7706 - acc: 0.8722 - val_loss: 0.7545 - val_acc: 0.8738 - 503ms/epoch - 503ms/step\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 1.4257 - acc: 0.8729\n",
            "\n",
            "Test Set Metrics:\n",
            "\tloss: 1.4257\n",
            "\tacc: 0.8729\n",
            "1/1 [==============================] - 0s 349ms/step\n",
            "Using GCN (local pooling) filters...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1/1 - 3s - loss: 1.6718 - acc: 0.0386 - val_loss: 1.6395 - val_acc: 0.0233 - 3s/epoch - 3s/step\n",
            "Epoch 2/200\n",
            "1/1 - 1s - loss: 1.6485 - acc: 0.0436 - val_loss: 1.6140 - val_acc: 0.0199 - 760ms/epoch - 760ms/step\n",
            "Epoch 3/200\n",
            "1/1 - 1s - loss: 1.6356 - acc: 0.0619 - val_loss: 1.5887 - val_acc: 0.0299 - 768ms/epoch - 768ms/step\n",
            "Epoch 4/200\n",
            "1/1 - 1s - loss: 1.5859 - acc: 0.0808 - val_loss: 1.5635 - val_acc: 0.0299 - 757ms/epoch - 757ms/step\n",
            "Epoch 5/200\n",
            "1/1 - 1s - loss: 1.5760 - acc: 0.2052 - val_loss: 1.5382 - val_acc: 0.5681 - 608ms/epoch - 608ms/step\n",
            "Epoch 6/200\n",
            "1/1 - 1s - loss: 1.5408 - acc: 0.6395 - val_loss: 1.5127 - val_acc: 0.6412 - 603ms/epoch - 603ms/step\n",
            "Epoch 7/200\n",
            "1/1 - 1s - loss: 1.5414 - acc: 0.6853 - val_loss: 1.4874 - val_acc: 0.8040 - 605ms/epoch - 605ms/step\n",
            "Epoch 8/200\n",
            "1/1 - 1s - loss: 1.5097 - acc: 0.6924 - val_loss: 1.4622 - val_acc: 0.8106 - 592ms/epoch - 592ms/step\n",
            "Epoch 9/200\n",
            "1/1 - 0s - loss: 1.4594 - acc: 0.7388 - val_loss: 1.4371 - val_acc: 0.8106 - 490ms/epoch - 490ms/step\n",
            "Epoch 10/200\n",
            "1/1 - 0s - loss: 1.4484 - acc: 0.7473 - val_loss: 1.4123 - val_acc: 0.8106 - 495ms/epoch - 495ms/step\n",
            "Epoch 11/200\n",
            "1/1 - 1s - loss: 1.4149 - acc: 0.7653 - val_loss: 1.3877 - val_acc: 0.8106 - 603ms/epoch - 603ms/step\n",
            "Epoch 12/200\n",
            "1/1 - 0s - loss: 1.3927 - acc: 0.7546 - val_loss: 1.3632 - val_acc: 0.8140 - 494ms/epoch - 494ms/step\n",
            "Epoch 13/200\n",
            "1/1 - 0s - loss: 1.3742 - acc: 0.7684 - val_loss: 1.3388 - val_acc: 0.8638 - 499ms/epoch - 499ms/step\n",
            "Epoch 14/200\n",
            "1/1 - 1s - loss: 1.3538 - acc: 0.7889 - val_loss: 1.3146 - val_acc: 0.8771 - 609ms/epoch - 609ms/step\n",
            "Epoch 15/200\n",
            "1/1 - 0s - loss: 1.3196 - acc: 0.8624 - val_loss: 1.2906 - val_acc: 0.8771 - 492ms/epoch - 492ms/step\n",
            "Epoch 16/200\n",
            "1/1 - 0s - loss: 1.3025 - acc: 0.8627 - val_loss: 1.2671 - val_acc: 0.8804 - 491ms/epoch - 491ms/step\n",
            "Epoch 17/200\n",
            "1/1 - 1s - loss: 1.2696 - acc: 0.8776 - val_loss: 1.2440 - val_acc: 0.8970 - 601ms/epoch - 601ms/step\n",
            "Epoch 18/200\n",
            "1/1 - 1s - loss: 1.2498 - acc: 0.8711 - val_loss: 1.2218 - val_acc: 0.9003 - 601ms/epoch - 601ms/step\n",
            "Epoch 19/200\n",
            "1/1 - 1s - loss: 1.2127 - acc: 0.8945 - val_loss: 1.1988 - val_acc: 0.9003 - 503ms/epoch - 503ms/step\n",
            "Epoch 20/200\n",
            "1/1 - 1s - loss: 1.2087 - acc: 0.8824 - val_loss: 1.1754 - val_acc: 0.9003 - 602ms/epoch - 602ms/step\n",
            "Epoch 21/200\n",
            "1/1 - 1s - loss: 1.1682 - acc: 0.8877 - val_loss: 1.1521 - val_acc: 0.9003 - 579ms/epoch - 579ms/step\n",
            "Epoch 22/200\n",
            "1/1 - 1s - loss: 1.1581 - acc: 0.8753 - val_loss: 1.1286 - val_acc: 0.9003 - 1s/epoch - 1s/step\n",
            "Epoch 23/200\n",
            "1/1 - 1s - loss: 1.1147 - acc: 0.8883 - val_loss: 1.1050 - val_acc: 0.9003 - 1s/epoch - 1s/step\n",
            "Epoch 24/200\n",
            "1/1 - 1s - loss: 1.1050 - acc: 0.8950 - val_loss: 1.0809 - val_acc: 0.9003 - 1s/epoch - 1s/step\n",
            "Epoch 25/200\n",
            "1/1 - 1s - loss: 1.0926 - acc: 0.8964 - val_loss: 1.0571 - val_acc: 0.9003 - 751ms/epoch - 751ms/step\n",
            "Epoch 26/200\n",
            "1/1 - 1s - loss: 1.0714 - acc: 0.8945 - val_loss: 1.0333 - val_acc: 0.9003 - 740ms/epoch - 740ms/step\n",
            "Epoch 27/200\n",
            "1/1 - 1s - loss: 1.0234 - acc: 0.9119 - val_loss: 1.0089 - val_acc: 0.9003 - 765ms/epoch - 765ms/step\n",
            "Epoch 28/200\n",
            "1/1 - 1s - loss: 1.0094 - acc: 0.8885 - val_loss: 0.9844 - val_acc: 0.9003 - 747ms/epoch - 747ms/step\n",
            "Epoch 29/200\n",
            "1/1 - 1s - loss: 0.9873 - acc: 0.8973 - val_loss: 0.9602 - val_acc: 0.9003 - 606ms/epoch - 606ms/step\n",
            "Epoch 30/200\n",
            "1/1 - 1s - loss: 0.9660 - acc: 0.8877 - val_loss: 0.9362 - val_acc: 0.9003 - 595ms/epoch - 595ms/step\n",
            "Epoch 31/200\n",
            "1/1 - 0s - loss: 0.9478 - acc: 0.8894 - val_loss: 0.9124 - val_acc: 0.9003 - 499ms/epoch - 499ms/step\n",
            "Epoch 32/200\n",
            "1/1 - 1s - loss: 0.9372 - acc: 0.8961 - val_loss: 0.8888 - val_acc: 0.9003 - 500ms/epoch - 500ms/step\n",
            "Epoch 33/200\n",
            "1/1 - 1s - loss: 0.8945 - acc: 0.8950 - val_loss: 0.8655 - val_acc: 0.9003 - 599ms/epoch - 599ms/step\n",
            "Epoch 34/200\n",
            "1/1 - 1s - loss: 0.8715 - acc: 0.8959 - val_loss: 0.8424 - val_acc: 0.9003 - 593ms/epoch - 593ms/step\n",
            "Epoch 35/200\n",
            "1/1 - 0s - loss: 0.8518 - acc: 0.8883 - val_loss: 0.8196 - val_acc: 0.9003 - 489ms/epoch - 489ms/step\n",
            "Epoch 36/200\n",
            "1/1 - 1s - loss: 0.8329 - acc: 0.9142 - val_loss: 0.7972 - val_acc: 0.9003 - 617ms/epoch - 617ms/step\n",
            "Epoch 37/200\n",
            "1/1 - 0s - loss: 0.8049 - acc: 0.9462 - val_loss: 0.7753 - val_acc: 0.9003 - 489ms/epoch - 489ms/step\n",
            "Epoch 38/200\n",
            "1/1 - 1s - loss: 0.8000 - acc: 0.8936 - val_loss: 0.7537 - val_acc: 0.9003 - 601ms/epoch - 601ms/step\n",
            "Epoch 39/200\n",
            "1/1 - 0s - loss: 0.7652 - acc: 0.9133 - val_loss: 0.7326 - val_acc: 0.9003 - 491ms/epoch - 491ms/step\n",
            "Epoch 40/200\n",
            "1/1 - 0s - loss: 0.7534 - acc: 0.8956 - val_loss: 0.7119 - val_acc: 0.9070 - 495ms/epoch - 495ms/step\n",
            "Epoch 41/200\n",
            "1/1 - 1s - loss: 0.7074 - acc: 0.9212 - val_loss: 0.6916 - val_acc: 0.9070 - 596ms/epoch - 596ms/step\n",
            "Epoch 42/200\n",
            "1/1 - 1s - loss: 0.7166 - acc: 0.8973 - val_loss: 0.6717 - val_acc: 0.9070 - 609ms/epoch - 609ms/step\n",
            "Epoch 43/200\n",
            "1/1 - 1s - loss: 0.6725 - acc: 0.9384 - val_loss: 0.6523 - val_acc: 0.9070 - 960ms/epoch - 960ms/step\n",
            "Epoch 44/200\n",
            "1/1 - 1s - loss: 0.6575 - acc: 0.9043 - val_loss: 0.6334 - val_acc: 0.9070 - 901ms/epoch - 901ms/step\n",
            "Epoch 45/200\n",
            "1/1 - 1s - loss: 0.6553 - acc: 0.9102 - val_loss: 0.6150 - val_acc: 0.9070 - 898ms/epoch - 898ms/step\n",
            "Epoch 46/200\n",
            "1/1 - 1s - loss: 0.6329 - acc: 0.9127 - val_loss: 0.5970 - val_acc: 0.9070 - 861ms/epoch - 861ms/step\n",
            "Epoch 47/200\n",
            "1/1 - 1s - loss: 0.6046 - acc: 0.9305 - val_loss: 0.5795 - val_acc: 0.9070 - 752ms/epoch - 752ms/step\n",
            "Epoch 48/200\n",
            "1/1 - 1s - loss: 0.5987 - acc: 0.9386 - val_loss: 0.5626 - val_acc: 0.9070 - 749ms/epoch - 749ms/step\n",
            "Epoch 49/200\n",
            "1/1 - 1s - loss: 0.5910 - acc: 0.9460 - val_loss: 0.5461 - val_acc: 0.9402 - 758ms/epoch - 758ms/step\n",
            "Epoch 50/200\n",
            "1/1 - 1s - loss: 0.5617 - acc: 0.9702 - val_loss: 0.5301 - val_acc: 0.9402 - 602ms/epoch - 602ms/step\n",
            "Epoch 51/200\n",
            "1/1 - 0s - loss: 0.5492 - acc: 0.9384 - val_loss: 0.5146 - val_acc: 0.9402 - 497ms/epoch - 497ms/step\n",
            "Epoch 52/200\n",
            "1/1 - 1s - loss: 0.5247 - acc: 0.9237 - val_loss: 0.4996 - val_acc: 0.9701 - 504ms/epoch - 504ms/step\n",
            "Epoch 53/200\n",
            "1/1 - 0s - loss: 0.5083 - acc: 0.9170 - val_loss: 0.4852 - val_acc: 0.9701 - 498ms/epoch - 498ms/step\n",
            "Epoch 54/200\n",
            "1/1 - 1s - loss: 0.4959 - acc: 0.9454 - val_loss: 0.4712 - val_acc: 0.9701 - 611ms/epoch - 611ms/step\n",
            "Epoch 55/200\n",
            "1/1 - 1s - loss: 0.4704 - acc: 0.9125 - val_loss: 0.4577 - val_acc: 0.9801 - 605ms/epoch - 605ms/step\n",
            "Epoch 56/200\n",
            "1/1 - 0s - loss: 0.4821 - acc: 0.9333 - val_loss: 0.4446 - val_acc: 0.9801 - 497ms/epoch - 497ms/step\n",
            "Epoch 57/200\n",
            "1/1 - 1s - loss: 0.4446 - acc: 0.9699 - val_loss: 0.4318 - val_acc: 0.9801 - 501ms/epoch - 501ms/step\n",
            "Epoch 58/200\n",
            "1/1 - 1s - loss: 0.4443 - acc: 0.9462 - val_loss: 0.4195 - val_acc: 0.9801 - 504ms/epoch - 504ms/step\n",
            "Epoch 59/200\n",
            "1/1 - 1s - loss: 0.4263 - acc: 0.9620 - val_loss: 0.4076 - val_acc: 0.9801 - 503ms/epoch - 503ms/step\n",
            "Epoch 60/200\n",
            "1/1 - 0s - loss: 0.4264 - acc: 0.9626 - val_loss: 0.3962 - val_acc: 0.9801 - 487ms/epoch - 487ms/step\n",
            "Epoch 61/200\n",
            "1/1 - 1s - loss: 0.4055 - acc: 0.9766 - val_loss: 0.3852 - val_acc: 0.9801 - 600ms/epoch - 600ms/step\n",
            "Epoch 62/200\n",
            "1/1 - 1s - loss: 0.3930 - acc: 0.9688 - val_loss: 0.3746 - val_acc: 0.9801 - 516ms/epoch - 516ms/step\n",
            "Epoch 63/200\n",
            "1/1 - 0s - loss: 0.3932 - acc: 0.9769 - val_loss: 0.3644 - val_acc: 0.9801 - 491ms/epoch - 491ms/step\n",
            "Epoch 64/200\n",
            "1/1 - 0s - loss: 0.3780 - acc: 0.9761 - val_loss: 0.3546 - val_acc: 0.9801 - 499ms/epoch - 499ms/step\n",
            "Epoch 65/200\n",
            "1/1 - 0s - loss: 0.3587 - acc: 0.9764 - val_loss: 0.3452 - val_acc: 0.9801 - 490ms/epoch - 490ms/step\n",
            "Epoch 66/200\n",
            "1/1 - 1s - loss: 0.3537 - acc: 0.9696 - val_loss: 0.3362 - val_acc: 0.9801 - 759ms/epoch - 759ms/step\n",
            "Epoch 67/200\n",
            "1/1 - 1s - loss: 0.3595 - acc: 0.9530 - val_loss: 0.3275 - val_acc: 0.9801 - 891ms/epoch - 891ms/step\n",
            "Epoch 68/200\n",
            "1/1 - 1s - loss: 0.3489 - acc: 0.9704 - val_loss: 0.3192 - val_acc: 0.9801 - 898ms/epoch - 898ms/step\n",
            "Epoch 69/200\n",
            "1/1 - 1s - loss: 0.3408 - acc: 0.9282 - val_loss: 0.3111 - val_acc: 0.9801 - 825ms/epoch - 825ms/step\n",
            "Epoch 70/200\n",
            "1/1 - 1s - loss: 0.3276 - acc: 0.9704 - val_loss: 0.3034 - val_acc: 0.9801 - 753ms/epoch - 753ms/step\n",
            "Epoch 71/200\n",
            "1/1 - 1s - loss: 0.3174 - acc: 0.9772 - val_loss: 0.2960 - val_acc: 0.9801 - 757ms/epoch - 757ms/step\n",
            "Epoch 72/200\n",
            "1/1 - 1s - loss: 0.3060 - acc: 0.9772 - val_loss: 0.2888 - val_acc: 0.9801 - 602ms/epoch - 602ms/step\n",
            "Epoch 73/200\n",
            "1/1 - 0s - loss: 0.3019 - acc: 0.9769 - val_loss: 0.2818 - val_acc: 0.9801 - 498ms/epoch - 498ms/step\n",
            "Epoch 74/200\n",
            "1/1 - 1s - loss: 0.2987 - acc: 0.9766 - val_loss: 0.2751 - val_acc: 0.9801 - 604ms/epoch - 604ms/step\n",
            "Epoch 75/200\n",
            "1/1 - 1s - loss: 0.2887 - acc: 0.9778 - val_loss: 0.2687 - val_acc: 0.9801 - 505ms/epoch - 505ms/step\n",
            "Epoch 76/200\n",
            "1/1 - 1s - loss: 0.2884 - acc: 0.9769 - val_loss: 0.2625 - val_acc: 0.9801 - 594ms/epoch - 594ms/step\n",
            "Epoch 77/200\n",
            "1/1 - 0s - loss: 0.2770 - acc: 0.9772 - val_loss: 0.2566 - val_acc: 0.9801 - 489ms/epoch - 489ms/step\n",
            "Epoch 78/200\n",
            "1/1 - 1s - loss: 0.2680 - acc: 0.9761 - val_loss: 0.2508 - val_acc: 0.9801 - 500ms/epoch - 500ms/step\n",
            "Epoch 79/200\n",
            "1/1 - 0s - loss: 0.2690 - acc: 0.9772 - val_loss: 0.2453 - val_acc: 0.9801 - 489ms/epoch - 489ms/step\n",
            "Epoch 80/200\n",
            "1/1 - 0s - loss: 0.2505 - acc: 0.9769 - val_loss: 0.2400 - val_acc: 0.9801 - 495ms/epoch - 495ms/step\n",
            "Epoch 81/200\n",
            "1/1 - 1s - loss: 0.2626 - acc: 0.9755 - val_loss: 0.2349 - val_acc: 0.9801 - 597ms/epoch - 597ms/step\n",
            "Epoch 82/200\n",
            "1/1 - 0s - loss: 0.2453 - acc: 0.9772 - val_loss: 0.2300 - val_acc: 0.9801 - 494ms/epoch - 494ms/step\n",
            "Epoch 83/200\n",
            "1/1 - 1s - loss: 0.2427 - acc: 0.9764 - val_loss: 0.2254 - val_acc: 0.9801 - 605ms/epoch - 605ms/step\n",
            "Epoch 84/200\n",
            "1/1 - 1s - loss: 0.2466 - acc: 0.9704 - val_loss: 0.2208 - val_acc: 0.9801 - 604ms/epoch - 604ms/step\n",
            "Epoch 85/200\n",
            "1/1 - 1s - loss: 0.2313 - acc: 0.9769 - val_loss: 0.2165 - val_acc: 0.9801 - 508ms/epoch - 508ms/step\n",
            "Epoch 86/200\n",
            "1/1 - 1s - loss: 0.2309 - acc: 0.9772 - val_loss: 0.2124 - val_acc: 0.9801 - 590ms/epoch - 590ms/step\n",
            "Epoch 87/200\n",
            "1/1 - 0s - loss: 0.2296 - acc: 0.9761 - val_loss: 0.2084 - val_acc: 0.9801 - 496ms/epoch - 496ms/step\n",
            "Epoch 88/200\n",
            "1/1 - 1s - loss: 0.2241 - acc: 0.9769 - val_loss: 0.2045 - val_acc: 0.9801 - 608ms/epoch - 608ms/step\n",
            "Epoch 89/200\n",
            "1/1 - 1s - loss: 0.2126 - acc: 0.9764 - val_loss: 0.2008 - val_acc: 0.9801 - 906ms/epoch - 906ms/step\n",
            "Epoch 90/200\n",
            "1/1 - 1s - loss: 0.2147 - acc: 0.9766 - val_loss: 0.1973 - val_acc: 0.9801 - 1s/epoch - 1s/step\n",
            "Epoch 91/200\n",
            "1/1 - 1s - loss: 0.2078 - acc: 0.9769 - val_loss: 0.1939 - val_acc: 0.9801 - 913ms/epoch - 913ms/step\n",
            "Epoch 92/200\n",
            "1/1 - 1s - loss: 0.2092 - acc: 0.9772 - val_loss: 0.1906 - val_acc: 0.9801 - 765ms/epoch - 765ms/step\n",
            "Epoch 93/200\n",
            "1/1 - 1s - loss: 0.2093 - acc: 0.9764 - val_loss: 0.1874 - val_acc: 0.9801 - 747ms/epoch - 747ms/step\n",
            "Epoch 94/200\n",
            "1/1 - 1s - loss: 0.2027 - acc: 0.9769 - val_loss: 0.1843 - val_acc: 0.9801 - 753ms/epoch - 753ms/step\n",
            "Epoch 95/200\n",
            "1/1 - 1s - loss: 0.1960 - acc: 0.9769 - val_loss: 0.1814 - val_acc: 0.9801 - 547ms/epoch - 547ms/step\n",
            "Epoch 96/200\n",
            "1/1 - 1s - loss: 0.2041 - acc: 0.9769 - val_loss: 0.1785 - val_acc: 0.9801 - 505ms/epoch - 505ms/step\n",
            "Epoch 97/200\n",
            "1/1 - 1s - loss: 0.1880 - acc: 0.9775 - val_loss: 0.1757 - val_acc: 0.9801 - 598ms/epoch - 598ms/step\n",
            "Epoch 98/200\n",
            "1/1 - 1s - loss: 0.1807 - acc: 0.9766 - val_loss: 0.1729 - val_acc: 0.9801 - 599ms/epoch - 599ms/step\n",
            "Epoch 99/200\n",
            "1/1 - 1s - loss: 0.1870 - acc: 0.9772 - val_loss: 0.1702 - val_acc: 0.9801 - 505ms/epoch - 505ms/step\n",
            "Epoch 100/200\n",
            "1/1 - 1s - loss: 0.1804 - acc: 0.9775 - val_loss: 0.1676 - val_acc: 0.9801 - 604ms/epoch - 604ms/step\n",
            "Epoch 101/200\n",
            "1/1 - 1s - loss: 0.1824 - acc: 0.9766 - val_loss: 0.1650 - val_acc: 0.9801 - 510ms/epoch - 510ms/step\n",
            "Epoch 102/200\n",
            "1/1 - 1s - loss: 0.1732 - acc: 0.9769 - val_loss: 0.1626 - val_acc: 0.9801 - 601ms/epoch - 601ms/step\n",
            "Epoch 103/200\n",
            "1/1 - 1s - loss: 0.1731 - acc: 0.9766 - val_loss: 0.1602 - val_acc: 0.9801 - 611ms/epoch - 611ms/step\n",
            "Epoch 104/200\n",
            "1/1 - 0s - loss: 0.1758 - acc: 0.9775 - val_loss: 0.1579 - val_acc: 0.9801 - 493ms/epoch - 493ms/step\n",
            "Epoch 105/200\n",
            "1/1 - 1s - loss: 0.1645 - acc: 0.9772 - val_loss: 0.1557 - val_acc: 0.9801 - 512ms/epoch - 512ms/step\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.4661 - acc: 0.9807\n",
            "\n",
            "Test Set Metrics:\n",
            "\tloss: 0.4661\n",
            "\tacc: 0.9807\n",
            "1/1 [==============================] - 0s 332ms/step\n",
            "Using GCN (local pooling) filters...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1/1 - 2s - loss: 1.4715 - acc: 0.7053 - val_loss: 1.4588 - val_acc: 0.7475 - 2s/epoch - 2s/step\n",
            "Epoch 2/200\n",
            "1/1 - 1s - loss: 1.4516 - acc: 0.7971 - val_loss: 1.4400 - val_acc: 0.8738 - 502ms/epoch - 502ms/step\n",
            "Epoch 3/200\n",
            "1/1 - 1s - loss: 1.4334 - acc: 0.8083 - val_loss: 1.4213 - val_acc: 0.8738 - 504ms/epoch - 504ms/step\n",
            "Epoch 4/200\n",
            "1/1 - 1s - loss: 1.4284 - acc: 0.7979 - val_loss: 1.4028 - val_acc: 0.8738 - 611ms/epoch - 611ms/step\n",
            "Epoch 5/200\n",
            "1/1 - 1s - loss: 1.4022 - acc: 0.8472 - val_loss: 1.3846 - val_acc: 0.8738 - 502ms/epoch - 502ms/step\n",
            "Epoch 6/200\n",
            "1/1 - 1s - loss: 1.3880 - acc: 0.8534 - val_loss: 1.3673 - val_acc: 0.8738 - 601ms/epoch - 601ms/step\n",
            "Epoch 7/200\n",
            "1/1 - 1s - loss: 1.3667 - acc: 0.8435 - val_loss: 1.3514 - val_acc: 0.8738 - 597ms/epoch - 597ms/step\n",
            "Epoch 8/200\n",
            "1/1 - 0s - loss: 1.3626 - acc: 0.8593 - val_loss: 1.3353 - val_acc: 0.8738 - 497ms/epoch - 497ms/step\n",
            "Epoch 9/200\n",
            "1/1 - 1s - loss: 1.3335 - acc: 0.8576 - val_loss: 1.3192 - val_acc: 0.8738 - 664ms/epoch - 664ms/step\n",
            "Epoch 10/200\n",
            "1/1 - 1s - loss: 1.3199 - acc: 0.8576 - val_loss: 1.3029 - val_acc: 0.8738 - 906ms/epoch - 906ms/step\n",
            "Epoch 11/200\n",
            "1/1 - 1s - loss: 1.2941 - acc: 0.8666 - val_loss: 1.2865 - val_acc: 0.8738 - 928ms/epoch - 928ms/step\n",
            "Epoch 12/200\n",
            "1/1 - 1s - loss: 1.2903 - acc: 0.8635 - val_loss: 1.2701 - val_acc: 0.8738 - 744ms/epoch - 744ms/step\n",
            "Epoch 13/200\n",
            "1/1 - 1s - loss: 1.2707 - acc: 0.8624 - val_loss: 1.2536 - val_acc: 0.8738 - 758ms/epoch - 758ms/step\n",
            "Epoch 14/200\n",
            "1/1 - 1s - loss: 1.2547 - acc: 0.8714 - val_loss: 1.2372 - val_acc: 0.8738 - 752ms/epoch - 752ms/step\n",
            "Epoch 15/200\n",
            "1/1 - 1s - loss: 1.2431 - acc: 0.8660 - val_loss: 1.2209 - val_acc: 0.8738 - 513ms/epoch - 513ms/step\n",
            "Epoch 16/200\n",
            "1/1 - 1s - loss: 1.2188 - acc: 0.8731 - val_loss: 1.2043 - val_acc: 0.8738 - 521ms/epoch - 521ms/step\n",
            "Epoch 17/200\n",
            "1/1 - 1s - loss: 1.2033 - acc: 0.8711 - val_loss: 1.1884 - val_acc: 0.8738 - 601ms/epoch - 601ms/step\n",
            "Epoch 18/200\n",
            "1/1 - 0s - loss: 1.1928 - acc: 0.8722 - val_loss: 1.1730 - val_acc: 0.8738 - 499ms/epoch - 499ms/step\n",
            "Epoch 19/200\n",
            "1/1 - 1s - loss: 1.1858 - acc: 0.8708 - val_loss: 1.1575 - val_acc: 0.8738 - 600ms/epoch - 600ms/step\n",
            "Epoch 20/200\n",
            "1/1 - 1s - loss: 1.1637 - acc: 0.8733 - val_loss: 1.1418 - val_acc: 0.8738 - 596ms/epoch - 596ms/step\n",
            "Epoch 21/200\n",
            "1/1 - 1s - loss: 1.1526 - acc: 0.8728 - val_loss: 1.1260 - val_acc: 0.8738 - 507ms/epoch - 507ms/step\n",
            "Epoch 22/200\n",
            "1/1 - 1s - loss: 1.1408 - acc: 0.8725 - val_loss: 1.1101 - val_acc: 0.8738 - 603ms/epoch - 603ms/step\n",
            "Epoch 23/200\n",
            "1/1 - 1s - loss: 1.1051 - acc: 0.8728 - val_loss: 1.0942 - val_acc: 0.8738 - 606ms/epoch - 606ms/step\n",
            "Epoch 24/200\n",
            "1/1 - 1s - loss: 1.1008 - acc: 0.8824 - val_loss: 1.0782 - val_acc: 0.8738 - 604ms/epoch - 604ms/step\n",
            "Epoch 25/200\n",
            "1/1 - 1s - loss: 1.0851 - acc: 0.8708 - val_loss: 1.0624 - val_acc: 0.8738 - 600ms/epoch - 600ms/step\n",
            "Epoch 26/200\n",
            "1/1 - 1s - loss: 1.0705 - acc: 0.8731 - val_loss: 1.0466 - val_acc: 0.8738 - 605ms/epoch - 605ms/step\n",
            "Epoch 27/200\n",
            "1/1 - 0s - loss: 1.0402 - acc: 0.8733 - val_loss: 1.0307 - val_acc: 0.8738 - 490ms/epoch - 490ms/step\n",
            "Epoch 28/200\n",
            "1/1 - 1s - loss: 1.0373 - acc: 0.8824 - val_loss: 1.0147 - val_acc: 0.8738 - 602ms/epoch - 602ms/step\n",
            "Epoch 29/200\n",
            "1/1 - 0s - loss: 1.0159 - acc: 0.8711 - val_loss: 0.9987 - val_acc: 0.8738 - 499ms/epoch - 499ms/step\n",
            "Epoch 30/200\n",
            "1/1 - 0s - loss: 1.0035 - acc: 0.8756 - val_loss: 0.9826 - val_acc: 0.8738 - 487ms/epoch - 487ms/step\n",
            "Epoch 31/200\n",
            "1/1 - 1s - loss: 0.9816 - acc: 0.8846 - val_loss: 0.9664 - val_acc: 0.8804 - 627ms/epoch - 627ms/step\n",
            "Epoch 32/200\n",
            "1/1 - 1s - loss: 0.9738 - acc: 0.8869 - val_loss: 0.9502 - val_acc: 0.8804 - 1s/epoch - 1s/step\n",
            "Epoch 33/200\n",
            "1/1 - 1s - loss: 0.9376 - acc: 0.8731 - val_loss: 0.9339 - val_acc: 0.8804 - 1s/epoch - 1s/step\n",
            "Epoch 34/200\n",
            "1/1 - 1s - loss: 0.9306 - acc: 0.8776 - val_loss: 0.9173 - val_acc: 0.8804 - 751ms/epoch - 751ms/step\n",
            "Epoch 35/200\n",
            "1/1 - 1s - loss: 0.9145 - acc: 0.8840 - val_loss: 0.9007 - val_acc: 0.8837 - 1s/epoch - 1s/step\n",
            "Epoch 36/200\n",
            "1/1 - 1s - loss: 0.9146 - acc: 0.8739 - val_loss: 0.8841 - val_acc: 0.8837 - 662ms/epoch - 662ms/step\n",
            "Epoch 37/200\n",
            "1/1 - 1s - loss: 0.8802 - acc: 0.8840 - val_loss: 0.8675 - val_acc: 0.8837 - 600ms/epoch - 600ms/step\n",
            "Epoch 38/200\n",
            "1/1 - 0s - loss: 0.8745 - acc: 0.8835 - val_loss: 0.8510 - val_acc: 0.8837 - 494ms/epoch - 494ms/step\n",
            "Epoch 39/200\n",
            "1/1 - 0s - loss: 0.8499 - acc: 0.8736 - val_loss: 0.8346 - val_acc: 0.8837 - 495ms/epoch - 495ms/step\n",
            "Epoch 40/200\n",
            "1/1 - 0s - loss: 0.8464 - acc: 0.8854 - val_loss: 0.8183 - val_acc: 0.8837 - 492ms/epoch - 492ms/step\n",
            "Epoch 41/200\n",
            "1/1 - 1s - loss: 0.8284 - acc: 0.8846 - val_loss: 0.8021 - val_acc: 0.8837 - 594ms/epoch - 594ms/step\n",
            "Epoch 42/200\n",
            "1/1 - 0s - loss: 0.8104 - acc: 0.8866 - val_loss: 0.7860 - val_acc: 0.8837 - 483ms/epoch - 483ms/step\n",
            "Epoch 43/200\n",
            "1/1 - 0s - loss: 0.7939 - acc: 0.8826 - val_loss: 0.7700 - val_acc: 0.8837 - 500ms/epoch - 500ms/step\n",
            "Epoch 44/200\n",
            "1/1 - 0s - loss: 0.7901 - acc: 0.8857 - val_loss: 0.7541 - val_acc: 0.8837 - 498ms/epoch - 498ms/step\n",
            "Epoch 45/200\n",
            "1/1 - 1s - loss: 0.7690 - acc: 0.8869 - val_loss: 0.7386 - val_acc: 0.8837 - 504ms/epoch - 504ms/step\n",
            "Epoch 46/200\n",
            "1/1 - 0s - loss: 0.7578 - acc: 0.8852 - val_loss: 0.7235 - val_acc: 0.8837 - 496ms/epoch - 496ms/step\n",
            "Epoch 47/200\n",
            "1/1 - 1s - loss: 0.7347 - acc: 0.8857 - val_loss: 0.7085 - val_acc: 0.8837 - 502ms/epoch - 502ms/step\n",
            "Epoch 48/200\n",
            "1/1 - 1s - loss: 0.7111 - acc: 0.8888 - val_loss: 0.6937 - val_acc: 0.8837 - 503ms/epoch - 503ms/step\n",
            "Epoch 49/200\n",
            "1/1 - 0s - loss: 0.7069 - acc: 0.8849 - val_loss: 0.6790 - val_acc: 0.8837 - 486ms/epoch - 486ms/step\n",
            "Epoch 50/200\n",
            "1/1 - 1s - loss: 0.6849 - acc: 0.8874 - val_loss: 0.6645 - val_acc: 0.8837 - 503ms/epoch - 503ms/step\n",
            "Epoch 51/200\n",
            "1/1 - 1s - loss: 0.6680 - acc: 0.8883 - val_loss: 0.6502 - val_acc: 0.8837 - 600ms/epoch - 600ms/step\n",
            "Epoch 52/200\n",
            "1/1 - 1s - loss: 0.6437 - acc: 0.8885 - val_loss: 0.6360 - val_acc: 0.8837 - 1s/epoch - 1s/step\n",
            "Epoch 53/200\n",
            "1/1 - 1s - loss: 0.6320 - acc: 0.8857 - val_loss: 0.6221 - val_acc: 0.8837 - 1s/epoch - 1s/step\n",
            "Epoch 54/200\n",
            "1/1 - 1s - loss: 0.6215 - acc: 0.8863 - val_loss: 0.6083 - val_acc: 0.8837 - 1s/epoch - 1s/step\n",
            "Epoch 55/200\n",
            "1/1 - 1s - loss: 0.6216 - acc: 0.8885 - val_loss: 0.5946 - val_acc: 0.8837 - 766ms/epoch - 766ms/step\n",
            "Epoch 56/200\n",
            "1/1 - 1s - loss: 0.6067 - acc: 0.8869 - val_loss: 0.5810 - val_acc: 0.8837 - 764ms/epoch - 764ms/step\n",
            "Epoch 57/200\n",
            "1/1 - 1s - loss: 0.5809 - acc: 0.8883 - val_loss: 0.5675 - val_acc: 0.8837 - 760ms/epoch - 760ms/step\n",
            "Epoch 58/200\n",
            "1/1 - 1s - loss: 0.5802 - acc: 0.8894 - val_loss: 0.5536 - val_acc: 0.8837 - 590ms/epoch - 590ms/step\n",
            "Epoch 59/200\n",
            "1/1 - 1s - loss: 0.5545 - acc: 0.8888 - val_loss: 0.5399 - val_acc: 0.8837 - 506ms/epoch - 506ms/step\n",
            "Epoch 60/200\n",
            "1/1 - 1s - loss: 0.5629 - acc: 0.8883 - val_loss: 0.5264 - val_acc: 0.8837 - 594ms/epoch - 594ms/step\n",
            "Epoch 61/200\n",
            "1/1 - 1s - loss: 0.5333 - acc: 0.8883 - val_loss: 0.5132 - val_acc: 0.8837 - 501ms/epoch - 501ms/step\n",
            "Epoch 62/200\n",
            "1/1 - 0s - loss: 0.5227 - acc: 0.8880 - val_loss: 0.5001 - val_acc: 0.8837 - 495ms/epoch - 495ms/step\n",
            "Epoch 63/200\n",
            "1/1 - 1s - loss: 0.4984 - acc: 0.8894 - val_loss: 0.4874 - val_acc: 0.8837 - 505ms/epoch - 505ms/step\n",
            "Epoch 64/200\n",
            "1/1 - 1s - loss: 0.4943 - acc: 0.8854 - val_loss: 0.4749 - val_acc: 0.8837 - 601ms/epoch - 601ms/step\n",
            "Epoch 65/200\n",
            "1/1 - 1s - loss: 0.4812 - acc: 0.8885 - val_loss: 0.4628 - val_acc: 0.8837 - 602ms/epoch - 602ms/step\n",
            "Epoch 66/200\n",
            "1/1 - 0s - loss: 0.4682 - acc: 0.8880 - val_loss: 0.4509 - val_acc: 0.8837 - 496ms/epoch - 496ms/step\n",
            "Epoch 67/200\n",
            "1/1 - 0s - loss: 0.4645 - acc: 0.8891 - val_loss: 0.4393 - val_acc: 0.8837 - 499ms/epoch - 499ms/step\n",
            "Epoch 68/200\n",
            "1/1 - 1s - loss: 0.4561 - acc: 0.8888 - val_loss: 0.4280 - val_acc: 0.8837 - 608ms/epoch - 608ms/step\n",
            "Epoch 69/200\n",
            "1/1 - 0s - loss: 0.4407 - acc: 0.8888 - val_loss: 0.4170 - val_acc: 0.8837 - 498ms/epoch - 498ms/step\n",
            "Epoch 70/200\n",
            "1/1 - 0s - loss: 0.4362 - acc: 0.8880 - val_loss: 0.4064 - val_acc: 0.8837 - 497ms/epoch - 497ms/step\n",
            "Epoch 71/200\n",
            "1/1 - 0s - loss: 0.4190 - acc: 0.8891 - val_loss: 0.3961 - val_acc: 0.8837 - 498ms/epoch - 498ms/step\n",
            "Epoch 72/200\n",
            "1/1 - 1s - loss: 0.4142 - acc: 0.8897 - val_loss: 0.3862 - val_acc: 0.8837 - 500ms/epoch - 500ms/step\n",
            "Epoch 73/200\n",
            "1/1 - 1s - loss: 0.3937 - acc: 0.9111 - val_loss: 0.3766 - val_acc: 0.8837 - 510ms/epoch - 510ms/step\n",
            "Epoch 74/200\n",
            "1/1 - 1s - loss: 0.3793 - acc: 0.8897 - val_loss: 0.3673 - val_acc: 0.8837 - 616ms/epoch - 616ms/step\n",
            "Epoch 75/200\n",
            "1/1 - 1s - loss: 0.3732 - acc: 0.8902 - val_loss: 0.3584 - val_acc: 0.8837 - 1s/epoch - 1s/step\n",
            "Epoch 76/200\n",
            "1/1 - 1s - loss: 0.3601 - acc: 0.8902 - val_loss: 0.3497 - val_acc: 0.8837 - 882ms/epoch - 882ms/step\n",
            "Epoch 77/200\n",
            "1/1 - 1s - loss: 0.3657 - acc: 0.9113 - val_loss: 0.3412 - val_acc: 0.8837 - 921ms/epoch - 921ms/step\n",
            "Epoch 78/200\n",
            "1/1 - 1s - loss: 0.3409 - acc: 0.8967 - val_loss: 0.3330 - val_acc: 0.8837 - 834ms/epoch - 834ms/step\n",
            "Epoch 79/200\n",
            "1/1 - 1s - loss: 0.3409 - acc: 0.8894 - val_loss: 0.3253 - val_acc: 0.8837 - 772ms/epoch - 772ms/step\n",
            "Epoch 80/200\n",
            "1/1 - 1s - loss: 0.3351 - acc: 0.8976 - val_loss: 0.3179 - val_acc: 0.8837 - 759ms/epoch - 759ms/step\n",
            "Epoch 81/200\n",
            "1/1 - 1s - loss: 0.3320 - acc: 0.9358 - val_loss: 0.3107 - val_acc: 0.8904 - 682ms/epoch - 682ms/step\n",
            "Epoch 82/200\n",
            "1/1 - 1s - loss: 0.3205 - acc: 0.9454 - val_loss: 0.3037 - val_acc: 0.8904 - 620ms/epoch - 620ms/step\n",
            "Epoch 83/200\n",
            "1/1 - 1s - loss: 0.3141 - acc: 0.9502 - val_loss: 0.2969 - val_acc: 0.9734 - 501ms/epoch - 501ms/step\n",
            "Epoch 84/200\n",
            "1/1 - 1s - loss: 0.3249 - acc: 0.8888 - val_loss: 0.2904 - val_acc: 0.9734 - 596ms/epoch - 596ms/step\n",
            "Epoch 85/200\n",
            "1/1 - 1s - loss: 0.3049 - acc: 0.8970 - val_loss: 0.2841 - val_acc: 0.9734 - 597ms/epoch - 597ms/step\n",
            "Epoch 86/200\n",
            "1/1 - 1s - loss: 0.3104 - acc: 0.8888 - val_loss: 0.2780 - val_acc: 0.9734 - 502ms/epoch - 502ms/step\n",
            "Epoch 87/200\n",
            "1/1 - 1s - loss: 0.2788 - acc: 0.9766 - val_loss: 0.2721 - val_acc: 0.9734 - 612ms/epoch - 612ms/step\n",
            "Epoch 88/200\n",
            "1/1 - 1s - loss: 0.2861 - acc: 0.9465 - val_loss: 0.2663 - val_acc: 0.9734 - 500ms/epoch - 500ms/step\n",
            "Epoch 89/200\n",
            "1/1 - 1s - loss: 0.2796 - acc: 0.9600 - val_loss: 0.2608 - val_acc: 0.9734 - 501ms/epoch - 501ms/step\n",
            "Epoch 90/200\n",
            "1/1 - 1s - loss: 0.2729 - acc: 0.9693 - val_loss: 0.2554 - val_acc: 0.9734 - 605ms/epoch - 605ms/step\n",
            "Epoch 91/200\n",
            "1/1 - 1s - loss: 0.2615 - acc: 0.9764 - val_loss: 0.2502 - val_acc: 0.9734 - 507ms/epoch - 507ms/step\n",
            "Epoch 92/200\n",
            "1/1 - 0s - loss: 0.2729 - acc: 0.9600 - val_loss: 0.2452 - val_acc: 0.9734 - 490ms/epoch - 490ms/step\n",
            "Epoch 93/200\n",
            "1/1 - 1s - loss: 0.2620 - acc: 0.9603 - val_loss: 0.2404 - val_acc: 0.9734 - 604ms/epoch - 604ms/step\n",
            "Epoch 94/200\n",
            "1/1 - 1s - loss: 0.2505 - acc: 0.9690 - val_loss: 0.2357 - val_acc: 0.9734 - 597ms/epoch - 597ms/step\n",
            "Epoch 95/200\n",
            "1/1 - 1s - loss: 0.2413 - acc: 0.9688 - val_loss: 0.2312 - val_acc: 0.9734 - 502ms/epoch - 502ms/step\n",
            "Epoch 96/200\n",
            "1/1 - 1s - loss: 0.2439 - acc: 0.9682 - val_loss: 0.2268 - val_acc: 0.9734 - 609ms/epoch - 609ms/step\n",
            "Epoch 97/200\n",
            "1/1 - 1s - loss: 0.2411 - acc: 0.9600 - val_loss: 0.2225 - val_acc: 0.9767 - 847ms/epoch - 847ms/step\n",
            "Epoch 98/200\n",
            "1/1 - 1s - loss: 0.2318 - acc: 0.9693 - val_loss: 0.2184 - val_acc: 0.9767 - 935ms/epoch - 935ms/step\n",
            "Epoch 99/200\n",
            "1/1 - 1s - loss: 0.2359 - acc: 0.9676 - val_loss: 0.2143 - val_acc: 0.9767 - 1s/epoch - 1s/step\n",
            "Epoch 100/200\n",
            "1/1 - 1s - loss: 0.2260 - acc: 0.9690 - val_loss: 0.2103 - val_acc: 0.9767 - 823ms/epoch - 823ms/step\n",
            "Epoch 101/200\n",
            "1/1 - 1s - loss: 0.2232 - acc: 0.9783 - val_loss: 0.2065 - val_acc: 0.9767 - 747ms/epoch - 747ms/step\n",
            "Epoch 102/200\n",
            "1/1 - 1s - loss: 0.2198 - acc: 0.9778 - val_loss: 0.2027 - val_acc: 0.9767 - 761ms/epoch - 761ms/step\n",
            "Epoch 103/200\n",
            "1/1 - 1s - loss: 0.2165 - acc: 0.9766 - val_loss: 0.1991 - val_acc: 0.9767 - 775ms/epoch - 775ms/step\n",
            "Epoch 104/200\n",
            "1/1 - 0s - loss: 0.2054 - acc: 0.9764 - val_loss: 0.1955 - val_acc: 0.9767 - 493ms/epoch - 493ms/step\n",
            "Epoch 105/200\n",
            "1/1 - 0s - loss: 0.2049 - acc: 0.9775 - val_loss: 0.1921 - val_acc: 0.9767 - 498ms/epoch - 498ms/step\n",
            "Epoch 106/200\n",
            "1/1 - 1s - loss: 0.2023 - acc: 0.9769 - val_loss: 0.1887 - val_acc: 0.9767 - 600ms/epoch - 600ms/step\n",
            "Epoch 107/200\n",
            "1/1 - 1s - loss: 0.1960 - acc: 0.9772 - val_loss: 0.1855 - val_acc: 0.9767 - 608ms/epoch - 608ms/step\n",
            "Epoch 108/200\n",
            "1/1 - 1s - loss: 0.1904 - acc: 0.9783 - val_loss: 0.1822 - val_acc: 0.9767 - 601ms/epoch - 601ms/step\n",
            "Epoch 109/200\n",
            "1/1 - 1s - loss: 0.1919 - acc: 0.9676 - val_loss: 0.1790 - val_acc: 0.9767 - 601ms/epoch - 601ms/step\n",
            "Epoch 110/200\n",
            "1/1 - 1s - loss: 0.1887 - acc: 0.9778 - val_loss: 0.1759 - val_acc: 0.9767 - 506ms/epoch - 506ms/step\n",
            "Epoch 111/200\n",
            "1/1 - 0s - loss: 0.1870 - acc: 0.9696 - val_loss: 0.1729 - val_acc: 0.9767 - 492ms/epoch - 492ms/step\n",
            "Epoch 112/200\n",
            "1/1 - 0s - loss: 0.1874 - acc: 0.9643 - val_loss: 0.1700 - val_acc: 0.9767 - 490ms/epoch - 490ms/step\n",
            "Epoch 113/200\n",
            "1/1 - 1s - loss: 0.1781 - acc: 0.9780 - val_loss: 0.1672 - val_acc: 0.9767 - 608ms/epoch - 608ms/step\n",
            "Epoch 114/200\n",
            "1/1 - 1s - loss: 0.1882 - acc: 0.9780 - val_loss: 0.1645 - val_acc: 0.9767 - 594ms/epoch - 594ms/step\n",
            "Epoch 115/200\n",
            "1/1 - 0s - loss: 0.1730 - acc: 0.9778 - val_loss: 0.1618 - val_acc: 0.9767 - 493ms/epoch - 493ms/step\n",
            "Epoch 116/200\n",
            "1/1 - 1s - loss: 0.1692 - acc: 0.9775 - val_loss: 0.1592 - val_acc: 0.9767 - 512ms/epoch - 512ms/step\n",
            "Epoch 117/200\n",
            "1/1 - 0s - loss: 0.1747 - acc: 0.9783 - val_loss: 0.1568 - val_acc: 0.9767 - 491ms/epoch - 491ms/step\n",
            "Epoch 118/200\n",
            "1/1 - 1s - loss: 0.1729 - acc: 0.9783 - val_loss: 0.1543 - val_acc: 0.9767 - 608ms/epoch - 608ms/step\n",
            "Epoch 119/200\n",
            "1/1 - 1s - loss: 0.1627 - acc: 0.9772 - val_loss: 0.1520 - val_acc: 0.9767 - 619ms/epoch - 619ms/step\n",
            "Epoch 120/200\n",
            "1/1 - 1s - loss: 0.1713 - acc: 0.9671 - val_loss: 0.1497 - val_acc: 0.9767 - 1s/epoch - 1s/step\n",
            "Epoch 121/200\n",
            "1/1 - 1s - loss: 0.1659 - acc: 0.9690 - val_loss: 0.1475 - val_acc: 0.9767 - 904ms/epoch - 904ms/step\n",
            "Epoch 122/200\n",
            "1/1 - 1s - loss: 0.1567 - acc: 0.9780 - val_loss: 0.1453 - val_acc: 0.9767 - 1s/epoch - 1s/step\n",
            "Epoch 123/200\n",
            "1/1 - 1s - loss: 0.1549 - acc: 0.9780 - val_loss: 0.1430 - val_acc: 0.9767 - 762ms/epoch - 762ms/step\n",
            "Epoch 124/200\n",
            "1/1 - 1s - loss: 0.1591 - acc: 0.9780 - val_loss: 0.1406 - val_acc: 0.9767 - 755ms/epoch - 755ms/step\n",
            "Epoch 125/200\n",
            "1/1 - 1s - loss: 0.1502 - acc: 0.9780 - val_loss: 0.1384 - val_acc: 0.9767 - 764ms/epoch - 764ms/step\n",
            "Epoch 126/200\n",
            "1/1 - 1s - loss: 0.1480 - acc: 0.9775 - val_loss: 0.1362 - val_acc: 0.9767 - 657ms/epoch - 657ms/step\n",
            "Epoch 127/200\n",
            "1/1 - 1s - loss: 0.1530 - acc: 0.9780 - val_loss: 0.1341 - val_acc: 0.9767 - 606ms/epoch - 606ms/step\n",
            "Epoch 128/200\n",
            "1/1 - 1s - loss: 0.1465 - acc: 0.9778 - val_loss: 0.1320 - val_acc: 0.9767 - 521ms/epoch - 521ms/step\n",
            "Epoch 129/200\n",
            "1/1 - 1s - loss: 0.1420 - acc: 0.9778 - val_loss: 0.1301 - val_acc: 0.9767 - 503ms/epoch - 503ms/step\n",
            "Epoch 130/200\n",
            "1/1 - 1s - loss: 0.1417 - acc: 0.9780 - val_loss: 0.1282 - val_acc: 0.9767 - 607ms/epoch - 607ms/step\n",
            "Epoch 131/200\n",
            "1/1 - 1s - loss: 0.1493 - acc: 0.9769 - val_loss: 0.1263 - val_acc: 0.9767 - 505ms/epoch - 505ms/step\n",
            "Epoch 132/200\n",
            "1/1 - 0s - loss: 0.1386 - acc: 0.9780 - val_loss: 0.1246 - val_acc: 0.9767 - 498ms/epoch - 498ms/step\n",
            "Epoch 133/200\n",
            "1/1 - 1s - loss: 0.1400 - acc: 0.9775 - val_loss: 0.1228 - val_acc: 0.9767 - 502ms/epoch - 502ms/step\n",
            "Epoch 134/200\n",
            "1/1 - 1s - loss: 0.1347 - acc: 0.9778 - val_loss: 0.1212 - val_acc: 0.9767 - 506ms/epoch - 506ms/step\n",
            "Epoch 135/200\n",
            "1/1 - 1s - loss: 0.1316 - acc: 0.9780 - val_loss: 0.1196 - val_acc: 0.9767 - 501ms/epoch - 501ms/step\n",
            "Epoch 136/200\n",
            "1/1 - 1s - loss: 0.1349 - acc: 0.9778 - val_loss: 0.1181 - val_acc: 0.9767 - 606ms/epoch - 606ms/step\n",
            "Epoch 137/200\n",
            "1/1 - 1s - loss: 0.1258 - acc: 0.9778 - val_loss: 0.1167 - val_acc: 0.9767 - 597ms/epoch - 597ms/step\n",
            "Epoch 138/200\n",
            "1/1 - 1s - loss: 0.1294 - acc: 0.9778 - val_loss: 0.1153 - val_acc: 0.9767 - 511ms/epoch - 511ms/step\n",
            "Epoch 139/200\n",
            "1/1 - 1s - loss: 0.1249 - acc: 0.9780 - val_loss: 0.1141 - val_acc: 0.9767 - 508ms/epoch - 508ms/step\n",
            "Epoch 140/200\n",
            "1/1 - 1s - loss: 0.1222 - acc: 0.9766 - val_loss: 0.1128 - val_acc: 0.9767 - 597ms/epoch - 597ms/step\n",
            "Epoch 141/200\n",
            "1/1 - 1s - loss: 0.1255 - acc: 0.9780 - val_loss: 0.1116 - val_acc: 0.9767 - 983ms/epoch - 983ms/step\n",
            "Epoch 142/200\n",
            "1/1 - 1s - loss: 0.1236 - acc: 0.9780 - val_loss: 0.1105 - val_acc: 0.9767 - 874ms/epoch - 874ms/step\n",
            "Epoch 143/200\n",
            "1/1 - 1s - loss: 0.1281 - acc: 0.9780 - val_loss: 0.1093 - val_acc: 0.9767 - 1s/epoch - 1s/step\n",
            "Epoch 144/200\n",
            "1/1 - 1s - loss: 0.1206 - acc: 0.9780 - val_loss: 0.1083 - val_acc: 0.9767 - 784ms/epoch - 784ms/step\n",
            "Epoch 145/200\n",
            "1/1 - 1s - loss: 0.1179 - acc: 0.9778 - val_loss: 0.1073 - val_acc: 0.9767 - 1s/epoch - 1s/step\n",
            "Epoch 146/200\n",
            "1/1 - 1s - loss: 0.1203 - acc: 0.9783 - val_loss: 0.1063 - val_acc: 0.9767 - 769ms/epoch - 769ms/step\n",
            "Epoch 147/200\n",
            "1/1 - 1s - loss: 0.1243 - acc: 0.9780 - val_loss: 0.1053 - val_acc: 0.9767 - 626ms/epoch - 626ms/step\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 0.2349 - acc: 0.9724\n",
            "\n",
            "Test Set Metrics:\n",
            "\tloss: 0.2349\n",
            "\tacc: 0.9724\n",
            "1/1 [==============================] - 0s 343ms/step\n",
            "Using GCN (local pooling) filters...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1/1 - 3s - loss: 1.5492 - acc: 0.4272 - val_loss: 1.5336 - val_acc: 0.5581 - 3s/epoch - 3s/step\n",
            "Epoch 2/200\n",
            "1/1 - 1s - loss: 1.5339 - acc: 0.5457 - val_loss: 1.5203 - val_acc: 0.5581 - 752ms/epoch - 752ms/step\n",
            "Epoch 3/200\n",
            "1/1 - 1s - loss: 1.5224 - acc: 0.5533 - val_loss: 1.5070 - val_acc: 0.5581 - 759ms/epoch - 759ms/step\n",
            "Epoch 4/200\n",
            "1/1 - 1s - loss: 1.5088 - acc: 0.5770 - val_loss: 1.4941 - val_acc: 0.5581 - 689ms/epoch - 689ms/step\n",
            "Epoch 5/200\n",
            "1/1 - 0s - loss: 1.4882 - acc: 0.5654 - val_loss: 1.4816 - val_acc: 0.5648 - 498ms/epoch - 498ms/step\n",
            "Epoch 6/200\n",
            "1/1 - 1s - loss: 1.4732 - acc: 0.5944 - val_loss: 1.4692 - val_acc: 0.5714 - 511ms/epoch - 511ms/step\n",
            "Epoch 7/200\n",
            "1/1 - 0s - loss: 1.4759 - acc: 0.5975 - val_loss: 1.4571 - val_acc: 0.6146 - 499ms/epoch - 499ms/step\n",
            "Epoch 8/200\n",
            "1/1 - 1s - loss: 1.4418 - acc: 0.6836 - val_loss: 1.4458 - val_acc: 0.7542 - 510ms/epoch - 510ms/step\n",
            "Epoch 9/200\n",
            "1/1 - 1s - loss: 1.4553 - acc: 0.7250 - val_loss: 1.4351 - val_acc: 0.8738 - 608ms/epoch - 608ms/step\n",
            "Epoch 10/200\n",
            "1/1 - 0s - loss: 1.4376 - acc: 0.8387 - val_loss: 1.4247 - val_acc: 0.8771 - 497ms/epoch - 497ms/step\n",
            "Epoch 11/200\n",
            "1/1 - 1s - loss: 1.4289 - acc: 0.8556 - val_loss: 1.4142 - val_acc: 0.8771 - 608ms/epoch - 608ms/step\n",
            "Epoch 12/200\n",
            "1/1 - 0s - loss: 1.4162 - acc: 0.8677 - val_loss: 1.4037 - val_acc: 0.8771 - 489ms/epoch - 489ms/step\n",
            "Epoch 13/200\n",
            "1/1 - 1s - loss: 1.4084 - acc: 0.8669 - val_loss: 1.3931 - val_acc: 0.8771 - 612ms/epoch - 612ms/step\n",
            "Epoch 14/200\n",
            "1/1 - 1s - loss: 1.3970 - acc: 0.8680 - val_loss: 1.3825 - val_acc: 0.8771 - 509ms/epoch - 509ms/step\n",
            "Epoch 15/200\n",
            "1/1 - 1s - loss: 1.3881 - acc: 0.8728 - val_loss: 1.3719 - val_acc: 0.8771 - 601ms/epoch - 601ms/step\n",
            "Epoch 16/200\n",
            "1/1 - 1s - loss: 1.3686 - acc: 0.8700 - val_loss: 1.3617 - val_acc: 0.8771 - 615ms/epoch - 615ms/step\n",
            "Epoch 17/200\n",
            "1/1 - 1s - loss: 1.3630 - acc: 0.8736 - val_loss: 1.3518 - val_acc: 0.8771 - 610ms/epoch - 610ms/step\n",
            "Epoch 18/200\n",
            "1/1 - 1s - loss: 1.3503 - acc: 0.8700 - val_loss: 1.3418 - val_acc: 0.8771 - 510ms/epoch - 510ms/step\n",
            "Epoch 19/200\n",
            "1/1 - 1s - loss: 1.3413 - acc: 0.8745 - val_loss: 1.3317 - val_acc: 0.8771 - 507ms/epoch - 507ms/step\n",
            "Epoch 20/200\n",
            "1/1 - 1s - loss: 1.3271 - acc: 0.8767 - val_loss: 1.3216 - val_acc: 0.8771 - 605ms/epoch - 605ms/step\n",
            "Epoch 21/200\n",
            "1/1 - 1s - loss: 1.3271 - acc: 0.8728 - val_loss: 1.3113 - val_acc: 0.8771 - 936ms/epoch - 936ms/step\n",
            "Epoch 22/200\n",
            "1/1 - 1s - loss: 1.3127 - acc: 0.8750 - val_loss: 1.3009 - val_acc: 0.8771 - 1s/epoch - 1s/step\n",
            "Epoch 23/200\n",
            "1/1 - 1s - loss: 1.2912 - acc: 0.8748 - val_loss: 1.2901 - val_acc: 0.8771 - 1s/epoch - 1s/step\n",
            "Epoch 24/200\n",
            "1/1 - 1s - loss: 1.2914 - acc: 0.8764 - val_loss: 1.2789 - val_acc: 0.8771 - 753ms/epoch - 753ms/step\n",
            "Epoch 25/200\n",
            "1/1 - 1s - loss: 1.2746 - acc: 0.8787 - val_loss: 1.2676 - val_acc: 0.8771 - 774ms/epoch - 774ms/step\n",
            "Epoch 26/200\n",
            "1/1 - 1s - loss: 1.2638 - acc: 0.8742 - val_loss: 1.2559 - val_acc: 0.8771 - 768ms/epoch - 768ms/step\n",
            "Epoch 27/200\n",
            "1/1 - 1s - loss: 1.2532 - acc: 0.8731 - val_loss: 1.2442 - val_acc: 0.8771 - 680ms/epoch - 680ms/step\n",
            "Epoch 28/200\n",
            "1/1 - 1s - loss: 1.2463 - acc: 0.8756 - val_loss: 1.2323 - val_acc: 0.8771 - 599ms/epoch - 599ms/step\n",
            "Epoch 29/200\n",
            "1/1 - 1s - loss: 1.2453 - acc: 0.8728 - val_loss: 1.2202 - val_acc: 0.8771 - 606ms/epoch - 606ms/step\n",
            "Epoch 30/200\n",
            "1/1 - 1s - loss: 1.2105 - acc: 0.8731 - val_loss: 1.2078 - val_acc: 0.8771 - 616ms/epoch - 616ms/step\n",
            "Epoch 31/200\n",
            "1/1 - 1s - loss: 1.2018 - acc: 0.8767 - val_loss: 1.1952 - val_acc: 0.8771 - 530ms/epoch - 530ms/step\n",
            "Epoch 32/200\n",
            "1/1 - 1s - loss: 1.1975 - acc: 0.8770 - val_loss: 1.1822 - val_acc: 0.8771 - 508ms/epoch - 508ms/step\n",
            "Epoch 33/200\n",
            "1/1 - 1s - loss: 1.1763 - acc: 0.8731 - val_loss: 1.1684 - val_acc: 0.8771 - 626ms/epoch - 626ms/step\n",
            "Epoch 34/200\n",
            "1/1 - 0s - loss: 1.1704 - acc: 0.8745 - val_loss: 1.1537 - val_acc: 0.8771 - 498ms/epoch - 498ms/step\n",
            "Epoch 35/200\n",
            "1/1 - 1s - loss: 1.1485 - acc: 0.8759 - val_loss: 1.1385 - val_acc: 0.8771 - 505ms/epoch - 505ms/step\n",
            "Epoch 36/200\n",
            "1/1 - 0s - loss: 1.1280 - acc: 0.8736 - val_loss: 1.1232 - val_acc: 0.8771 - 493ms/epoch - 493ms/step\n",
            "Epoch 37/200\n",
            "1/1 - 1s - loss: 1.1395 - acc: 0.8725 - val_loss: 1.1084 - val_acc: 0.8771 - 501ms/epoch - 501ms/step\n",
            "Epoch 38/200\n",
            "1/1 - 1s - loss: 1.1170 - acc: 0.8745 - val_loss: 1.0938 - val_acc: 0.8771 - 502ms/epoch - 502ms/step\n",
            "Epoch 39/200\n",
            "1/1 - 1s - loss: 1.0874 - acc: 0.8745 - val_loss: 1.0791 - val_acc: 0.8771 - 613ms/epoch - 613ms/step\n",
            "Epoch 40/200\n",
            "1/1 - 1s - loss: 1.0768 - acc: 0.8762 - val_loss: 1.0643 - val_acc: 0.8771 - 508ms/epoch - 508ms/step\n",
            "Epoch 41/200\n",
            "1/1 - 0s - loss: 1.0608 - acc: 0.8759 - val_loss: 1.0495 - val_acc: 0.8771 - 499ms/epoch - 499ms/step\n",
            "Epoch 42/200\n",
            "1/1 - 1s - loss: 1.0511 - acc: 0.8750 - val_loss: 1.0348 - val_acc: 0.8771 - 694ms/epoch - 694ms/step\n",
            "Epoch 43/200\n",
            "1/1 - 1s - loss: 1.0398 - acc: 0.8722 - val_loss: 1.0201 - val_acc: 0.8771 - 900ms/epoch - 900ms/step\n",
            "Epoch 44/200\n",
            "1/1 - 1s - loss: 1.0365 - acc: 0.8748 - val_loss: 1.0058 - val_acc: 0.8771 - 938ms/epoch - 938ms/step\n",
            "Epoch 45/200\n",
            "1/1 - 1s - loss: 1.0239 - acc: 0.8781 - val_loss: 0.9917 - val_acc: 0.8771 - 1s/epoch - 1s/step\n",
            "Epoch 46/200\n",
            "1/1 - 1s - loss: 1.0027 - acc: 0.8838 - val_loss: 0.9776 - val_acc: 0.8771 - 769ms/epoch - 769ms/step\n",
            "Epoch 47/200\n",
            "1/1 - 1s - loss: 0.9904 - acc: 0.8753 - val_loss: 0.9636 - val_acc: 0.8771 - 766ms/epoch - 766ms/step\n",
            "Epoch 48/200\n",
            "1/1 - 1s - loss: 0.9769 - acc: 0.8840 - val_loss: 0.9495 - val_acc: 0.8771 - 570ms/epoch - 570ms/step\n",
            "Epoch 49/200\n",
            "1/1 - 1s - loss: 0.9475 - acc: 0.8790 - val_loss: 0.9358 - val_acc: 0.8970 - 608ms/epoch - 608ms/step\n",
            "Epoch 50/200\n",
            "1/1 - 1s - loss: 0.9463 - acc: 0.8846 - val_loss: 0.9223 - val_acc: 0.9037 - 623ms/epoch - 623ms/step\n",
            "Epoch 51/200\n",
            "1/1 - 1s - loss: 0.9289 - acc: 0.8764 - val_loss: 0.9085 - val_acc: 0.9037 - 606ms/epoch - 606ms/step\n",
            "Epoch 52/200\n",
            "1/1 - 1s - loss: 0.9258 - acc: 0.8874 - val_loss: 0.8944 - val_acc: 0.9037 - 505ms/epoch - 505ms/step\n",
            "Epoch 53/200\n",
            "1/1 - 1s - loss: 0.9027 - acc: 0.8852 - val_loss: 0.8792 - val_acc: 0.9037 - 596ms/epoch - 596ms/step\n",
            "Epoch 54/200\n",
            "1/1 - 1s - loss: 0.8928 - acc: 0.8793 - val_loss: 0.8635 - val_acc: 0.9037 - 602ms/epoch - 602ms/step\n",
            "Epoch 55/200\n",
            "1/1 - 1s - loss: 0.8622 - acc: 0.8871 - val_loss: 0.8478 - val_acc: 0.9037 - 598ms/epoch - 598ms/step\n",
            "Epoch 56/200\n",
            "1/1 - 1s - loss: 0.8688 - acc: 0.8874 - val_loss: 0.8320 - val_acc: 0.9037 - 502ms/epoch - 502ms/step\n",
            "Epoch 57/200\n",
            "1/1 - 1s - loss: 0.8439 - acc: 0.8869 - val_loss: 0.8161 - val_acc: 0.9037 - 606ms/epoch - 606ms/step\n",
            "Epoch 58/200\n",
            "1/1 - 1s - loss: 0.8319 - acc: 0.8874 - val_loss: 0.8002 - val_acc: 0.9037 - 511ms/epoch - 511ms/step\n",
            "Epoch 59/200\n",
            "1/1 - 1s - loss: 0.7970 - acc: 0.8866 - val_loss: 0.7843 - val_acc: 0.9037 - 605ms/epoch - 605ms/step\n",
            "Epoch 60/200\n",
            "1/1 - 1s - loss: 0.7783 - acc: 0.8880 - val_loss: 0.7682 - val_acc: 0.9037 - 597ms/epoch - 597ms/step\n",
            "Epoch 61/200\n",
            "1/1 - 1s - loss: 0.7633 - acc: 0.8874 - val_loss: 0.7522 - val_acc: 0.9037 - 506ms/epoch - 506ms/step\n",
            "Epoch 62/200\n",
            "1/1 - 1s - loss: 0.7672 - acc: 0.8911 - val_loss: 0.7357 - val_acc: 0.9037 - 503ms/epoch - 503ms/step\n",
            "Epoch 63/200\n",
            "1/1 - 1s - loss: 0.7466 - acc: 0.8883 - val_loss: 0.7192 - val_acc: 0.9037 - 509ms/epoch - 509ms/step\n",
            "Epoch 64/200\n",
            "1/1 - 1s - loss: 0.7243 - acc: 0.8880 - val_loss: 0.7028 - val_acc: 0.8970 - 607ms/epoch - 607ms/step\n",
            "Epoch 65/200\n",
            "1/1 - 1s - loss: 0.7220 - acc: 0.8888 - val_loss: 0.6864 - val_acc: 0.8970 - 897ms/epoch - 897ms/step\n",
            "Epoch 66/200\n",
            "1/1 - 1s - loss: 0.6994 - acc: 0.8880 - val_loss: 0.6702 - val_acc: 0.8970 - 936ms/epoch - 936ms/step\n",
            "Epoch 67/200\n",
            "1/1 - 1s - loss: 0.6655 - acc: 0.8885 - val_loss: 0.6541 - val_acc: 0.8970 - 886ms/epoch - 886ms/step\n",
            "Epoch 68/200\n",
            "1/1 - 1s - loss: 0.6577 - acc: 0.8877 - val_loss: 0.6382 - val_acc: 0.8970 - 1s/epoch - 1s/step\n",
            "Epoch 69/200\n",
            "1/1 - 1s - loss: 0.6341 - acc: 0.8880 - val_loss: 0.6224 - val_acc: 0.8970 - 770ms/epoch - 770ms/step\n",
            "Epoch 70/200\n",
            "1/1 - 1s - loss: 0.6444 - acc: 0.8900 - val_loss: 0.6068 - val_acc: 0.8970 - 657ms/epoch - 657ms/step\n",
            "Epoch 71/200\n",
            "1/1 - 1s - loss: 0.6276 - acc: 0.8880 - val_loss: 0.5915 - val_acc: 0.8970 - 605ms/epoch - 605ms/step\n",
            "Epoch 72/200\n",
            "1/1 - 1s - loss: 0.5970 - acc: 0.8874 - val_loss: 0.5765 - val_acc: 0.8970 - 508ms/epoch - 508ms/step\n",
            "Epoch 73/200\n",
            "1/1 - 0s - loss: 0.5744 - acc: 0.8877 - val_loss: 0.5618 - val_acc: 0.8970 - 489ms/epoch - 489ms/step\n",
            "Epoch 74/200\n",
            "1/1 - 0s - loss: 0.5817 - acc: 0.8894 - val_loss: 0.5473 - val_acc: 0.8970 - 500ms/epoch - 500ms/step\n",
            "Epoch 75/200\n",
            "1/1 - 1s - loss: 0.5433 - acc: 0.8905 - val_loss: 0.5331 - val_acc: 0.8970 - 592ms/epoch - 592ms/step\n",
            "Epoch 76/200\n",
            "1/1 - 1s - loss: 0.5455 - acc: 0.8891 - val_loss: 0.5192 - val_acc: 0.8970 - 602ms/epoch - 602ms/step\n",
            "Epoch 77/200\n",
            "1/1 - 1s - loss: 0.5383 - acc: 0.8976 - val_loss: 0.5053 - val_acc: 0.8970 - 523ms/epoch - 523ms/step\n",
            "Epoch 78/200\n",
            "1/1 - 0s - loss: 0.5255 - acc: 0.8930 - val_loss: 0.4910 - val_acc: 0.8970 - 499ms/epoch - 499ms/step\n",
            "Epoch 79/200\n",
            "1/1 - 1s - loss: 0.5113 - acc: 0.8902 - val_loss: 0.4764 - val_acc: 0.8970 - 609ms/epoch - 609ms/step\n",
            "Epoch 80/200\n",
            "1/1 - 1s - loss: 0.4967 - acc: 0.8964 - val_loss: 0.4620 - val_acc: 0.8970 - 599ms/epoch - 599ms/step\n",
            "Epoch 81/200\n",
            "1/1 - 1s - loss: 0.4827 - acc: 0.8885 - val_loss: 0.4479 - val_acc: 0.9037 - 501ms/epoch - 501ms/step\n",
            "Epoch 82/200\n",
            "1/1 - 1s - loss: 0.4612 - acc: 0.9127 - val_loss: 0.4338 - val_acc: 0.9734 - 514ms/epoch - 514ms/step\n",
            "Epoch 83/200\n",
            "1/1 - 1s - loss: 0.4345 - acc: 0.9220 - val_loss: 0.4201 - val_acc: 0.9734 - 611ms/epoch - 611ms/step\n",
            "Epoch 84/200\n",
            "1/1 - 1s - loss: 0.4451 - acc: 0.9634 - val_loss: 0.4065 - val_acc: 0.9734 - 615ms/epoch - 615ms/step\n",
            "Epoch 85/200\n",
            "1/1 - 1s - loss: 0.4278 - acc: 0.9690 - val_loss: 0.3933 - val_acc: 0.9734 - 504ms/epoch - 504ms/step\n",
            "Epoch 86/200\n",
            "1/1 - 1s - loss: 0.4216 - acc: 0.9704 - val_loss: 0.3803 - val_acc: 0.9734 - 1s/epoch - 1s/step\n",
            "Epoch 87/200\n",
            "1/1 - 1s - loss: 0.3877 - acc: 0.9772 - val_loss: 0.3676 - val_acc: 0.9734 - 893ms/epoch - 893ms/step\n",
            "Epoch 88/200\n",
            "1/1 - 1s - loss: 0.3818 - acc: 0.9688 - val_loss: 0.3552 - val_acc: 0.9734 - 1s/epoch - 1s/step\n",
            "Epoch 89/200\n",
            "1/1 - 1s - loss: 0.3777 - acc: 0.9533 - val_loss: 0.3433 - val_acc: 0.9734 - 828ms/epoch - 828ms/step\n",
            "Epoch 90/200\n",
            "1/1 - 1s - loss: 0.3434 - acc: 0.9772 - val_loss: 0.3318 - val_acc: 0.9801 - 1s/epoch - 1s/step\n",
            "Epoch 91/200\n",
            "1/1 - 1s - loss: 0.3366 - acc: 0.9775 - val_loss: 0.3207 - val_acc: 0.9801 - 748ms/epoch - 748ms/step\n",
            "Epoch 92/200\n",
            "1/1 - 1s - loss: 0.3334 - acc: 0.9688 - val_loss: 0.3100 - val_acc: 0.9801 - 757ms/epoch - 757ms/step\n",
            "Epoch 93/200\n",
            "1/1 - 1s - loss: 0.3337 - acc: 0.9685 - val_loss: 0.2998 - val_acc: 0.9801 - 597ms/epoch - 597ms/step\n",
            "Epoch 94/200\n",
            "1/1 - 0s - loss: 0.3159 - acc: 0.9688 - val_loss: 0.2899 - val_acc: 0.9801 - 498ms/epoch - 498ms/step\n",
            "Epoch 95/200\n",
            "1/1 - 1s - loss: 0.3034 - acc: 0.9778 - val_loss: 0.2805 - val_acc: 0.9801 - 605ms/epoch - 605ms/step\n",
            "Epoch 96/200\n",
            "1/1 - 0s - loss: 0.3043 - acc: 0.9780 - val_loss: 0.2715 - val_acc: 0.9801 - 496ms/epoch - 496ms/step\n",
            "Epoch 97/200\n",
            "1/1 - 1s - loss: 0.2807 - acc: 0.9783 - val_loss: 0.2631 - val_acc: 0.9801 - 604ms/epoch - 604ms/step\n",
            "Epoch 98/200\n",
            "1/1 - 1s - loss: 0.2768 - acc: 0.9778 - val_loss: 0.2551 - val_acc: 0.9801 - 598ms/epoch - 598ms/step\n",
            "Epoch 99/200\n",
            "1/1 - 1s - loss: 0.2674 - acc: 0.9780 - val_loss: 0.2474 - val_acc: 0.9801 - 507ms/epoch - 507ms/step\n",
            "Epoch 100/200\n",
            "1/1 - 1s - loss: 0.2686 - acc: 0.9778 - val_loss: 0.2402 - val_acc: 0.9801 - 600ms/epoch - 600ms/step\n",
            "Epoch 101/200\n",
            "1/1 - 0s - loss: 0.2551 - acc: 0.9778 - val_loss: 0.2333 - val_acc: 0.9801 - 493ms/epoch - 493ms/step\n",
            "Epoch 102/200\n",
            "1/1 - 1s - loss: 0.2545 - acc: 0.9766 - val_loss: 0.2267 - val_acc: 0.9801 - 595ms/epoch - 595ms/step\n",
            "Epoch 103/200\n",
            "1/1 - 1s - loss: 0.2443 - acc: 0.9778 - val_loss: 0.2204 - val_acc: 0.9801 - 594ms/epoch - 594ms/step\n",
            "Epoch 104/200\n",
            "1/1 - 1s - loss: 0.2387 - acc: 0.9769 - val_loss: 0.2144 - val_acc: 0.9801 - 506ms/epoch - 506ms/step\n",
            "Epoch 105/200\n",
            "1/1 - 0s - loss: 0.2289 - acc: 0.9778 - val_loss: 0.2086 - val_acc: 0.9801 - 494ms/epoch - 494ms/step\n",
            "Epoch 106/200\n",
            "1/1 - 0s - loss: 0.2311 - acc: 0.9682 - val_loss: 0.2028 - val_acc: 0.9801 - 500ms/epoch - 500ms/step\n",
            "Epoch 107/200\n",
            "1/1 - 1s - loss: 0.2198 - acc: 0.9775 - val_loss: 0.1972 - val_acc: 0.9801 - 604ms/epoch - 604ms/step\n",
            "Epoch 108/200\n",
            "1/1 - 1s - loss: 0.2156 - acc: 0.9772 - val_loss: 0.1919 - val_acc: 0.9801 - 597ms/epoch - 597ms/step\n",
            "Epoch 109/200\n",
            "1/1 - 1s - loss: 0.2105 - acc: 0.9780 - val_loss: 0.1869 - val_acc: 0.9801 - 1s/epoch - 1s/step\n",
            "Epoch 110/200\n",
            "1/1 - 1s - loss: 0.2026 - acc: 0.9693 - val_loss: 0.1821 - val_acc: 0.9801 - 874ms/epoch - 874ms/step\n",
            "Epoch 111/200\n",
            "1/1 - 1s - loss: 0.1962 - acc: 0.9780 - val_loss: 0.1774 - val_acc: 0.9801 - 1s/epoch - 1s/step\n",
            "Epoch 112/200\n",
            "1/1 - 1s - loss: 0.1904 - acc: 0.9778 - val_loss: 0.1730 - val_acc: 0.9801 - 794ms/epoch - 794ms/step\n",
            "Epoch 113/200\n",
            "1/1 - 1s - loss: 0.1860 - acc: 0.9778 - val_loss: 0.1687 - val_acc: 0.9801 - 758ms/epoch - 758ms/step\n",
            "Epoch 114/200\n",
            "1/1 - 1s - loss: 0.1806 - acc: 0.9783 - val_loss: 0.1646 - val_acc: 0.9801 - 762ms/epoch - 762ms/step\n",
            "Epoch 115/200\n",
            "1/1 - 1s - loss: 0.1764 - acc: 0.9778 - val_loss: 0.1607 - val_acc: 0.9801 - 765ms/epoch - 765ms/step\n",
            "Epoch 116/200\n",
            "1/1 - 1s - loss: 0.1801 - acc: 0.9778 - val_loss: 0.1570 - val_acc: 0.9801 - 587ms/epoch - 587ms/step\n",
            "Epoch 117/200\n",
            "1/1 - 1s - loss: 0.1747 - acc: 0.9772 - val_loss: 0.1533 - val_acc: 0.9801 - 606ms/epoch - 606ms/step\n",
            "Epoch 118/200\n",
            "1/1 - 1s - loss: 0.1639 - acc: 0.9769 - val_loss: 0.1499 - val_acc: 0.9801 - 608ms/epoch - 608ms/step\n",
            "Epoch 119/200\n",
            "1/1 - 1s - loss: 0.1752 - acc: 0.9778 - val_loss: 0.1466 - val_acc: 0.9801 - 611ms/epoch - 611ms/step\n",
            "Epoch 120/200\n",
            "1/1 - 0s - loss: 0.1573 - acc: 0.9778 - val_loss: 0.1435 - val_acc: 0.9801 - 498ms/epoch - 498ms/step\n",
            "Epoch 121/200\n",
            "1/1 - 1s - loss: 0.1562 - acc: 0.9778 - val_loss: 0.1405 - val_acc: 0.9801 - 619ms/epoch - 619ms/step\n",
            "Epoch 122/200\n",
            "1/1 - 1s - loss: 0.1582 - acc: 0.9778 - val_loss: 0.1376 - val_acc: 0.9801 - 600ms/epoch - 600ms/step\n",
            "Epoch 123/200\n",
            "1/1 - 1s - loss: 0.1576 - acc: 0.9775 - val_loss: 0.1349 - val_acc: 0.9801 - 598ms/epoch - 598ms/step\n",
            "Epoch 124/200\n",
            "1/1 - 1s - loss: 0.1472 - acc: 0.9772 - val_loss: 0.1323 - val_acc: 0.9801 - 507ms/epoch - 507ms/step\n",
            "Epoch 125/200\n",
            "1/1 - 1s - loss: 0.1468 - acc: 0.9778 - val_loss: 0.1299 - val_acc: 0.9801 - 506ms/epoch - 506ms/step\n",
            "Epoch 126/200\n",
            "1/1 - 1s - loss: 0.1425 - acc: 0.9780 - val_loss: 0.1276 - val_acc: 0.9801 - 508ms/epoch - 508ms/step\n",
            "Epoch 127/200\n",
            "1/1 - 1s - loss: 0.1401 - acc: 0.9778 - val_loss: 0.1253 - val_acc: 0.9801 - 601ms/epoch - 601ms/step\n",
            "Epoch 128/200\n",
            "1/1 - 1s - loss: 0.1393 - acc: 0.9783 - val_loss: 0.1232 - val_acc: 0.9801 - 605ms/epoch - 605ms/step\n",
            "Epoch 129/200\n",
            "1/1 - 1s - loss: 0.1339 - acc: 0.9780 - val_loss: 0.1213 - val_acc: 0.9801 - 598ms/epoch - 598ms/step\n",
            "Epoch 130/200\n",
            "1/1 - 0s - loss: 0.1449 - acc: 0.9780 - val_loss: 0.1194 - val_acc: 0.9801 - 492ms/epoch - 492ms/step\n",
            "Epoch 131/200\n",
            "1/1 - 1s - loss: 0.1310 - acc: 0.9778 - val_loss: 0.1177 - val_acc: 0.9801 - 599ms/epoch - 599ms/step\n",
            "Epoch 132/200\n",
            "1/1 - 1s - loss: 0.1352 - acc: 0.9778 - val_loss: 0.1160 - val_acc: 0.9801 - 606ms/epoch - 606ms/step\n",
            "Epoch 133/200\n",
            "1/1 - 1s - loss: 0.1318 - acc: 0.9780 - val_loss: 0.1144 - val_acc: 0.9801 - 1s/epoch - 1s/step\n",
            "Epoch 134/200\n",
            "1/1 - 1s - loss: 0.1292 - acc: 0.9778 - val_loss: 0.1129 - val_acc: 0.9801 - 1s/epoch - 1s/step\n",
            "Epoch 135/200\n",
            "1/1 - 1s - loss: 0.1246 - acc: 0.9775 - val_loss: 0.1115 - val_acc: 0.9801 - 945ms/epoch - 945ms/step\n",
            "Epoch 136/200\n",
            "1/1 - 1s - loss: 0.1273 - acc: 0.9778 - val_loss: 0.1102 - val_acc: 0.9801 - 769ms/epoch - 769ms/step\n",
            "Epoch 137/200\n",
            "1/1 - 1s - loss: 0.1188 - acc: 0.9780 - val_loss: 0.1089 - val_acc: 0.9801 - 756ms/epoch - 756ms/step\n",
            "Epoch 138/200\n",
            "1/1 - 1s - loss: 0.1258 - acc: 0.9778 - val_loss: 0.1077 - val_acc: 0.9801 - 757ms/epoch - 757ms/step\n",
            "Epoch 139/200\n",
            "1/1 - 1s - loss: 0.1199 - acc: 0.9778 - val_loss: 0.1066 - val_acc: 0.9801 - 598ms/epoch - 598ms/step\n",
            "Epoch 140/200\n",
            "1/1 - 1s - loss: 0.1235 - acc: 0.9780 - val_loss: 0.1055 - val_acc: 0.9801 - 607ms/epoch - 607ms/step\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.3529 - acc: 0.9724\n",
            "\n",
            "Test Set Metrics:\n",
            "\tloss: 0.3529\n",
            "\tacc: 0.9724\n",
            "1/1 [==============================] - 0s 360ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GCN (local pooling) filters...\n",
            "Epoch 1/200\n",
            "1/1 - 3s - loss: 1.7578 - acc: 0.0988 - val_loss: 1.7328 - val_acc: 0.0997 - 3s/epoch - 3s/step\n",
            "Epoch 2/200\n",
            "1/1 - 0s - loss: 1.7365 - acc: 0.0994 - val_loss: 1.7161 - val_acc: 0.0997 - 457ms/epoch - 457ms/step\n",
            "Epoch 3/200\n",
            "1/1 - 0s - loss: 1.7214 - acc: 0.0971 - val_loss: 1.6997 - val_acc: 0.1096 - 484ms/epoch - 484ms/step\n",
            "Epoch 4/200\n",
            "1/1 - 0s - loss: 1.7072 - acc: 0.1084 - val_loss: 1.6836 - val_acc: 0.1096 - 467ms/epoch - 467ms/step\n",
            "Epoch 5/200\n",
            "1/1 - 0s - loss: 1.6908 - acc: 0.0994 - val_loss: 1.6678 - val_acc: 0.1130 - 430ms/epoch - 430ms/step\n",
            "Epoch 6/200\n",
            "1/1 - 0s - loss: 1.6775 - acc: 0.1106 - val_loss: 1.6523 - val_acc: 0.1130 - 380ms/epoch - 380ms/step\n",
            "Epoch 7/200\n",
            "1/1 - 0s - loss: 1.6614 - acc: 0.1002 - val_loss: 1.6373 - val_acc: 0.1130 - 372ms/epoch - 372ms/step\n",
            "Epoch 8/200\n",
            "1/1 - 0s - loss: 1.6465 - acc: 0.1106 - val_loss: 1.6223 - val_acc: 0.1130 - 374ms/epoch - 374ms/step\n",
            "Epoch 9/200\n",
            "1/1 - 0s - loss: 1.6321 - acc: 0.1120 - val_loss: 1.6075 - val_acc: 0.1130 - 376ms/epoch - 376ms/step\n",
            "Epoch 10/200\n",
            "1/1 - 0s - loss: 1.6153 - acc: 0.1098 - val_loss: 1.5930 - val_acc: 0.1130 - 377ms/epoch - 377ms/step\n",
            "Epoch 11/200\n",
            "1/1 - 0s - loss: 1.6039 - acc: 0.1092 - val_loss: 1.5787 - val_acc: 0.1130 - 380ms/epoch - 380ms/step\n",
            "Epoch 12/200\n",
            "1/1 - 0s - loss: 1.5945 - acc: 0.1084 - val_loss: 1.5647 - val_acc: 0.1130 - 367ms/epoch - 367ms/step\n",
            "Epoch 13/200\n",
            "1/1 - 0s - loss: 1.5702 - acc: 0.1103 - val_loss: 1.5511 - val_acc: 0.1130 - 292ms/epoch - 292ms/step\n",
            "Epoch 14/200\n",
            "1/1 - 0s - loss: 1.5604 - acc: 0.1089 - val_loss: 1.5373 - val_acc: 0.1130 - 243ms/epoch - 243ms/step\n",
            "Epoch 15/200\n",
            "1/1 - 0s - loss: 1.5513 - acc: 0.1092 - val_loss: 1.5237 - val_acc: 0.1130 - 243ms/epoch - 243ms/step\n",
            "Epoch 16/200\n",
            "1/1 - 0s - loss: 1.5335 - acc: 0.1092 - val_loss: 1.5102 - val_acc: 0.1130 - 302ms/epoch - 302ms/step\n",
            "Epoch 17/200\n",
            "1/1 - 0s - loss: 1.5147 - acc: 0.1106 - val_loss: 1.4971 - val_acc: 0.1096 - 304ms/epoch - 304ms/step\n",
            "Epoch 18/200\n",
            "1/1 - 0s - loss: 1.5031 - acc: 0.1095 - val_loss: 1.4843 - val_acc: 0.1096 - 239ms/epoch - 239ms/step\n",
            "Epoch 19/200\n",
            "1/1 - 0s - loss: 1.4935 - acc: 0.1109 - val_loss: 1.4716 - val_acc: 0.1096 - 234ms/epoch - 234ms/step\n",
            "Epoch 20/200\n",
            "1/1 - 0s - loss: 1.4789 - acc: 0.1103 - val_loss: 1.4586 - val_acc: 0.1096 - 240ms/epoch - 240ms/step\n",
            "Epoch 21/200\n",
            "1/1 - 0s - loss: 1.4657 - acc: 0.1092 - val_loss: 1.4456 - val_acc: 0.1096 - 235ms/epoch - 235ms/step\n",
            "Epoch 22/200\n",
            "1/1 - 0s - loss: 1.4585 - acc: 0.1131 - val_loss: 1.4325 - val_acc: 0.1096 - 290ms/epoch - 290ms/step\n",
            "Epoch 23/200\n",
            "1/1 - 0s - loss: 1.4356 - acc: 0.1148 - val_loss: 1.4192 - val_acc: 0.1096 - 306ms/epoch - 306ms/step\n",
            "Epoch 24/200\n",
            "1/1 - 0s - loss: 1.4285 - acc: 0.1191 - val_loss: 1.4060 - val_acc: 0.1096 - 241ms/epoch - 241ms/step\n",
            "Epoch 25/200\n",
            "1/1 - 0s - loss: 1.4216 - acc: 0.1244 - val_loss: 1.3927 - val_acc: 0.1096 - 244ms/epoch - 244ms/step\n",
            "Epoch 26/200\n",
            "1/1 - 0s - loss: 1.4042 - acc: 0.1297 - val_loss: 1.3794 - val_acc: 0.1096 - 232ms/epoch - 232ms/step\n",
            "Epoch 27/200\n",
            "1/1 - 0s - loss: 1.3924 - acc: 0.1934 - val_loss: 1.3659 - val_acc: 0.1229 - 241ms/epoch - 241ms/step\n",
            "Epoch 28/200\n",
            "1/1 - 0s - loss: 1.3732 - acc: 0.3189 - val_loss: 1.3523 - val_acc: 0.5349 - 300ms/epoch - 300ms/step\n",
            "Epoch 29/200\n",
            "1/1 - 0s - loss: 1.3558 - acc: 0.4484 - val_loss: 1.3380 - val_acc: 0.7708 - 298ms/epoch - 298ms/step\n",
            "Epoch 30/200\n",
            "1/1 - 0s - loss: 1.3483 - acc: 0.7408 - val_loss: 1.3233 - val_acc: 0.8837 - 240ms/epoch - 240ms/step\n",
            "Epoch 31/200\n",
            "1/1 - 0s - loss: 1.3344 - acc: 0.7656 - val_loss: 1.3085 - val_acc: 0.8837 - 294ms/epoch - 294ms/step\n",
            "Epoch 32/200\n",
            "1/1 - 0s - loss: 1.3131 - acc: 0.8399 - val_loss: 1.2935 - val_acc: 0.8837 - 242ms/epoch - 242ms/step\n",
            "Epoch 33/200\n",
            "1/1 - 0s - loss: 1.2992 - acc: 0.8399 - val_loss: 1.2783 - val_acc: 0.8837 - 288ms/epoch - 288ms/step\n",
            "Epoch 34/200\n",
            "1/1 - 0s - loss: 1.2874 - acc: 0.8534 - val_loss: 1.2629 - val_acc: 0.8837 - 311ms/epoch - 311ms/step\n",
            "Epoch 35/200\n",
            "1/1 - 0s - loss: 1.2674 - acc: 0.8601 - val_loss: 1.2474 - val_acc: 0.8904 - 233ms/epoch - 233ms/step\n",
            "Epoch 36/200\n",
            "1/1 - 0s - loss: 1.2560 - acc: 0.8655 - val_loss: 1.2316 - val_acc: 0.8904 - 236ms/epoch - 236ms/step\n",
            "Epoch 37/200\n",
            "1/1 - 0s - loss: 1.2488 - acc: 0.8652 - val_loss: 1.2157 - val_acc: 0.8937 - 244ms/epoch - 244ms/step\n",
            "Epoch 38/200\n",
            "1/1 - 0s - loss: 1.2252 - acc: 0.8607 - val_loss: 1.1995 - val_acc: 0.9502 - 235ms/epoch - 235ms/step\n",
            "Epoch 39/200\n",
            "1/1 - 0s - loss: 1.2044 - acc: 0.9499 - val_loss: 1.1830 - val_acc: 0.9502 - 240ms/epoch - 240ms/step\n",
            "Epoch 40/200\n",
            "1/1 - 0s - loss: 1.1931 - acc: 0.9296 - val_loss: 1.1661 - val_acc: 0.9502 - 249ms/epoch - 249ms/step\n",
            "Epoch 41/200\n",
            "1/1 - 0s - loss: 1.1721 - acc: 0.9499 - val_loss: 1.1488 - val_acc: 0.9635 - 298ms/epoch - 298ms/step\n",
            "Epoch 42/200\n",
            "1/1 - 1s - loss: 1.1554 - acc: 0.9372 - val_loss: 1.1314 - val_acc: 0.9635 - 562ms/epoch - 562ms/step\n",
            "Epoch 43/200\n",
            "1/1 - 1s - loss: 1.1401 - acc: 0.9536 - val_loss: 1.1139 - val_acc: 0.9635 - 599ms/epoch - 599ms/step\n",
            "Epoch 44/200\n",
            "1/1 - 1s - loss: 1.1227 - acc: 0.9536 - val_loss: 1.0961 - val_acc: 0.9635 - 589ms/epoch - 589ms/step\n",
            "Epoch 45/200\n",
            "1/1 - 1s - loss: 1.1071 - acc: 0.9507 - val_loss: 1.0779 - val_acc: 0.9635 - 574ms/epoch - 574ms/step\n",
            "Epoch 46/200\n",
            "1/1 - 1s - loss: 1.0901 - acc: 0.9544 - val_loss: 1.0595 - val_acc: 0.9635 - 574ms/epoch - 574ms/step\n",
            "Epoch 47/200\n",
            "1/1 - 0s - loss: 1.0610 - acc: 0.9575 - val_loss: 1.0408 - val_acc: 0.9635 - 423ms/epoch - 423ms/step\n",
            "Epoch 48/200\n",
            "1/1 - 0s - loss: 1.0452 - acc: 0.9752 - val_loss: 1.0219 - val_acc: 0.9635 - 401ms/epoch - 401ms/step\n",
            "Epoch 49/200\n",
            "1/1 - 0s - loss: 1.0348 - acc: 0.9569 - val_loss: 1.0029 - val_acc: 0.9635 - 378ms/epoch - 378ms/step\n",
            "Epoch 50/200\n",
            "1/1 - 0s - loss: 1.0098 - acc: 0.9747 - val_loss: 0.9838 - val_acc: 0.9635 - 371ms/epoch - 371ms/step\n",
            "Epoch 51/200\n",
            "1/1 - 0s - loss: 0.9941 - acc: 0.9657 - val_loss: 0.9645 - val_acc: 0.9801 - 377ms/epoch - 377ms/step\n",
            "Epoch 52/200\n",
            "1/1 - 0s - loss: 0.9762 - acc: 0.9685 - val_loss: 0.9452 - val_acc: 0.9801 - 372ms/epoch - 372ms/step\n",
            "Epoch 53/200\n",
            "1/1 - 0s - loss: 0.9646 - acc: 0.9552 - val_loss: 0.9257 - val_acc: 0.9801 - 383ms/epoch - 383ms/step\n",
            "Epoch 54/200\n",
            "1/1 - 0s - loss: 0.9319 - acc: 0.9581 - val_loss: 0.9061 - val_acc: 0.9801 - 266ms/epoch - 266ms/step\n",
            "Epoch 55/200\n",
            "1/1 - 0s - loss: 0.9142 - acc: 0.9744 - val_loss: 0.8865 - val_acc: 0.9801 - 288ms/epoch - 288ms/step\n",
            "Epoch 56/200\n",
            "1/1 - 0s - loss: 0.8919 - acc: 0.9741 - val_loss: 0.8669 - val_acc: 0.9801 - 296ms/epoch - 296ms/step\n",
            "Epoch 57/200\n",
            "1/1 - 0s - loss: 0.8740 - acc: 0.9772 - val_loss: 0.8472 - val_acc: 0.9801 - 234ms/epoch - 234ms/step\n",
            "Epoch 58/200\n",
            "1/1 - 0s - loss: 0.8505 - acc: 0.9775 - val_loss: 0.8274 - val_acc: 0.9801 - 293ms/epoch - 293ms/step\n",
            "Epoch 59/200\n",
            "1/1 - 0s - loss: 0.8360 - acc: 0.9752 - val_loss: 0.8077 - val_acc: 0.9801 - 293ms/epoch - 293ms/step\n",
            "Epoch 60/200\n",
            "1/1 - 0s - loss: 0.8141 - acc: 0.9772 - val_loss: 0.7880 - val_acc: 0.9801 - 234ms/epoch - 234ms/step\n",
            "Epoch 61/200\n",
            "1/1 - 0s - loss: 0.8031 - acc: 0.9747 - val_loss: 0.7684 - val_acc: 0.9801 - 289ms/epoch - 289ms/step\n",
            "Epoch 62/200\n",
            "1/1 - 0s - loss: 0.7737 - acc: 0.9575 - val_loss: 0.7488 - val_acc: 0.9801 - 240ms/epoch - 240ms/step\n",
            "Epoch 63/200\n",
            "1/1 - 0s - loss: 0.7575 - acc: 0.9758 - val_loss: 0.7293 - val_acc: 0.9801 - 290ms/epoch - 290ms/step\n",
            "Epoch 64/200\n",
            "1/1 - 0s - loss: 0.7280 - acc: 0.9758 - val_loss: 0.7098 - val_acc: 0.9801 - 289ms/epoch - 289ms/step\n",
            "Epoch 65/200\n",
            "1/1 - 0s - loss: 0.7258 - acc: 0.9761 - val_loss: 0.6905 - val_acc: 0.9801 - 236ms/epoch - 236ms/step\n",
            "Epoch 66/200\n",
            "1/1 - 0s - loss: 0.7028 - acc: 0.9772 - val_loss: 0.6713 - val_acc: 0.9801 - 234ms/epoch - 234ms/step\n",
            "Epoch 67/200\n",
            "1/1 - 0s - loss: 0.6788 - acc: 0.9766 - val_loss: 0.6522 - val_acc: 0.9801 - 237ms/epoch - 237ms/step\n",
            "Epoch 68/200\n",
            "1/1 - 0s - loss: 0.6563 - acc: 0.9755 - val_loss: 0.6333 - val_acc: 0.9801 - 290ms/epoch - 290ms/step\n",
            "Epoch 69/200\n",
            "1/1 - 0s - loss: 0.6344 - acc: 0.9772 - val_loss: 0.6147 - val_acc: 0.9801 - 295ms/epoch - 295ms/step\n",
            "Epoch 70/200\n",
            "1/1 - 0s - loss: 0.6272 - acc: 0.9764 - val_loss: 0.5962 - val_acc: 0.9801 - 289ms/epoch - 289ms/step\n",
            "Epoch 71/200\n",
            "1/1 - 0s - loss: 0.6041 - acc: 0.9769 - val_loss: 0.5780 - val_acc: 0.9801 - 288ms/epoch - 288ms/step\n",
            "Epoch 72/200\n",
            "1/1 - 0s - loss: 0.5946 - acc: 0.9764 - val_loss: 0.5601 - val_acc: 0.9801 - 246ms/epoch - 246ms/step\n",
            "Epoch 73/200\n",
            "1/1 - 0s - loss: 0.5644 - acc: 0.9772 - val_loss: 0.5425 - val_acc: 0.9801 - 240ms/epoch - 240ms/step\n",
            "Epoch 74/200\n",
            "1/1 - 0s - loss: 0.5563 - acc: 0.9764 - val_loss: 0.5252 - val_acc: 0.9801 - 234ms/epoch - 234ms/step\n",
            "Epoch 75/200\n",
            "1/1 - 0s - loss: 0.5291 - acc: 0.9775 - val_loss: 0.5083 - val_acc: 0.9801 - 290ms/epoch - 290ms/step\n",
            "Epoch 76/200\n",
            "1/1 - 0s - loss: 0.5184 - acc: 0.9778 - val_loss: 0.4918 - val_acc: 0.9801 - 245ms/epoch - 245ms/step\n",
            "Epoch 77/200\n",
            "1/1 - 0s - loss: 0.5004 - acc: 0.9772 - val_loss: 0.4756 - val_acc: 0.9801 - 240ms/epoch - 240ms/step\n",
            "Epoch 78/200\n",
            "1/1 - 0s - loss: 0.4780 - acc: 0.9769 - val_loss: 0.4599 - val_acc: 0.9801 - 245ms/epoch - 245ms/step\n",
            "Epoch 79/200\n",
            "1/1 - 0s - loss: 0.4700 - acc: 0.9769 - val_loss: 0.4447 - val_acc: 0.9801 - 240ms/epoch - 240ms/step\n",
            "Epoch 80/200\n",
            "1/1 - 0s - loss: 0.4637 - acc: 0.9772 - val_loss: 0.4298 - val_acc: 0.9801 - 241ms/epoch - 241ms/step\n",
            "Epoch 81/200\n",
            "1/1 - 0s - loss: 0.4318 - acc: 0.9775 - val_loss: 0.4154 - val_acc: 0.9801 - 239ms/epoch - 239ms/step\n",
            "Epoch 82/200\n",
            "1/1 - 0s - loss: 0.4325 - acc: 0.9775 - val_loss: 0.4015 - val_acc: 0.9801 - 255ms/epoch - 255ms/step\n",
            "Epoch 83/200\n",
            "1/1 - 0s - loss: 0.4153 - acc: 0.9752 - val_loss: 0.3880 - val_acc: 0.9801 - 290ms/epoch - 290ms/step\n",
            "Epoch 84/200\n",
            "1/1 - 0s - loss: 0.4046 - acc: 0.9775 - val_loss: 0.3749 - val_acc: 0.9801 - 432ms/epoch - 432ms/step\n",
            "Epoch 85/200\n",
            "1/1 - 1s - loss: 0.3957 - acc: 0.9769 - val_loss: 0.3624 - val_acc: 0.9801 - 579ms/epoch - 579ms/step\n",
            "Epoch 86/200\n",
            "1/1 - 1s - loss: 0.3749 - acc: 0.9769 - val_loss: 0.3502 - val_acc: 0.9801 - 580ms/epoch - 580ms/step\n",
            "Epoch 87/200\n",
            "1/1 - 0s - loss: 0.3617 - acc: 0.9769 - val_loss: 0.3385 - val_acc: 0.9801 - 428ms/epoch - 428ms/step\n",
            "Epoch 88/200\n",
            "1/1 - 0s - loss: 0.3480 - acc: 0.9769 - val_loss: 0.3273 - val_acc: 0.9801 - 447ms/epoch - 447ms/step\n",
            "Epoch 89/200\n",
            "1/1 - 0s - loss: 0.3430 - acc: 0.9764 - val_loss: 0.3165 - val_acc: 0.9801 - 382ms/epoch - 382ms/step\n",
            "Epoch 90/200\n",
            "1/1 - 0s - loss: 0.3305 - acc: 0.9772 - val_loss: 0.3061 - val_acc: 0.9801 - 368ms/epoch - 368ms/step\n",
            "Epoch 91/200\n",
            "1/1 - 0s - loss: 0.3263 - acc: 0.9778 - val_loss: 0.2961 - val_acc: 0.9801 - 372ms/epoch - 372ms/step\n",
            "Epoch 92/200\n",
            "1/1 - 0s - loss: 0.3135 - acc: 0.9761 - val_loss: 0.2866 - val_acc: 0.9801 - 358ms/epoch - 358ms/step\n",
            "Epoch 93/200\n",
            "1/1 - 0s - loss: 0.3041 - acc: 0.9761 - val_loss: 0.2774 - val_acc: 0.9801 - 376ms/epoch - 376ms/step\n",
            "Epoch 94/200\n",
            "1/1 - 0s - loss: 0.2946 - acc: 0.9772 - val_loss: 0.2686 - val_acc: 0.9801 - 395ms/epoch - 395ms/step\n",
            "Epoch 95/200\n",
            "1/1 - 0s - loss: 0.2837 - acc: 0.9775 - val_loss: 0.2601 - val_acc: 0.9801 - 302ms/epoch - 302ms/step\n",
            "Epoch 96/200\n",
            "1/1 - 0s - loss: 0.2727 - acc: 0.9769 - val_loss: 0.2521 - val_acc: 0.9801 - 236ms/epoch - 236ms/step\n",
            "Epoch 97/200\n",
            "1/1 - 0s - loss: 0.2698 - acc: 0.9769 - val_loss: 0.2444 - val_acc: 0.9801 - 310ms/epoch - 310ms/step\n",
            "Epoch 98/200\n",
            "1/1 - 0s - loss: 0.2640 - acc: 0.9778 - val_loss: 0.2371 - val_acc: 0.9801 - 298ms/epoch - 298ms/step\n",
            "Epoch 99/200\n",
            "1/1 - 0s - loss: 0.2531 - acc: 0.9772 - val_loss: 0.2301 - val_acc: 0.9801 - 290ms/epoch - 290ms/step\n",
            "Epoch 100/200\n",
            "1/1 - 0s - loss: 0.2488 - acc: 0.9769 - val_loss: 0.2234 - val_acc: 0.9801 - 239ms/epoch - 239ms/step\n",
            "Epoch 101/200\n",
            "1/1 - 0s - loss: 0.2303 - acc: 0.9775 - val_loss: 0.2171 - val_acc: 0.9801 - 295ms/epoch - 295ms/step\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 0.9736 - acc: 0.9779\n",
            "\n",
            "Test Set Metrics:\n",
            "\tloss: 0.9736\n",
            "\tacc: 0.9779\n",
            "1/1 [==============================] - 0s 234ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GCN (local pooling) filters...\n",
            "Epoch 1/200\n",
            "1/1 - 4s - loss: 1.6398 - acc: 0.0099 - val_loss: 1.6213 - val_acc: 0.0066 - 4s/epoch - 4s/step\n",
            "Epoch 2/200\n",
            "1/1 - 0s - loss: 1.6207 - acc: 0.0121 - val_loss: 1.6019 - val_acc: 0.0066 - 431ms/epoch - 431ms/step\n",
            "Epoch 3/200\n",
            "1/1 - 1s - loss: 1.6057 - acc: 0.0099 - val_loss: 1.5826 - val_acc: 0.0066 - 567ms/epoch - 567ms/step\n",
            "Epoch 4/200\n",
            "1/1 - 0s - loss: 1.5810 - acc: 0.0346 - val_loss: 1.5633 - val_acc: 0.0066 - 416ms/epoch - 416ms/step\n",
            "Epoch 5/200\n",
            "1/1 - 0s - loss: 1.5645 - acc: 0.0661 - val_loss: 1.5443 - val_acc: 0.0266 - 361ms/epoch - 361ms/step\n",
            "Epoch 6/200\n",
            "1/1 - 0s - loss: 1.5491 - acc: 0.0766 - val_loss: 1.5256 - val_acc: 0.1030 - 363ms/epoch - 363ms/step\n",
            "Epoch 7/200\n",
            "1/1 - 0s - loss: 1.5280 - acc: 0.1511 - val_loss: 1.5072 - val_acc: 0.3189 - 351ms/epoch - 351ms/step\n",
            "Epoch 8/200\n",
            "1/1 - 0s - loss: 1.5120 - acc: 0.2460 - val_loss: 1.4887 - val_acc: 0.3787 - 357ms/epoch - 357ms/step\n",
            "Epoch 9/200\n",
            "1/1 - 0s - loss: 1.4876 - acc: 0.2888 - val_loss: 1.4703 - val_acc: 0.3854 - 365ms/epoch - 365ms/step\n",
            "Epoch 10/200\n",
            "1/1 - 0s - loss: 1.4752 - acc: 0.3330 - val_loss: 1.4516 - val_acc: 0.3920 - 368ms/epoch - 368ms/step\n",
            "Epoch 11/200\n",
            "1/1 - 0s - loss: 1.4563 - acc: 0.3718 - val_loss: 1.4334 - val_acc: 0.6213 - 233ms/epoch - 233ms/step\n",
            "Epoch 12/200\n",
            "1/1 - 0s - loss: 1.4369 - acc: 0.6130 - val_loss: 1.4146 - val_acc: 0.6213 - 289ms/epoch - 289ms/step\n",
            "Epoch 13/200\n",
            "1/1 - 0s - loss: 1.4092 - acc: 0.6209 - val_loss: 1.3956 - val_acc: 0.6246 - 308ms/epoch - 308ms/step\n",
            "Epoch 14/200\n",
            "1/1 - 0s - loss: 1.4022 - acc: 0.6496 - val_loss: 1.3765 - val_acc: 0.7641 - 227ms/epoch - 227ms/step\n",
            "Epoch 15/200\n",
            "1/1 - 0s - loss: 1.3830 - acc: 0.6406 - val_loss: 1.3574 - val_acc: 0.8605 - 300ms/epoch - 300ms/step\n",
            "Epoch 16/200\n",
            "1/1 - 0s - loss: 1.3598 - acc: 0.7478 - val_loss: 1.3382 - val_acc: 0.8605 - 293ms/epoch - 293ms/step\n",
            "Epoch 17/200\n",
            "1/1 - 0s - loss: 1.3272 - acc: 0.8694 - val_loss: 1.3185 - val_acc: 0.8638 - 231ms/epoch - 231ms/step\n",
            "Epoch 18/200\n",
            "1/1 - 0s - loss: 1.3173 - acc: 0.8745 - val_loss: 1.2988 - val_acc: 0.8638 - 225ms/epoch - 225ms/step\n",
            "Epoch 19/200\n",
            "1/1 - 0s - loss: 1.2939 - acc: 0.8767 - val_loss: 1.2789 - val_acc: 0.8638 - 227ms/epoch - 227ms/step\n",
            "Epoch 20/200\n",
            "1/1 - 0s - loss: 1.2775 - acc: 0.8900 - val_loss: 1.2590 - val_acc: 0.8638 - 227ms/epoch - 227ms/step\n",
            "Epoch 21/200\n",
            "1/1 - 0s - loss: 1.2605 - acc: 0.8950 - val_loss: 1.2390 - val_acc: 0.8704 - 238ms/epoch - 238ms/step\n",
            "Epoch 22/200\n",
            "1/1 - 0s - loss: 1.2499 - acc: 0.8728 - val_loss: 1.2187 - val_acc: 0.9402 - 224ms/epoch - 224ms/step\n",
            "Epoch 23/200\n",
            "1/1 - 0s - loss: 1.2249 - acc: 0.9519 - val_loss: 1.1982 - val_acc: 0.9668 - 230ms/epoch - 230ms/step\n",
            "Epoch 24/200\n",
            "1/1 - 0s - loss: 1.1982 - acc: 0.9668 - val_loss: 1.1775 - val_acc: 0.9668 - 236ms/epoch - 236ms/step\n",
            "Epoch 25/200\n",
            "1/1 - 0s - loss: 1.1735 - acc: 0.9555 - val_loss: 1.1565 - val_acc: 0.9668 - 223ms/epoch - 223ms/step\n",
            "Epoch 26/200\n",
            "1/1 - 0s - loss: 1.1495 - acc: 0.9702 - val_loss: 1.1353 - val_acc: 0.9668 - 232ms/epoch - 232ms/step\n",
            "Epoch 27/200\n",
            "1/1 - 0s - loss: 1.1316 - acc: 0.9654 - val_loss: 1.1138 - val_acc: 0.9734 - 289ms/epoch - 289ms/step\n",
            "Epoch 28/200\n",
            "1/1 - 0s - loss: 1.1181 - acc: 0.9659 - val_loss: 1.0923 - val_acc: 0.9734 - 240ms/epoch - 240ms/step\n",
            "Epoch 29/200\n",
            "1/1 - 0s - loss: 1.0995 - acc: 0.9628 - val_loss: 1.0707 - val_acc: 0.9734 - 288ms/epoch - 288ms/step\n",
            "Epoch 30/200\n",
            "1/1 - 0s - loss: 1.0704 - acc: 0.9719 - val_loss: 1.0490 - val_acc: 0.9734 - 293ms/epoch - 293ms/step\n",
            "Epoch 31/200\n",
            "1/1 - 0s - loss: 1.0506 - acc: 0.9766 - val_loss: 1.0273 - val_acc: 0.9734 - 289ms/epoch - 289ms/step\n",
            "Epoch 32/200\n",
            "1/1 - 0s - loss: 1.0395 - acc: 0.9710 - val_loss: 1.0057 - val_acc: 0.9734 - 235ms/epoch - 235ms/step\n",
            "Epoch 33/200\n",
            "1/1 - 0s - loss: 1.0096 - acc: 0.9764 - val_loss: 0.9842 - val_acc: 0.9734 - 222ms/epoch - 222ms/step\n",
            "Epoch 34/200\n",
            "1/1 - 0s - loss: 0.9901 - acc: 0.9654 - val_loss: 0.9625 - val_acc: 0.9734 - 230ms/epoch - 230ms/step\n",
            "Epoch 35/200\n",
            "1/1 - 0s - loss: 0.9572 - acc: 0.9688 - val_loss: 0.9404 - val_acc: 0.9734 - 237ms/epoch - 237ms/step\n",
            "Epoch 36/200\n",
            "1/1 - 0s - loss: 0.9346 - acc: 0.9755 - val_loss: 0.9186 - val_acc: 0.9734 - 229ms/epoch - 229ms/step\n",
            "Epoch 37/200\n",
            "1/1 - 0s - loss: 0.9159 - acc: 0.9665 - val_loss: 0.8970 - val_acc: 0.9734 - 229ms/epoch - 229ms/step\n",
            "Epoch 38/200\n",
            "1/1 - 0s - loss: 0.9043 - acc: 0.9654 - val_loss: 0.8754 - val_acc: 0.9734 - 228ms/epoch - 228ms/step\n",
            "Epoch 39/200\n",
            "1/1 - 0s - loss: 0.8798 - acc: 0.9766 - val_loss: 0.8540 - val_acc: 0.9734 - 303ms/epoch - 303ms/step\n",
            "Epoch 40/200\n",
            "1/1 - 0s - loss: 0.8483 - acc: 0.9567 - val_loss: 0.8328 - val_acc: 0.9734 - 231ms/epoch - 231ms/step\n",
            "Epoch 41/200\n",
            "1/1 - 0s - loss: 0.8257 - acc: 0.9671 - val_loss: 0.8117 - val_acc: 0.9734 - 288ms/epoch - 288ms/step\n",
            "Epoch 42/200\n",
            "1/1 - 1s - loss: 0.8142 - acc: 0.9727 - val_loss: 0.7908 - val_acc: 0.9734 - 538ms/epoch - 538ms/step\n",
            "Epoch 43/200\n",
            "1/1 - 1s - loss: 0.7924 - acc: 0.9747 - val_loss: 0.7701 - val_acc: 0.9734 - 576ms/epoch - 576ms/step\n",
            "Epoch 44/200\n",
            "1/1 - 0s - loss: 0.7683 - acc: 0.9775 - val_loss: 0.7492 - val_acc: 0.9734 - 412ms/epoch - 412ms/step\n",
            "Epoch 45/200\n",
            "1/1 - 1s - loss: 0.7512 - acc: 0.9693 - val_loss: 0.7285 - val_acc: 0.9734 - 580ms/epoch - 580ms/step\n",
            "Epoch 46/200\n",
            "1/1 - 0s - loss: 0.7326 - acc: 0.9758 - val_loss: 0.7081 - val_acc: 0.9734 - 424ms/epoch - 424ms/step\n",
            "Epoch 47/200\n",
            "1/1 - 1s - loss: 0.7111 - acc: 0.9761 - val_loss: 0.6880 - val_acc: 0.9734 - 613ms/epoch - 613ms/step\n",
            "Epoch 48/200\n",
            "1/1 - 0s - loss: 0.6989 - acc: 0.9682 - val_loss: 0.6681 - val_acc: 0.9734 - 373ms/epoch - 373ms/step\n",
            "Epoch 49/200\n",
            "1/1 - 0s - loss: 0.6701 - acc: 0.9671 - val_loss: 0.6484 - val_acc: 0.9734 - 356ms/epoch - 356ms/step\n",
            "Epoch 50/200\n",
            "1/1 - 0s - loss: 0.6496 - acc: 0.9761 - val_loss: 0.6289 - val_acc: 0.9734 - 374ms/epoch - 374ms/step\n",
            "Epoch 51/200\n",
            "1/1 - 0s - loss: 0.6330 - acc: 0.9755 - val_loss: 0.6098 - val_acc: 0.9734 - 365ms/epoch - 365ms/step\n",
            "Epoch 52/200\n",
            "1/1 - 0s - loss: 0.6042 - acc: 0.9682 - val_loss: 0.5911 - val_acc: 0.9734 - 320ms/epoch - 320ms/step\n",
            "Epoch 53/200\n",
            "1/1 - 0s - loss: 0.5957 - acc: 0.9761 - val_loss: 0.5727 - val_acc: 0.9734 - 226ms/epoch - 226ms/step\n",
            "Epoch 54/200\n",
            "1/1 - 0s - loss: 0.5869 - acc: 0.9707 - val_loss: 0.5548 - val_acc: 0.9734 - 292ms/epoch - 292ms/step\n",
            "Epoch 55/200\n",
            "1/1 - 0s - loss: 0.5639 - acc: 0.9766 - val_loss: 0.5371 - val_acc: 0.9734 - 226ms/epoch - 226ms/step\n",
            "Epoch 56/200\n",
            "1/1 - 0s - loss: 0.5395 - acc: 0.9764 - val_loss: 0.5196 - val_acc: 0.9734 - 285ms/epoch - 285ms/step\n",
            "Epoch 57/200\n",
            "1/1 - 0s - loss: 0.5298 - acc: 0.9778 - val_loss: 0.5023 - val_acc: 0.9734 - 249ms/epoch - 249ms/step\n",
            "Epoch 58/200\n",
            "1/1 - 0s - loss: 0.4930 - acc: 0.9752 - val_loss: 0.4855 - val_acc: 0.9734 - 227ms/epoch - 227ms/step\n",
            "Epoch 59/200\n",
            "1/1 - 0s - loss: 0.4868 - acc: 0.9764 - val_loss: 0.4691 - val_acc: 0.9734 - 290ms/epoch - 290ms/step\n",
            "Epoch 60/200\n",
            "1/1 - 0s - loss: 0.4714 - acc: 0.9769 - val_loss: 0.4530 - val_acc: 0.9734 - 223ms/epoch - 223ms/step\n",
            "Epoch 61/200\n",
            "1/1 - 0s - loss: 0.4659 - acc: 0.9758 - val_loss: 0.4374 - val_acc: 0.9734 - 226ms/epoch - 226ms/step\n",
            "Epoch 62/200\n",
            "1/1 - 0s - loss: 0.4563 - acc: 0.9693 - val_loss: 0.4221 - val_acc: 0.9734 - 225ms/epoch - 225ms/step\n",
            "Epoch 63/200\n",
            "1/1 - 0s - loss: 0.4260 - acc: 0.9750 - val_loss: 0.4071 - val_acc: 0.9734 - 227ms/epoch - 227ms/step\n",
            "Epoch 64/200\n",
            "1/1 - 0s - loss: 0.4082 - acc: 0.9780 - val_loss: 0.3927 - val_acc: 0.9734 - 285ms/epoch - 285ms/step\n",
            "Epoch 65/200\n",
            "1/1 - 0s - loss: 0.4045 - acc: 0.9766 - val_loss: 0.3787 - val_acc: 0.9734 - 227ms/epoch - 227ms/step\n",
            "Epoch 66/200\n",
            "1/1 - 0s - loss: 0.3835 - acc: 0.9752 - val_loss: 0.3653 - val_acc: 0.9734 - 226ms/epoch - 226ms/step\n",
            "Epoch 67/200\n",
            "1/1 - 0s - loss: 0.3650 - acc: 0.9761 - val_loss: 0.3525 - val_acc: 0.9734 - 284ms/epoch - 284ms/step\n",
            "Epoch 68/200\n",
            "1/1 - 0s - loss: 0.3521 - acc: 0.9758 - val_loss: 0.3402 - val_acc: 0.9734 - 302ms/epoch - 302ms/step\n",
            "Epoch 69/200\n",
            "1/1 - 0s - loss: 0.3549 - acc: 0.9769 - val_loss: 0.3285 - val_acc: 0.9734 - 229ms/epoch - 229ms/step\n",
            "Epoch 70/200\n",
            "1/1 - 0s - loss: 0.3299 - acc: 0.9769 - val_loss: 0.3172 - val_acc: 0.9734 - 227ms/epoch - 227ms/step\n",
            "Epoch 71/200\n",
            "1/1 - 0s - loss: 0.3212 - acc: 0.9758 - val_loss: 0.3065 - val_acc: 0.9734 - 288ms/epoch - 288ms/step\n",
            "Epoch 72/200\n",
            "1/1 - 0s - loss: 0.3125 - acc: 0.9778 - val_loss: 0.2962 - val_acc: 0.9734 - 284ms/epoch - 284ms/step\n",
            "Epoch 73/200\n",
            "1/1 - 0s - loss: 0.2973 - acc: 0.9769 - val_loss: 0.2864 - val_acc: 0.9734 - 224ms/epoch - 224ms/step\n",
            "Epoch 74/200\n",
            "1/1 - 0s - loss: 0.2964 - acc: 0.9772 - val_loss: 0.2770 - val_acc: 0.9734 - 227ms/epoch - 227ms/step\n",
            "Epoch 75/200\n",
            "1/1 - 0s - loss: 0.2898 - acc: 0.9690 - val_loss: 0.2680 - val_acc: 0.9734 - 302ms/epoch - 302ms/step\n",
            "Epoch 76/200\n",
            "1/1 - 0s - loss: 0.2764 - acc: 0.9769 - val_loss: 0.2594 - val_acc: 0.9734 - 237ms/epoch - 237ms/step\n",
            "Epoch 77/200\n",
            "1/1 - 0s - loss: 0.2675 - acc: 0.9766 - val_loss: 0.2512 - val_acc: 0.9734 - 227ms/epoch - 227ms/step\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 1.1063 - acc: 0.9807\n",
            "\n",
            "Test Set Metrics:\n",
            "\tloss: 1.1063\n",
            "\tacc: 0.9807\n",
            "1/1 [==============================] - 0s 241ms/step\n",
            "Using GCN (local pooling) filters...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1/1 - 2s - loss: 2.2238 - acc: 0.0084 - val_loss: 2.1920 - val_acc: 0.0100 - 2s/epoch - 2s/step\n",
            "Epoch 2/200\n",
            "1/1 - 0s - loss: 2.1965 - acc: 0.0073 - val_loss: 2.1573 - val_acc: 0.0100 - 249ms/epoch - 249ms/step\n",
            "Epoch 3/200\n",
            "1/1 - 0s - loss: 2.1679 - acc: 0.0073 - val_loss: 2.1228 - val_acc: 0.0066 - 229ms/epoch - 229ms/step\n",
            "Epoch 4/200\n",
            "1/1 - 0s - loss: 2.1033 - acc: 0.0070 - val_loss: 2.0886 - val_acc: 0.0066 - 289ms/epoch - 289ms/step\n",
            "Epoch 5/200\n",
            "1/1 - 0s - loss: 2.0919 - acc: 0.0079 - val_loss: 2.0547 - val_acc: 0.0033 - 246ms/epoch - 246ms/step\n",
            "Epoch 6/200\n",
            "1/1 - 0s - loss: 2.0432 - acc: 0.0070 - val_loss: 2.0212 - val_acc: 0.0033 - 228ms/epoch - 228ms/step\n",
            "Epoch 7/200\n",
            "1/1 - 0s - loss: 2.0238 - acc: 0.0082 - val_loss: 1.9880 - val_acc: 0.0033 - 243ms/epoch - 243ms/step\n",
            "Epoch 8/200\n",
            "1/1 - 0s - loss: 1.9845 - acc: 0.0062 - val_loss: 1.9551 - val_acc: 0.0066 - 231ms/epoch - 231ms/step\n",
            "Epoch 9/200\n",
            "1/1 - 0s - loss: 1.9667 - acc: 0.0068 - val_loss: 1.9225 - val_acc: 0.0066 - 289ms/epoch - 289ms/step\n",
            "Epoch 10/200\n",
            "1/1 - 0s - loss: 1.9130 - acc: 0.0172 - val_loss: 1.8903 - val_acc: 0.0066 - 231ms/epoch - 231ms/step\n",
            "Epoch 11/200\n",
            "1/1 - 0s - loss: 1.8932 - acc: 0.0121 - val_loss: 1.8584 - val_acc: 0.0066 - 286ms/epoch - 286ms/step\n",
            "Epoch 12/200\n",
            "1/1 - 0s - loss: 1.8650 - acc: 0.0068 - val_loss: 1.8268 - val_acc: 0.0066 - 298ms/epoch - 298ms/step\n",
            "Epoch 13/200\n",
            "1/1 - 0s - loss: 1.8342 - acc: 0.0175 - val_loss: 1.7956 - val_acc: 0.0066 - 229ms/epoch - 229ms/step\n",
            "Epoch 14/200\n",
            "1/1 - 0s - loss: 1.7953 - acc: 0.0394 - val_loss: 1.7647 - val_acc: 0.0897 - 288ms/epoch - 288ms/step\n",
            "Epoch 15/200\n",
            "1/1 - 0s - loss: 1.7590 - acc: 0.0588 - val_loss: 1.7343 - val_acc: 0.0897 - 287ms/epoch - 287ms/step\n",
            "Epoch 16/200\n",
            "1/1 - 0s - loss: 1.7355 - acc: 0.0709 - val_loss: 1.7043 - val_acc: 0.0930 - 286ms/epoch - 286ms/step\n",
            "Epoch 17/200\n",
            "1/1 - 0s - loss: 1.7128 - acc: 0.0951 - val_loss: 1.6747 - val_acc: 0.0930 - 292ms/epoch - 292ms/step\n",
            "Epoch 18/200\n",
            "1/1 - 0s - loss: 1.6767 - acc: 0.1199 - val_loss: 1.6452 - val_acc: 0.1063 - 236ms/epoch - 236ms/step\n",
            "Epoch 19/200\n",
            "1/1 - 0s - loss: 1.6528 - acc: 0.1095 - val_loss: 1.6160 - val_acc: 0.1063 - 284ms/epoch - 284ms/step\n",
            "Epoch 20/200\n",
            "1/1 - 0s - loss: 1.6167 - acc: 0.1120 - val_loss: 1.5871 - val_acc: 0.1063 - 294ms/epoch - 294ms/step\n",
            "Epoch 21/200\n",
            "1/1 - 0s - loss: 1.5826 - acc: 0.2049 - val_loss: 1.5586 - val_acc: 0.1993 - 293ms/epoch - 293ms/step\n",
            "Epoch 22/200\n",
            "1/1 - 0s - loss: 1.5702 - acc: 0.1838 - val_loss: 1.5304 - val_acc: 0.1993 - 231ms/epoch - 231ms/step\n",
            "Epoch 23/200\n",
            "1/1 - 0s - loss: 1.5403 - acc: 0.2077 - val_loss: 1.5026 - val_acc: 0.3023 - 289ms/epoch - 289ms/step\n",
            "Epoch 24/200\n",
            "1/1 - 0s - loss: 1.5195 - acc: 0.3158 - val_loss: 1.4751 - val_acc: 0.4850 - 285ms/epoch - 285ms/step\n",
            "Epoch 25/200\n",
            "1/1 - 0s - loss: 1.4762 - acc: 0.4236 - val_loss: 1.4479 - val_acc: 0.4850 - 288ms/epoch - 288ms/step\n",
            "Epoch 26/200\n",
            "1/1 - 0s - loss: 1.4591 - acc: 0.4244 - val_loss: 1.4210 - val_acc: 0.4917 - 235ms/epoch - 235ms/step\n",
            "Epoch 27/200\n",
            "1/1 - 0s - loss: 1.4358 - acc: 0.4064 - val_loss: 1.3943 - val_acc: 0.4917 - 241ms/epoch - 241ms/step\n",
            "Epoch 28/200\n",
            "1/1 - 0s - loss: 1.4010 - acc: 0.4453 - val_loss: 1.3679 - val_acc: 0.6678 - 231ms/epoch - 231ms/step\n",
            "Epoch 29/200\n",
            "1/1 - 0s - loss: 1.3710 - acc: 0.6507 - val_loss: 1.3413 - val_acc: 0.6711 - 238ms/epoch - 238ms/step\n",
            "Epoch 30/200\n",
            "1/1 - 0s - loss: 1.3458 - acc: 0.6721 - val_loss: 1.3147 - val_acc: 0.6744 - 287ms/epoch - 287ms/step\n",
            "Epoch 31/200\n",
            "1/1 - 0s - loss: 1.3124 - acc: 0.7717 - val_loss: 1.2890 - val_acc: 0.6744 - 298ms/epoch - 298ms/step\n",
            "Epoch 32/200\n",
            "1/1 - 1s - loss: 1.3030 - acc: 0.6938 - val_loss: 1.2639 - val_acc: 0.8704 - 594ms/epoch - 594ms/step\n",
            "Epoch 33/200\n",
            "1/1 - 1s - loss: 1.2768 - acc: 0.7816 - val_loss: 1.2392 - val_acc: 0.8704 - 587ms/epoch - 587ms/step\n",
            "Epoch 34/200\n",
            "1/1 - 0s - loss: 1.2455 - acc: 0.8005 - val_loss: 1.2148 - val_acc: 0.8704 - 436ms/epoch - 436ms/step\n",
            "Epoch 35/200\n",
            "1/1 - 0s - loss: 1.2145 - acc: 0.8100 - val_loss: 1.1897 - val_acc: 0.8771 - 451ms/epoch - 451ms/step\n",
            "Epoch 36/200\n",
            "1/1 - 0s - loss: 1.1843 - acc: 0.8528 - val_loss: 1.1643 - val_acc: 0.8771 - 468ms/epoch - 468ms/step\n",
            "Epoch 37/200\n",
            "1/1 - 0s - loss: 1.1825 - acc: 0.9257 - val_loss: 1.1392 - val_acc: 0.8837 - 407ms/epoch - 407ms/step\n",
            "Epoch 38/200\n",
            "1/1 - 0s - loss: 1.1428 - acc: 0.9164 - val_loss: 1.1142 - val_acc: 0.8904 - 353ms/epoch - 353ms/step\n",
            "Epoch 39/200\n",
            "1/1 - 0s - loss: 1.1114 - acc: 0.8905 - val_loss: 1.0899 - val_acc: 0.9668 - 369ms/epoch - 369ms/step\n",
            "Epoch 40/200\n",
            "1/1 - 0s - loss: 1.0960 - acc: 0.9288 - val_loss: 1.0634 - val_acc: 0.9767 - 377ms/epoch - 377ms/step\n",
            "Epoch 41/200\n",
            "1/1 - 0s - loss: 1.0596 - acc: 0.9395 - val_loss: 1.0358 - val_acc: 0.9767 - 374ms/epoch - 374ms/step\n",
            "Epoch 42/200\n",
            "1/1 - 0s - loss: 1.0415 - acc: 0.9488 - val_loss: 1.0081 - val_acc: 0.9767 - 366ms/epoch - 366ms/step\n",
            "Epoch 43/200\n",
            "1/1 - 0s - loss: 1.0222 - acc: 0.9372 - val_loss: 0.9804 - val_acc: 0.9767 - 326ms/epoch - 326ms/step\n",
            "Epoch 44/200\n",
            "1/1 - 0s - loss: 0.9811 - acc: 0.9536 - val_loss: 0.9531 - val_acc: 0.9801 - 236ms/epoch - 236ms/step\n",
            "Epoch 45/200\n",
            "1/1 - 0s - loss: 0.9580 - acc: 0.9530 - val_loss: 0.9256 - val_acc: 0.9801 - 284ms/epoch - 284ms/step\n",
            "Epoch 46/200\n",
            "1/1 - 0s - loss: 0.9397 - acc: 0.9769 - val_loss: 0.8981 - val_acc: 0.9801 - 241ms/epoch - 241ms/step\n",
            "Epoch 47/200\n",
            "1/1 - 0s - loss: 0.9131 - acc: 0.9721 - val_loss: 0.8714 - val_acc: 0.9801 - 231ms/epoch - 231ms/step\n",
            "Epoch 48/200\n",
            "1/1 - 0s - loss: 0.8893 - acc: 0.9766 - val_loss: 0.8450 - val_acc: 0.9801 - 295ms/epoch - 295ms/step\n",
            "Epoch 49/200\n",
            "1/1 - 0s - loss: 0.8565 - acc: 0.9735 - val_loss: 0.8191 - val_acc: 0.9801 - 286ms/epoch - 286ms/step\n",
            "Epoch 50/200\n",
            "1/1 - 0s - loss: 0.8245 - acc: 0.9721 - val_loss: 0.7937 - val_acc: 0.9801 - 245ms/epoch - 245ms/step\n",
            "Epoch 51/200\n",
            "1/1 - 0s - loss: 0.8058 - acc: 0.9707 - val_loss: 0.7688 - val_acc: 0.9801 - 232ms/epoch - 232ms/step\n",
            "Epoch 52/200\n",
            "1/1 - 0s - loss: 0.7843 - acc: 0.9778 - val_loss: 0.7445 - val_acc: 0.9801 - 292ms/epoch - 292ms/step\n",
            "Epoch 53/200\n",
            "1/1 - 0s - loss: 0.7641 - acc: 0.9758 - val_loss: 0.7208 - val_acc: 0.9801 - 287ms/epoch - 287ms/step\n",
            "Epoch 54/200\n",
            "1/1 - 0s - loss: 0.7372 - acc: 0.9778 - val_loss: 0.6978 - val_acc: 0.9801 - 234ms/epoch - 234ms/step\n",
            "Epoch 55/200\n",
            "1/1 - 0s - loss: 0.7101 - acc: 0.9775 - val_loss: 0.6755 - val_acc: 0.9801 - 286ms/epoch - 286ms/step\n",
            "Epoch 56/200\n",
            "1/1 - 0s - loss: 0.6767 - acc: 0.9772 - val_loss: 0.6541 - val_acc: 0.9801 - 240ms/epoch - 240ms/step\n",
            "Epoch 57/200\n",
            "1/1 - 0s - loss: 0.6738 - acc: 0.9766 - val_loss: 0.6334 - val_acc: 0.9801 - 298ms/epoch - 298ms/step\n",
            "Epoch 58/200\n",
            "1/1 - 0s - loss: 0.6396 - acc: 0.9775 - val_loss: 0.6134 - val_acc: 0.9801 - 246ms/epoch - 246ms/step\n",
            "Epoch 59/200\n",
            "1/1 - 0s - loss: 0.6270 - acc: 0.9690 - val_loss: 0.5940 - val_acc: 0.9801 - 287ms/epoch - 287ms/step\n",
            "Epoch 60/200\n",
            "1/1 - 0s - loss: 0.6085 - acc: 0.9778 - val_loss: 0.5751 - val_acc: 0.9801 - 291ms/epoch - 291ms/step\n",
            "Epoch 61/200\n",
            "1/1 - 0s - loss: 0.5939 - acc: 0.9764 - val_loss: 0.5569 - val_acc: 0.9801 - 288ms/epoch - 288ms/step\n",
            "Epoch 62/200\n",
            "1/1 - 0s - loss: 0.5608 - acc: 0.9764 - val_loss: 0.5392 - val_acc: 0.9801 - 285ms/epoch - 285ms/step\n",
            "Epoch 63/200\n",
            "1/1 - 0s - loss: 0.5400 - acc: 0.9783 - val_loss: 0.5221 - val_acc: 0.9801 - 237ms/epoch - 237ms/step\n",
            "Epoch 64/200\n",
            "1/1 - 0s - loss: 0.5270 - acc: 0.9775 - val_loss: 0.5054 - val_acc: 0.9801 - 235ms/epoch - 235ms/step\n",
            "Epoch 65/200\n",
            "1/1 - 0s - loss: 0.5264 - acc: 0.9780 - val_loss: 0.4893 - val_acc: 0.9801 - 227ms/epoch - 227ms/step\n",
            "Epoch 66/200\n",
            "1/1 - 0s - loss: 0.4967 - acc: 0.9786 - val_loss: 0.4737 - val_acc: 0.9801 - 263ms/epoch - 263ms/step\n",
            "Epoch 67/200\n",
            "1/1 - 0s - loss: 0.4804 - acc: 0.9780 - val_loss: 0.4586 - val_acc: 0.9801 - 232ms/epoch - 232ms/step\n",
            "Epoch 68/200\n",
            "1/1 - 0s - loss: 0.4692 - acc: 0.9775 - val_loss: 0.4442 - val_acc: 0.9801 - 231ms/epoch - 231ms/step\n",
            "Epoch 69/200\n",
            "1/1 - 0s - loss: 0.4494 - acc: 0.9769 - val_loss: 0.4303 - val_acc: 0.9801 - 233ms/epoch - 233ms/step\n",
            "Epoch 70/200\n",
            "1/1 - 0s - loss: 0.4438 - acc: 0.9778 - val_loss: 0.4170 - val_acc: 0.9801 - 297ms/epoch - 297ms/step\n",
            "Epoch 71/200\n",
            "1/1 - 0s - loss: 0.4361 - acc: 0.9761 - val_loss: 0.4042 - val_acc: 0.9801 - 286ms/epoch - 286ms/step\n",
            "Epoch 72/200\n",
            "1/1 - 0s - loss: 0.4304 - acc: 0.9783 - val_loss: 0.3919 - val_acc: 0.9801 - 240ms/epoch - 240ms/step\n",
            "Epoch 73/200\n",
            "1/1 - 0s - loss: 0.4091 - acc: 0.9769 - val_loss: 0.3801 - val_acc: 0.9801 - 233ms/epoch - 233ms/step\n",
            "Epoch 74/200\n",
            "1/1 - 0s - loss: 0.3975 - acc: 0.9775 - val_loss: 0.3688 - val_acc: 0.9801 - 306ms/epoch - 306ms/step\n",
            "Epoch 75/200\n",
            "1/1 - 1s - loss: 0.3691 - acc: 0.9775 - val_loss: 0.3579 - val_acc: 0.9801 - 588ms/epoch - 588ms/step\n",
            "Epoch 76/200\n",
            "1/1 - 0s - loss: 0.3653 - acc: 0.9775 - val_loss: 0.3475 - val_acc: 0.9801 - 412ms/epoch - 412ms/step\n",
            "Epoch 77/200\n",
            "1/1 - 1s - loss: 0.3561 - acc: 0.9772 - val_loss: 0.3375 - val_acc: 0.9801 - 568ms/epoch - 568ms/step\n",
            "Epoch 78/200\n",
            "1/1 - 0s - loss: 0.3414 - acc: 0.9780 - val_loss: 0.3279 - val_acc: 0.9801 - 447ms/epoch - 447ms/step\n",
            "Epoch 79/200\n",
            "1/1 - 0s - loss: 0.3362 - acc: 0.9778 - val_loss: 0.3189 - val_acc: 0.9801 - 412ms/epoch - 412ms/step\n",
            "Epoch 80/200\n",
            "1/1 - 1s - loss: 0.3267 - acc: 0.9789 - val_loss: 0.3102 - val_acc: 0.9801 - 568ms/epoch - 568ms/step\n",
            "Epoch 81/200\n",
            "1/1 - 0s - loss: 0.3275 - acc: 0.9769 - val_loss: 0.3019 - val_acc: 0.9801 - 367ms/epoch - 367ms/step\n",
            "Epoch 82/200\n",
            "1/1 - 0s - loss: 0.3176 - acc: 0.9780 - val_loss: 0.2939 - val_acc: 0.9801 - 373ms/epoch - 373ms/step\n",
            "Epoch 83/200\n",
            "1/1 - 0s - loss: 0.3073 - acc: 0.9786 - val_loss: 0.2863 - val_acc: 0.9801 - 356ms/epoch - 356ms/step\n",
            "Epoch 84/200\n",
            "1/1 - 0s - loss: 0.3024 - acc: 0.9778 - val_loss: 0.2789 - val_acc: 0.9801 - 373ms/epoch - 373ms/step\n",
            "Epoch 85/200\n",
            "1/1 - 0s - loss: 0.2900 - acc: 0.9766 - val_loss: 0.2719 - val_acc: 0.9801 - 379ms/epoch - 379ms/step\n",
            "Epoch 86/200\n",
            "1/1 - 0s - loss: 0.2794 - acc: 0.9769 - val_loss: 0.2652 - val_acc: 0.9801 - 367ms/epoch - 367ms/step\n",
            "Epoch 87/200\n",
            "1/1 - 0s - loss: 0.2758 - acc: 0.9780 - val_loss: 0.2587 - val_acc: 0.9801 - 300ms/epoch - 300ms/step\n",
            "Epoch 88/200\n",
            "1/1 - 0s - loss: 0.2689 - acc: 0.9775 - val_loss: 0.2525 - val_acc: 0.9801 - 283ms/epoch - 283ms/step\n",
            "Epoch 89/200\n",
            "1/1 - 0s - loss: 0.2583 - acc: 0.9786 - val_loss: 0.2466 - val_acc: 0.9801 - 301ms/epoch - 301ms/step\n",
            "Epoch 90/200\n",
            "1/1 - 0s - loss: 0.2637 - acc: 0.9780 - val_loss: 0.2409 - val_acc: 0.9801 - 291ms/epoch - 291ms/step\n",
            "Epoch 91/200\n",
            "1/1 - 0s - loss: 0.2546 - acc: 0.9786 - val_loss: 0.2356 - val_acc: 0.9801 - 290ms/epoch - 290ms/step\n",
            "Epoch 92/200\n",
            "1/1 - 0s - loss: 0.2487 - acc: 0.9786 - val_loss: 0.2304 - val_acc: 0.9801 - 287ms/epoch - 287ms/step\n",
            "Epoch 93/200\n",
            "1/1 - 0s - loss: 0.2404 - acc: 0.9758 - val_loss: 0.2255 - val_acc: 0.9801 - 287ms/epoch - 287ms/step\n",
            "Epoch 94/200\n",
            "1/1 - 0s - loss: 0.2389 - acc: 0.9783 - val_loss: 0.2208 - val_acc: 0.9801 - 289ms/epoch - 289ms/step\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.9605 - acc: 0.9669\n",
            "\n",
            "Test Set Metrics:\n",
            "\tloss: 0.9605\n",
            "\tacc: 0.9669\n",
            "1/1 [==============================] - 0s 226ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GCN (local pooling) filters...\n",
            "Epoch 1/200\n",
            "1/1 - 2s - loss: 1.7387 - acc: 0.0177 - val_loss: 1.7122 - val_acc: 0.0133 - 2s/epoch - 2s/step\n",
            "Epoch 2/200\n",
            "1/1 - 0s - loss: 1.7209 - acc: 0.0121 - val_loss: 1.6932 - val_acc: 0.0133 - 461ms/epoch - 461ms/step\n",
            "Epoch 3/200\n",
            "1/1 - 1s - loss: 1.7061 - acc: 0.0172 - val_loss: 1.6750 - val_acc: 0.0133 - 588ms/epoch - 588ms/step\n",
            "Epoch 4/200\n",
            "1/1 - 1s - loss: 1.6744 - acc: 0.0225 - val_loss: 1.6571 - val_acc: 0.0133 - 581ms/epoch - 581ms/step\n",
            "Epoch 5/200\n",
            "1/1 - 0s - loss: 1.6559 - acc: 0.0127 - val_loss: 1.6395 - val_acc: 0.0133 - 451ms/epoch - 451ms/step\n",
            "Epoch 6/200\n",
            "1/1 - 0s - loss: 1.6486 - acc: 0.0194 - val_loss: 1.6223 - val_acc: 0.0133 - 407ms/epoch - 407ms/step\n",
            "Epoch 7/200\n",
            "1/1 - 0s - loss: 1.6241 - acc: 0.0180 - val_loss: 1.6055 - val_acc: 0.0133 - 388ms/epoch - 388ms/step\n",
            "Epoch 8/200\n",
            "1/1 - 0s - loss: 1.6171 - acc: 0.0504 - val_loss: 1.5891 - val_acc: 0.0133 - 374ms/epoch - 374ms/step\n",
            "Epoch 9/200\n",
            "1/1 - 1s - loss: 1.5943 - acc: 0.0172 - val_loss: 1.5730 - val_acc: 0.0133 - 536ms/epoch - 536ms/step\n",
            "Epoch 10/200\n",
            "1/1 - 0s - loss: 1.5846 - acc: 0.0093 - val_loss: 1.5571 - val_acc: 0.0133 - 372ms/epoch - 372ms/step\n",
            "Epoch 11/200\n",
            "1/1 - 1s - loss: 1.5580 - acc: 0.0191 - val_loss: 1.5414 - val_acc: 0.0133 - 548ms/epoch - 548ms/step\n",
            "Epoch 12/200\n",
            "1/1 - 0s - loss: 1.5454 - acc: 0.0236 - val_loss: 1.5261 - val_acc: 0.0133 - 299ms/epoch - 299ms/step\n",
            "Epoch 13/200\n",
            "1/1 - 0s - loss: 1.5308 - acc: 0.0310 - val_loss: 1.5114 - val_acc: 0.0133 - 231ms/epoch - 231ms/step\n",
            "Epoch 14/200\n",
            "1/1 - 0s - loss: 1.5203 - acc: 0.0183 - val_loss: 1.4970 - val_acc: 0.0100 - 244ms/epoch - 244ms/step\n",
            "Epoch 15/200\n",
            "1/1 - 0s - loss: 1.5005 - acc: 0.0256 - val_loss: 1.4828 - val_acc: 0.0133 - 241ms/epoch - 241ms/step\n",
            "Epoch 16/200\n",
            "1/1 - 0s - loss: 1.4821 - acc: 0.0374 - val_loss: 1.4687 - val_acc: 0.0133 - 236ms/epoch - 236ms/step\n",
            "Epoch 17/200\n",
            "1/1 - 0s - loss: 1.4682 - acc: 0.0678 - val_loss: 1.4544 - val_acc: 0.0133 - 256ms/epoch - 256ms/step\n",
            "Epoch 18/200\n",
            "1/1 - 0s - loss: 1.4580 - acc: 0.0861 - val_loss: 1.4401 - val_acc: 0.0133 - 299ms/epoch - 299ms/step\n",
            "Epoch 19/200\n",
            "1/1 - 0s - loss: 1.4451 - acc: 0.0284 - val_loss: 1.4258 - val_acc: 0.0133 - 243ms/epoch - 243ms/step\n",
            "Epoch 20/200\n",
            "1/1 - 0s - loss: 1.4341 - acc: 0.1016 - val_loss: 1.4112 - val_acc: 0.0332 - 298ms/epoch - 298ms/step\n",
            "Epoch 21/200\n",
            "1/1 - 0s - loss: 1.4145 - acc: 0.1072 - val_loss: 1.3965 - val_acc: 0.1130 - 242ms/epoch - 242ms/step\n",
            "Epoch 22/200\n",
            "1/1 - 0s - loss: 1.3988 - acc: 0.1064 - val_loss: 1.3817 - val_acc: 0.1130 - 295ms/epoch - 295ms/step\n",
            "Epoch 23/200\n",
            "1/1 - 0s - loss: 1.3856 - acc: 0.1084 - val_loss: 1.3666 - val_acc: 0.1130 - 296ms/epoch - 296ms/step\n",
            "Epoch 24/200\n",
            "1/1 - 0s - loss: 1.3732 - acc: 0.1036 - val_loss: 1.3513 - val_acc: 0.1130 - 289ms/epoch - 289ms/step\n",
            "Epoch 25/200\n",
            "1/1 - 0s - loss: 1.3585 - acc: 0.0816 - val_loss: 1.3359 - val_acc: 0.1196 - 245ms/epoch - 245ms/step\n",
            "Epoch 26/200\n",
            "1/1 - 0s - loss: 1.3350 - acc: 0.1115 - val_loss: 1.3207 - val_acc: 0.1196 - 290ms/epoch - 290ms/step\n",
            "Epoch 27/200\n",
            "1/1 - 0s - loss: 1.3280 - acc: 0.1075 - val_loss: 1.3056 - val_acc: 0.1196 - 302ms/epoch - 302ms/step\n",
            "Epoch 28/200\n",
            "1/1 - 0s - loss: 1.3134 - acc: 0.1238 - val_loss: 1.2904 - val_acc: 0.1196 - 239ms/epoch - 239ms/step\n",
            "Epoch 29/200\n",
            "1/1 - 0s - loss: 1.2944 - acc: 0.1160 - val_loss: 1.2753 - val_acc: 0.1196 - 294ms/epoch - 294ms/step\n",
            "Epoch 30/200\n",
            "1/1 - 0s - loss: 1.2906 - acc: 0.1236 - val_loss: 1.2605 - val_acc: 0.1296 - 298ms/epoch - 298ms/step\n",
            "Epoch 31/200\n",
            "1/1 - 0s - loss: 1.2653 - acc: 0.1525 - val_loss: 1.2457 - val_acc: 0.3256 - 240ms/epoch - 240ms/step\n",
            "Epoch 32/200\n",
            "1/1 - 0s - loss: 1.2519 - acc: 0.4295 - val_loss: 1.2308 - val_acc: 0.5282 - 301ms/epoch - 301ms/step\n",
            "Epoch 33/200\n",
            "1/1 - 0s - loss: 1.2350 - acc: 0.5249 - val_loss: 1.2159 - val_acc: 0.6578 - 236ms/epoch - 236ms/step\n",
            "Epoch 34/200\n",
            "1/1 - 0s - loss: 1.2157 - acc: 0.5283 - val_loss: 1.2009 - val_acc: 0.7608 - 245ms/epoch - 245ms/step\n",
            "Epoch 35/200\n",
            "1/1 - 0s - loss: 1.2062 - acc: 0.7098 - val_loss: 1.1860 - val_acc: 0.7641 - 298ms/epoch - 298ms/step\n",
            "Epoch 36/200\n",
            "1/1 - 0s - loss: 1.1867 - acc: 0.7675 - val_loss: 1.1709 - val_acc: 0.8339 - 242ms/epoch - 242ms/step\n",
            "Epoch 37/200\n",
            "1/1 - 0s - loss: 1.1743 - acc: 0.8745 - val_loss: 1.1556 - val_acc: 0.9336 - 292ms/epoch - 292ms/step\n",
            "Epoch 38/200\n",
            "1/1 - 0s - loss: 1.1638 - acc: 0.9184 - val_loss: 1.1402 - val_acc: 0.9668 - 406ms/epoch - 406ms/step\n",
            "Epoch 39/200\n",
            "1/1 - 1s - loss: 1.1459 - acc: 0.9561 - val_loss: 1.1248 - val_acc: 0.9668 - 569ms/epoch - 569ms/step\n",
            "Epoch 40/200\n",
            "1/1 - 1s - loss: 1.1310 - acc: 0.9474 - val_loss: 1.1091 - val_acc: 0.9867 - 575ms/epoch - 575ms/step\n",
            "Epoch 41/200\n",
            "1/1 - 1s - loss: 1.1073 - acc: 0.9502 - val_loss: 1.0933 - val_acc: 0.9867 - 578ms/epoch - 578ms/step\n",
            "Epoch 42/200\n",
            "1/1 - 1s - loss: 1.0962 - acc: 0.9688 - val_loss: 1.0775 - val_acc: 0.9867 - 588ms/epoch - 588ms/step\n",
            "Epoch 43/200\n",
            "1/1 - 0s - loss: 1.0796 - acc: 0.9696 - val_loss: 1.0615 - val_acc: 0.9867 - 417ms/epoch - 417ms/step\n",
            "Epoch 44/200\n",
            "1/1 - 0s - loss: 1.0658 - acc: 0.9797 - val_loss: 1.0453 - val_acc: 0.9867 - 372ms/epoch - 372ms/step\n",
            "Epoch 45/200\n",
            "1/1 - 0s - loss: 1.0443 - acc: 0.9772 - val_loss: 1.0291 - val_acc: 0.9867 - 377ms/epoch - 377ms/step\n",
            "Epoch 46/200\n",
            "1/1 - 0s - loss: 1.0283 - acc: 0.9775 - val_loss: 1.0123 - val_acc: 0.9867 - 368ms/epoch - 368ms/step\n",
            "Epoch 47/200\n",
            "1/1 - 0s - loss: 1.0200 - acc: 0.9778 - val_loss: 0.9953 - val_acc: 0.9867 - 370ms/epoch - 370ms/step\n",
            "Epoch 48/200\n",
            "1/1 - 0s - loss: 1.0024 - acc: 0.9809 - val_loss: 0.9779 - val_acc: 0.9867 - 370ms/epoch - 370ms/step\n",
            "Epoch 49/200\n",
            "1/1 - 0s - loss: 0.9877 - acc: 0.9758 - val_loss: 0.9603 - val_acc: 0.9867 - 290ms/epoch - 290ms/step\n",
            "Epoch 50/200\n",
            "1/1 - 0s - loss: 0.9680 - acc: 0.9761 - val_loss: 0.9427 - val_acc: 0.9867 - 291ms/epoch - 291ms/step\n",
            "Epoch 51/200\n",
            "1/1 - 0s - loss: 0.9385 - acc: 0.9778 - val_loss: 0.9249 - val_acc: 0.9867 - 293ms/epoch - 293ms/step\n",
            "Epoch 52/200\n",
            "1/1 - 0s - loss: 0.9268 - acc: 0.9814 - val_loss: 0.9071 - val_acc: 0.9867 - 241ms/epoch - 241ms/step\n",
            "Epoch 53/200\n",
            "1/1 - 0s - loss: 0.9130 - acc: 0.9786 - val_loss: 0.8892 - val_acc: 0.9867 - 236ms/epoch - 236ms/step\n",
            "Epoch 54/200\n",
            "1/1 - 0s - loss: 0.8937 - acc: 0.9786 - val_loss: 0.8713 - val_acc: 0.9867 - 307ms/epoch - 307ms/step\n",
            "Epoch 55/200\n",
            "1/1 - 0s - loss: 0.8732 - acc: 0.9811 - val_loss: 0.8534 - val_acc: 0.9867 - 239ms/epoch - 239ms/step\n",
            "Epoch 56/200\n",
            "1/1 - 0s - loss: 0.8568 - acc: 0.9772 - val_loss: 0.8355 - val_acc: 0.9867 - 294ms/epoch - 294ms/step\n",
            "Epoch 57/200\n",
            "1/1 - 0s - loss: 0.8311 - acc: 0.9758 - val_loss: 0.8177 - val_acc: 0.9867 - 294ms/epoch - 294ms/step\n",
            "Epoch 58/200\n",
            "1/1 - 0s - loss: 0.8139 - acc: 0.9797 - val_loss: 0.7998 - val_acc: 0.9867 - 294ms/epoch - 294ms/step\n",
            "Epoch 59/200\n",
            "1/1 - 0s - loss: 0.8035 - acc: 0.9775 - val_loss: 0.7820 - val_acc: 0.9867 - 237ms/epoch - 237ms/step\n",
            "Epoch 60/200\n",
            "1/1 - 0s - loss: 0.7841 - acc: 0.9789 - val_loss: 0.7643 - val_acc: 0.9867 - 292ms/epoch - 292ms/step\n",
            "Epoch 61/200\n",
            "1/1 - 0s - loss: 0.7762 - acc: 0.9780 - val_loss: 0.7468 - val_acc: 0.9867 - 299ms/epoch - 299ms/step\n",
            "Epoch 62/200\n",
            "1/1 - 0s - loss: 0.7634 - acc: 0.9758 - val_loss: 0.7293 - val_acc: 0.9867 - 236ms/epoch - 236ms/step\n",
            "Epoch 63/200\n",
            "1/1 - 0s - loss: 0.7350 - acc: 0.9780 - val_loss: 0.7120 - val_acc: 0.9867 - 296ms/epoch - 296ms/step\n",
            "Epoch 64/200\n",
            "1/1 - 0s - loss: 0.7080 - acc: 0.9789 - val_loss: 0.6947 - val_acc: 0.9867 - 243ms/epoch - 243ms/step\n",
            "Epoch 65/200\n",
            "1/1 - 0s - loss: 0.6876 - acc: 0.9792 - val_loss: 0.6775 - val_acc: 0.9867 - 291ms/epoch - 291ms/step\n",
            "Epoch 66/200\n",
            "1/1 - 0s - loss: 0.6935 - acc: 0.9755 - val_loss: 0.6603 - val_acc: 0.9867 - 298ms/epoch - 298ms/step\n",
            "Epoch 67/200\n",
            "1/1 - 0s - loss: 0.6734 - acc: 0.9741 - val_loss: 0.6433 - val_acc: 0.9867 - 241ms/epoch - 241ms/step\n",
            "Epoch 68/200\n",
            "1/1 - 0s - loss: 0.6418 - acc: 0.9820 - val_loss: 0.6264 - val_acc: 0.9867 - 305ms/epoch - 305ms/step\n",
            "Epoch 69/200\n",
            "1/1 - 0s - loss: 0.6389 - acc: 0.9789 - val_loss: 0.6097 - val_acc: 0.9867 - 288ms/epoch - 288ms/step\n",
            "Epoch 70/200\n",
            "1/1 - 0s - loss: 0.6190 - acc: 0.9752 - val_loss: 0.5932 - val_acc: 0.9867 - 249ms/epoch - 249ms/step\n",
            "Epoch 71/200\n",
            "1/1 - 0s - loss: 0.5983 - acc: 0.9778 - val_loss: 0.5769 - val_acc: 0.9867 - 234ms/epoch - 234ms/step\n",
            "Epoch 72/200\n",
            "1/1 - 0s - loss: 0.5694 - acc: 0.9761 - val_loss: 0.5609 - val_acc: 0.9867 - 301ms/epoch - 301ms/step\n",
            "Epoch 73/200\n",
            "1/1 - 1s - loss: 0.5588 - acc: 0.9775 - val_loss: 0.5452 - val_acc: 0.9867 - 526ms/epoch - 526ms/step\n",
            "Epoch 74/200\n",
            "1/1 - 1s - loss: 0.5482 - acc: 0.9800 - val_loss: 0.5298 - val_acc: 0.9867 - 583ms/epoch - 583ms/step\n",
            "Epoch 75/200\n",
            "1/1 - 0s - loss: 0.5317 - acc: 0.9761 - val_loss: 0.5147 - val_acc: 0.9867 - 438ms/epoch - 438ms/step\n",
            "Epoch 76/200\n",
            "1/1 - 0s - loss: 0.5223 - acc: 0.9789 - val_loss: 0.4999 - val_acc: 0.9867 - 456ms/epoch - 456ms/step\n",
            "Epoch 77/200\n",
            "1/1 - 0s - loss: 0.5015 - acc: 0.9792 - val_loss: 0.4854 - val_acc: 0.9867 - 408ms/epoch - 408ms/step\n",
            "Epoch 78/200\n",
            "1/1 - 1s - loss: 0.4925 - acc: 0.9772 - val_loss: 0.4711 - val_acc: 0.9867 - 591ms/epoch - 591ms/step\n",
            "Epoch 79/200\n",
            "1/1 - 0s - loss: 0.4734 - acc: 0.9780 - val_loss: 0.4572 - val_acc: 0.9867 - 406ms/epoch - 406ms/step\n",
            "Epoch 80/200\n",
            "1/1 - 0s - loss: 0.4694 - acc: 0.9721 - val_loss: 0.4436 - val_acc: 0.9867 - 377ms/epoch - 377ms/step\n",
            "Epoch 81/200\n",
            "1/1 - 0s - loss: 0.4336 - acc: 0.9789 - val_loss: 0.4305 - val_acc: 0.9867 - 376ms/epoch - 376ms/step\n",
            "Epoch 82/200\n",
            "1/1 - 0s - loss: 0.4312 - acc: 0.9800 - val_loss: 0.4177 - val_acc: 0.9867 - 380ms/epoch - 380ms/step\n",
            "Epoch 83/200\n",
            "1/1 - 0s - loss: 0.4242 - acc: 0.9789 - val_loss: 0.4052 - val_acc: 0.9834 - 378ms/epoch - 378ms/step\n",
            "Epoch 84/200\n",
            "1/1 - 0s - loss: 0.4060 - acc: 0.9786 - val_loss: 0.3931 - val_acc: 0.9834 - 365ms/epoch - 365ms/step\n",
            "Epoch 85/200\n",
            "1/1 - 0s - loss: 0.3940 - acc: 0.9797 - val_loss: 0.3814 - val_acc: 0.9834 - 364ms/epoch - 364ms/step\n",
            "Epoch 86/200\n",
            "1/1 - 0s - loss: 0.3853 - acc: 0.9783 - val_loss: 0.3701 - val_acc: 0.9834 - 353ms/epoch - 353ms/step\n",
            "Epoch 87/200\n",
            "1/1 - 0s - loss: 0.3800 - acc: 0.9772 - val_loss: 0.3591 - val_acc: 0.9834 - 247ms/epoch - 247ms/step\n",
            "Epoch 88/200\n",
            "1/1 - 0s - loss: 0.3547 - acc: 0.9797 - val_loss: 0.3485 - val_acc: 0.9801 - 307ms/epoch - 307ms/step\n",
            "Epoch 89/200\n",
            "1/1 - 0s - loss: 0.3583 - acc: 0.9795 - val_loss: 0.3381 - val_acc: 0.9801 - 299ms/epoch - 299ms/step\n",
            "Epoch 90/200\n",
            "1/1 - 0s - loss: 0.3457 - acc: 0.9797 - val_loss: 0.3278 - val_acc: 0.9801 - 296ms/epoch - 296ms/step\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 1.1067 - acc: 0.9862\n",
            "\n",
            "Test Set Metrics:\n",
            "\tloss: 1.1067\n",
            "\tacc: 0.9862\n",
            "1/1 [==============================] - 0s 227ms/step\n",
            "Using GCN (local pooling) filters...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1/1 - 2s - loss: 1.7993 - acc: 0.0090 - val_loss: 1.7767 - val_acc: 0.0066 - 2s/epoch - 2s/step\n",
            "Epoch 2/200\n",
            "1/1 - 0s - loss: 1.7774 - acc: 0.0160 - val_loss: 1.7572 - val_acc: 0.0066 - 233ms/epoch - 233ms/step\n",
            "Epoch 3/200\n",
            "1/1 - 0s - loss: 1.7437 - acc: 0.0132 - val_loss: 1.7378 - val_acc: 0.0066 - 296ms/epoch - 296ms/step\n",
            "Epoch 4/200\n",
            "1/1 - 0s - loss: 1.7443 - acc: 0.0245 - val_loss: 1.7184 - val_acc: 0.0066 - 487ms/epoch - 487ms/step\n",
            "Epoch 5/200\n",
            "1/1 - 0s - loss: 1.6982 - acc: 0.0276 - val_loss: 1.6992 - val_acc: 0.0066 - 447ms/epoch - 447ms/step\n",
            "Epoch 6/200\n",
            "1/1 - 0s - loss: 1.6947 - acc: 0.0332 - val_loss: 1.6801 - val_acc: 0.0066 - 417ms/epoch - 417ms/step\n",
            "Epoch 7/200\n",
            "1/1 - 1s - loss: 1.6819 - acc: 0.0228 - val_loss: 1.6612 - val_acc: 0.0066 - 593ms/epoch - 593ms/step\n",
            "Epoch 8/200\n",
            "1/1 - 1s - loss: 1.6548 - acc: 0.0450 - val_loss: 1.6423 - val_acc: 0.0066 - 581ms/epoch - 581ms/step\n",
            "Epoch 9/200\n",
            "1/1 - 1s - loss: 1.6483 - acc: 0.0419 - val_loss: 1.6235 - val_acc: 0.0066 - 577ms/epoch - 577ms/step\n",
            "Epoch 10/200\n",
            "1/1 - 0s - loss: 1.6273 - acc: 0.0656 - val_loss: 1.6048 - val_acc: 0.0432 - 369ms/epoch - 369ms/step\n",
            "Epoch 11/200\n",
            "1/1 - 0s - loss: 1.6107 - acc: 0.1618 - val_loss: 1.5861 - val_acc: 0.1163 - 371ms/epoch - 371ms/step\n",
            "Epoch 12/200\n",
            "1/1 - 0s - loss: 1.5893 - acc: 0.1478 - val_loss: 1.5674 - val_acc: 0.3289 - 364ms/epoch - 364ms/step\n",
            "Epoch 13/200\n",
            "1/1 - 0s - loss: 1.5675 - acc: 0.2781 - val_loss: 1.5487 - val_acc: 0.5282 - 357ms/epoch - 357ms/step\n",
            "Epoch 14/200\n",
            "1/1 - 0s - loss: 1.5427 - acc: 0.5531 - val_loss: 1.5301 - val_acc: 0.7608 - 387ms/epoch - 387ms/step\n",
            "Epoch 15/200\n",
            "1/1 - 0s - loss: 1.5224 - acc: 0.6822 - val_loss: 1.5113 - val_acc: 0.7674 - 374ms/epoch - 374ms/step\n",
            "Epoch 16/200\n",
            "1/1 - 0s - loss: 1.5132 - acc: 0.7180 - val_loss: 1.4925 - val_acc: 0.7674 - 238ms/epoch - 238ms/step\n",
            "Epoch 17/200\n",
            "1/1 - 0s - loss: 1.4903 - acc: 0.7242 - val_loss: 1.4738 - val_acc: 0.7674 - 236ms/epoch - 236ms/step\n",
            "Epoch 18/200\n",
            "1/1 - 0s - loss: 1.4646 - acc: 0.7512 - val_loss: 1.4551 - val_acc: 0.7674 - 300ms/epoch - 300ms/step\n",
            "Epoch 19/200\n",
            "1/1 - 0s - loss: 1.4496 - acc: 0.8323 - val_loss: 1.4364 - val_acc: 0.7741 - 295ms/epoch - 295ms/step\n",
            "Epoch 20/200\n",
            "1/1 - 0s - loss: 1.4291 - acc: 0.7594 - val_loss: 1.4178 - val_acc: 0.8306 - 240ms/epoch - 240ms/step\n",
            "Epoch 21/200\n",
            "1/1 - 0s - loss: 1.4171 - acc: 0.8306 - val_loss: 1.3991 - val_acc: 0.8472 - 296ms/epoch - 296ms/step\n",
            "Epoch 22/200\n",
            "1/1 - 0s - loss: 1.3870 - acc: 0.8500 - val_loss: 1.3804 - val_acc: 0.8738 - 238ms/epoch - 238ms/step\n",
            "Epoch 23/200\n",
            "1/1 - 0s - loss: 1.3755 - acc: 0.8404 - val_loss: 1.3618 - val_acc: 0.8738 - 237ms/epoch - 237ms/step\n",
            "Epoch 24/200\n",
            "1/1 - 0s - loss: 1.3574 - acc: 0.8446 - val_loss: 1.3431 - val_acc: 0.8738 - 306ms/epoch - 306ms/step\n",
            "Epoch 25/200\n",
            "1/1 - 0s - loss: 1.3419 - acc: 0.8607 - val_loss: 1.3245 - val_acc: 0.8738 - 239ms/epoch - 239ms/step\n",
            "Epoch 26/200\n",
            "1/1 - 0s - loss: 1.3176 - acc: 0.8683 - val_loss: 1.3059 - val_acc: 0.8738 - 297ms/epoch - 297ms/step\n",
            "Epoch 27/200\n",
            "1/1 - 0s - loss: 1.3020 - acc: 0.8686 - val_loss: 1.2875 - val_acc: 0.8738 - 232ms/epoch - 232ms/step\n",
            "Epoch 28/200\n",
            "1/1 - 0s - loss: 1.2925 - acc: 0.8688 - val_loss: 1.2691 - val_acc: 0.8738 - 296ms/epoch - 296ms/step\n",
            "Epoch 29/200\n",
            "1/1 - 0s - loss: 1.2722 - acc: 0.8694 - val_loss: 1.2507 - val_acc: 0.8738 - 295ms/epoch - 295ms/step\n",
            "Epoch 30/200\n",
            "1/1 - 0s - loss: 1.2579 - acc: 0.8745 - val_loss: 1.2324 - val_acc: 0.8738 - 297ms/epoch - 297ms/step\n",
            "Epoch 31/200\n",
            "1/1 - 0s - loss: 1.2473 - acc: 0.8725 - val_loss: 1.2142 - val_acc: 0.8738 - 245ms/epoch - 245ms/step\n",
            "Epoch 32/200\n",
            "1/1 - 0s - loss: 1.2166 - acc: 0.8714 - val_loss: 1.1962 - val_acc: 0.8738 - 301ms/epoch - 301ms/step\n",
            "Epoch 33/200\n",
            "1/1 - 0s - loss: 1.2003 - acc: 0.8725 - val_loss: 1.1782 - val_acc: 0.8738 - 233ms/epoch - 233ms/step\n",
            "Epoch 34/200\n",
            "1/1 - 0s - loss: 1.1734 - acc: 0.8731 - val_loss: 1.1603 - val_acc: 0.8738 - 234ms/epoch - 234ms/step\n",
            "Epoch 35/200\n",
            "1/1 - 0s - loss: 1.1648 - acc: 0.8736 - val_loss: 1.1425 - val_acc: 0.8738 - 298ms/epoch - 298ms/step\n",
            "Epoch 36/200\n",
            "1/1 - 0s - loss: 1.1396 - acc: 0.8753 - val_loss: 1.1251 - val_acc: 0.8738 - 231ms/epoch - 231ms/step\n",
            "Epoch 37/200\n",
            "1/1 - 0s - loss: 1.1322 - acc: 0.8722 - val_loss: 1.1075 - val_acc: 0.8738 - 291ms/epoch - 291ms/step\n",
            "Epoch 38/200\n",
            "1/1 - 0s - loss: 1.0932 - acc: 0.8731 - val_loss: 1.0900 - val_acc: 0.8738 - 304ms/epoch - 304ms/step\n",
            "Epoch 39/200\n",
            "1/1 - 0s - loss: 1.0948 - acc: 0.8719 - val_loss: 1.0724 - val_acc: 0.8738 - 229ms/epoch - 229ms/step\n",
            "Epoch 40/200\n",
            "1/1 - 0s - loss: 1.0734 - acc: 0.8733 - val_loss: 1.0550 - val_acc: 0.8738 - 228ms/epoch - 228ms/step\n",
            "Epoch 41/200\n",
            "1/1 - 0s - loss: 1.0547 - acc: 0.8739 - val_loss: 1.0375 - val_acc: 0.8738 - 294ms/epoch - 294ms/step\n",
            "Epoch 42/200\n",
            "1/1 - 0s - loss: 1.0508 - acc: 0.8719 - val_loss: 1.0201 - val_acc: 0.8738 - 228ms/epoch - 228ms/step\n",
            "Epoch 43/200\n",
            "1/1 - 0s - loss: 1.0329 - acc: 0.8728 - val_loss: 1.0032 - val_acc: 0.8738 - 333ms/epoch - 333ms/step\n",
            "Epoch 44/200\n",
            "1/1 - 0s - loss: 1.0057 - acc: 0.8722 - val_loss: 0.9866 - val_acc: 0.8738 - 410ms/epoch - 410ms/step\n",
            "Epoch 45/200\n",
            "1/1 - 1s - loss: 1.0004 - acc: 0.8722 - val_loss: 0.9705 - val_acc: 0.8738 - 577ms/epoch - 577ms/step\n",
            "Epoch 46/200\n",
            "1/1 - 1s - loss: 0.9755 - acc: 0.8725 - val_loss: 0.9547 - val_acc: 0.8738 - 558ms/epoch - 558ms/step\n",
            "Epoch 47/200\n",
            "1/1 - 1s - loss: 0.9595 - acc: 0.8725 - val_loss: 0.9389 - val_acc: 0.8738 - 576ms/epoch - 576ms/step\n",
            "Epoch 48/200\n",
            "1/1 - 0s - loss: 0.9441 - acc: 0.8725 - val_loss: 0.9233 - val_acc: 0.8738 - 447ms/epoch - 447ms/step\n",
            "Epoch 49/200\n",
            "1/1 - 0s - loss: 0.9315 - acc: 0.8725 - val_loss: 0.9081 - val_acc: 0.8738 - 407ms/epoch - 407ms/step\n",
            "Epoch 50/200\n",
            "1/1 - 0s - loss: 0.9132 - acc: 0.8725 - val_loss: 0.8931 - val_acc: 0.8738 - 386ms/epoch - 386ms/step\n",
            "Epoch 51/200\n",
            "1/1 - 0s - loss: 0.9021 - acc: 0.8722 - val_loss: 0.8781 - val_acc: 0.8738 - 384ms/epoch - 384ms/step\n",
            "Epoch 52/200\n",
            "1/1 - 0s - loss: 0.9008 - acc: 0.8725 - val_loss: 0.8633 - val_acc: 0.8738 - 366ms/epoch - 366ms/step\n",
            "Epoch 53/200\n",
            "1/1 - 0s - loss: 0.8684 - acc: 0.8722 - val_loss: 0.8486 - val_acc: 0.8738 - 377ms/epoch - 377ms/step\n",
            "Epoch 54/200\n",
            "1/1 - 0s - loss: 0.8617 - acc: 0.8728 - val_loss: 0.8341 - val_acc: 0.8738 - 349ms/epoch - 349ms/step\n",
            "Epoch 55/200\n",
            "1/1 - 0s - loss: 0.8411 - acc: 0.8731 - val_loss: 0.8199 - val_acc: 0.8738 - 374ms/epoch - 374ms/step\n",
            "Epoch 56/200\n",
            "1/1 - 0s - loss: 0.8231 - acc: 0.8731 - val_loss: 0.8060 - val_acc: 0.8738 - 233ms/epoch - 233ms/step\n",
            "Epoch 57/200\n",
            "1/1 - 0s - loss: 0.8097 - acc: 0.8728 - val_loss: 0.7923 - val_acc: 0.8738 - 301ms/epoch - 301ms/step\n",
            "Epoch 58/200\n",
            "1/1 - 0s - loss: 0.8031 - acc: 0.8722 - val_loss: 0.7786 - val_acc: 0.8738 - 294ms/epoch - 294ms/step\n",
            "Epoch 59/200\n",
            "1/1 - 0s - loss: 0.7890 - acc: 0.8731 - val_loss: 0.7650 - val_acc: 0.8738 - 239ms/epoch - 239ms/step\n",
            "Epoch 60/200\n",
            "1/1 - 0s - loss: 0.7675 - acc: 0.8728 - val_loss: 0.7516 - val_acc: 0.8738 - 233ms/epoch - 233ms/step\n",
            "Epoch 61/200\n",
            "1/1 - 0s - loss: 0.7618 - acc: 0.8725 - val_loss: 0.7382 - val_acc: 0.8738 - 305ms/epoch - 305ms/step\n",
            "Epoch 62/200\n",
            "1/1 - 0s - loss: 0.7396 - acc: 0.8728 - val_loss: 0.7250 - val_acc: 0.8738 - 306ms/epoch - 306ms/step\n",
            "Epoch 63/200\n",
            "1/1 - 0s - loss: 0.7338 - acc: 0.8725 - val_loss: 0.7118 - val_acc: 0.8738 - 235ms/epoch - 235ms/step\n",
            "Epoch 64/200\n",
            "1/1 - 0s - loss: 0.7204 - acc: 0.8725 - val_loss: 0.6987 - val_acc: 0.8738 - 241ms/epoch - 241ms/step\n",
            "Epoch 65/200\n",
            "1/1 - 0s - loss: 0.7008 - acc: 0.8725 - val_loss: 0.6858 - val_acc: 0.8738 - 234ms/epoch - 234ms/step\n",
            "Epoch 66/200\n",
            "1/1 - 0s - loss: 0.6894 - acc: 0.8731 - val_loss: 0.6730 - val_acc: 0.8738 - 245ms/epoch - 245ms/step\n",
            "Epoch 67/200\n",
            "1/1 - 0s - loss: 0.6772 - acc: 0.8725 - val_loss: 0.6603 - val_acc: 0.8738 - 231ms/epoch - 231ms/step\n",
            "Epoch 68/200\n",
            "1/1 - 0s - loss: 0.6620 - acc: 0.8725 - val_loss: 0.6477 - val_acc: 0.8738 - 232ms/epoch - 232ms/step\n",
            "Epoch 69/200\n",
            "1/1 - 0s - loss: 0.6544 - acc: 0.8725 - val_loss: 0.6352 - val_acc: 0.8738 - 293ms/epoch - 293ms/step\n",
            "Epoch 70/200\n",
            "1/1 - 0s - loss: 0.6442 - acc: 0.8722 - val_loss: 0.6229 - val_acc: 0.8738 - 226ms/epoch - 226ms/step\n",
            "Epoch 71/200\n",
            "1/1 - 0s - loss: 0.6215 - acc: 0.8725 - val_loss: 0.6107 - val_acc: 0.8738 - 296ms/epoch - 296ms/step\n",
            "Epoch 72/200\n",
            "1/1 - 0s - loss: 0.6126 - acc: 0.8725 - val_loss: 0.5987 - val_acc: 0.8738 - 239ms/epoch - 239ms/step\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 1.3767 - acc: 0.8729\n",
            "\n",
            "Test Set Metrics:\n",
            "\tloss: 1.3767\n",
            "\tacc: 0.8729\n",
            "1/1 [==============================] - 0s 225ms/step\n",
            "Using GCN (local pooling) filters...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1/1 - 3s - loss: 1.8032 - acc: 0.0115 - val_loss: 1.7853 - val_acc: 0.0166 - 3s/epoch - 3s/step\n",
            "Epoch 2/200\n",
            "1/1 - 0s - loss: 1.7933 - acc: 0.0146 - val_loss: 1.7728 - val_acc: 0.0166 - 371ms/epoch - 371ms/step\n",
            "Epoch 3/200\n",
            "1/1 - 0s - loss: 1.7726 - acc: 0.0132 - val_loss: 1.7604 - val_acc: 0.0166 - 385ms/epoch - 385ms/step\n",
            "Epoch 4/200\n",
            "1/1 - 0s - loss: 1.7668 - acc: 0.0124 - val_loss: 1.7482 - val_acc: 0.0166 - 369ms/epoch - 369ms/step\n",
            "Epoch 5/200\n",
            "1/1 - 0s - loss: 1.7619 - acc: 0.0115 - val_loss: 1.7365 - val_acc: 0.0166 - 366ms/epoch - 366ms/step\n",
            "Epoch 6/200\n",
            "1/1 - 0s - loss: 1.7411 - acc: 0.0110 - val_loss: 1.7251 - val_acc: 0.0365 - 386ms/epoch - 386ms/step\n",
            "Epoch 7/200\n",
            "1/1 - 0s - loss: 1.7251 - acc: 0.0172 - val_loss: 1.7140 - val_acc: 0.0465 - 375ms/epoch - 375ms/step\n",
            "Epoch 8/200\n",
            "1/1 - 0s - loss: 1.7222 - acc: 0.0228 - val_loss: 1.7031 - val_acc: 0.1096 - 298ms/epoch - 298ms/step\n",
            "Epoch 9/200\n",
            "1/1 - 0s - loss: 1.7076 - acc: 0.0149 - val_loss: 1.6924 - val_acc: 0.1163 - 294ms/epoch - 294ms/step\n",
            "Epoch 10/200\n",
            "1/1 - 0s - loss: 1.6883 - acc: 0.0903 - val_loss: 1.6816 - val_acc: 0.1163 - 310ms/epoch - 310ms/step\n",
            "Epoch 11/200\n",
            "1/1 - 0s - loss: 1.6922 - acc: 0.0175 - val_loss: 1.6709 - val_acc: 0.1163 - 234ms/epoch - 234ms/step\n",
            "Epoch 12/200\n",
            "1/1 - 0s - loss: 1.6789 - acc: 0.0906 - val_loss: 1.6599 - val_acc: 0.1229 - 235ms/epoch - 235ms/step\n",
            "Epoch 13/200\n",
            "1/1 - 0s - loss: 1.6594 - acc: 0.0977 - val_loss: 1.6489 - val_acc: 0.1262 - 256ms/epoch - 256ms/step\n",
            "Epoch 14/200\n",
            "1/1 - 0s - loss: 1.6554 - acc: 0.1171 - val_loss: 1.6379 - val_acc: 0.1262 - 300ms/epoch - 300ms/step\n",
            "Epoch 15/200\n",
            "1/1 - 0s - loss: 1.6426 - acc: 0.1244 - val_loss: 1.6270 - val_acc: 0.1329 - 249ms/epoch - 249ms/step\n",
            "Epoch 16/200\n",
            "1/1 - 0s - loss: 1.6304 - acc: 0.1939 - val_loss: 1.6166 - val_acc: 0.3654 - 252ms/epoch - 252ms/step\n",
            "Epoch 17/200\n",
            "1/1 - 0s - loss: 1.6179 - acc: 0.3034 - val_loss: 1.6061 - val_acc: 0.3654 - 300ms/epoch - 300ms/step\n",
            "Epoch 18/200\n",
            "1/1 - 0s - loss: 1.6118 - acc: 0.3847 - val_loss: 1.5956 - val_acc: 0.7409 - 240ms/epoch - 240ms/step\n",
            "Epoch 19/200\n",
            "1/1 - 0s - loss: 1.6001 - acc: 0.6043 - val_loss: 1.5851 - val_acc: 0.8073 - 232ms/epoch - 232ms/step\n",
            "Epoch 20/200\n",
            "1/1 - 0s - loss: 1.5877 - acc: 0.6172 - val_loss: 1.5745 - val_acc: 0.8073 - 252ms/epoch - 252ms/step\n",
            "Epoch 21/200\n",
            "1/1 - 0s - loss: 1.5802 - acc: 0.6699 - val_loss: 1.5646 - val_acc: 0.8073 - 294ms/epoch - 294ms/step\n",
            "Epoch 22/200\n",
            "1/1 - 0s - loss: 1.5709 - acc: 0.7726 - val_loss: 1.5549 - val_acc: 0.8605 - 236ms/epoch - 236ms/step\n",
            "Epoch 23/200\n",
            "1/1 - 0s - loss: 1.5692 - acc: 0.7855 - val_loss: 1.5449 - val_acc: 0.8605 - 292ms/epoch - 292ms/step\n",
            "Epoch 24/200\n",
            "1/1 - 0s - loss: 1.5527 - acc: 0.7374 - val_loss: 1.5345 - val_acc: 0.8605 - 245ms/epoch - 245ms/step\n",
            "Epoch 25/200\n",
            "1/1 - 0s - loss: 1.5394 - acc: 0.7613 - val_loss: 1.5240 - val_acc: 0.8605 - 240ms/epoch - 240ms/step\n",
            "Epoch 26/200\n",
            "1/1 - 0s - loss: 1.5414 - acc: 0.7841 - val_loss: 1.5133 - val_acc: 0.8738 - 316ms/epoch - 316ms/step\n",
            "Epoch 27/200\n",
            "1/1 - 0s - loss: 1.5377 - acc: 0.7957 - val_loss: 1.5022 - val_acc: 0.8738 - 302ms/epoch - 302ms/step\n",
            "Epoch 28/200\n",
            "1/1 - 0s - loss: 1.5062 - acc: 0.8033 - val_loss: 1.4909 - val_acc: 0.8738 - 239ms/epoch - 239ms/step\n",
            "Epoch 29/200\n",
            "1/1 - 0s - loss: 1.5090 - acc: 0.8044 - val_loss: 1.4796 - val_acc: 0.8738 - 247ms/epoch - 247ms/step\n",
            "Epoch 30/200\n",
            "1/1 - 0s - loss: 1.4951 - acc: 0.8289 - val_loss: 1.4681 - val_acc: 0.8738 - 290ms/epoch - 290ms/step\n",
            "Epoch 31/200\n",
            "1/1 - 0s - loss: 1.4811 - acc: 0.8413 - val_loss: 1.4565 - val_acc: 0.8738 - 235ms/epoch - 235ms/step\n",
            "Epoch 32/200\n",
            "1/1 - 0s - loss: 1.4663 - acc: 0.8483 - val_loss: 1.4447 - val_acc: 0.8738 - 303ms/epoch - 303ms/step\n",
            "Epoch 33/200\n",
            "1/1 - 0s - loss: 1.4564 - acc: 0.8306 - val_loss: 1.4327 - val_acc: 0.8738 - 233ms/epoch - 233ms/step\n",
            "Epoch 34/200\n",
            "1/1 - 0s - loss: 1.4472 - acc: 0.8339 - val_loss: 1.4207 - val_acc: 0.8738 - 233ms/epoch - 233ms/step\n",
            "Epoch 35/200\n",
            "1/1 - 0s - loss: 1.4308 - acc: 0.8686 - val_loss: 1.4085 - val_acc: 0.8738 - 310ms/epoch - 310ms/step\n",
            "Epoch 36/200\n",
            "1/1 - 0s - loss: 1.4263 - acc: 0.8525 - val_loss: 1.3965 - val_acc: 0.8704 - 242ms/epoch - 242ms/step\n",
            "Epoch 37/200\n",
            "1/1 - 0s - loss: 1.4080 - acc: 0.8731 - val_loss: 1.3846 - val_acc: 0.8837 - 294ms/epoch - 294ms/step\n",
            "Epoch 38/200\n",
            "1/1 - 0s - loss: 1.3971 - acc: 0.8455 - val_loss: 1.3727 - val_acc: 0.8837 - 420ms/epoch - 420ms/step\n",
            "Epoch 39/200\n",
            "1/1 - 1s - loss: 1.3885 - acc: 0.8531 - val_loss: 1.3608 - val_acc: 0.8837 - 583ms/epoch - 583ms/step\n",
            "Epoch 40/200\n",
            "1/1 - 1s - loss: 1.3733 - acc: 0.8522 - val_loss: 1.3488 - val_acc: 0.8837 - 583ms/epoch - 583ms/step\n",
            "Epoch 41/200\n",
            "1/1 - 0s - loss: 1.3666 - acc: 0.8480 - val_loss: 1.3367 - val_acc: 0.8837 - 468ms/epoch - 468ms/step\n",
            "Epoch 42/200\n",
            "1/1 - 1s - loss: 1.3482 - acc: 0.8542 - val_loss: 1.3248 - val_acc: 0.8804 - 587ms/epoch - 587ms/step\n",
            "Epoch 43/200\n",
            "1/1 - 0s - loss: 1.3322 - acc: 0.8652 - val_loss: 1.3132 - val_acc: 0.8804 - 403ms/epoch - 403ms/step\n",
            "Epoch 44/200\n",
            "1/1 - 0s - loss: 1.3265 - acc: 0.8759 - val_loss: 1.3015 - val_acc: 0.8804 - 394ms/epoch - 394ms/step\n",
            "Epoch 45/200\n",
            "1/1 - 0s - loss: 1.3116 - acc: 0.8728 - val_loss: 1.2897 - val_acc: 0.8804 - 371ms/epoch - 371ms/step\n",
            "Epoch 46/200\n",
            "1/1 - 0s - loss: 1.3040 - acc: 0.8767 - val_loss: 1.2775 - val_acc: 0.8804 - 375ms/epoch - 375ms/step\n",
            "Epoch 47/200\n",
            "1/1 - 0s - loss: 1.2877 - acc: 0.8781 - val_loss: 1.2651 - val_acc: 0.8804 - 376ms/epoch - 376ms/step\n",
            "Epoch 48/200\n",
            "1/1 - 0s - loss: 1.2751 - acc: 0.8759 - val_loss: 1.2525 - val_acc: 0.8804 - 379ms/epoch - 379ms/step\n",
            "Epoch 49/200\n",
            "1/1 - 0s - loss: 1.2689 - acc: 0.8517 - val_loss: 1.2396 - val_acc: 0.8804 - 379ms/epoch - 379ms/step\n",
            "Epoch 50/200\n",
            "1/1 - 0s - loss: 1.2450 - acc: 0.8787 - val_loss: 1.2266 - val_acc: 0.8804 - 378ms/epoch - 378ms/step\n",
            "Epoch 51/200\n",
            "1/1 - 0s - loss: 1.2428 - acc: 0.8764 - val_loss: 1.2134 - val_acc: 0.8804 - 294ms/epoch - 294ms/step\n",
            "Epoch 52/200\n",
            "1/1 - 0s - loss: 1.2261 - acc: 0.8781 - val_loss: 1.2001 - val_acc: 0.8804 - 242ms/epoch - 242ms/step\n",
            "Epoch 53/200\n",
            "1/1 - 0s - loss: 1.2115 - acc: 0.8795 - val_loss: 1.1866 - val_acc: 0.8804 - 243ms/epoch - 243ms/step\n",
            "Epoch 54/200\n",
            "1/1 - 0s - loss: 1.2011 - acc: 0.8787 - val_loss: 1.1730 - val_acc: 0.8804 - 243ms/epoch - 243ms/step\n",
            "Epoch 55/200\n",
            "1/1 - 0s - loss: 1.1789 - acc: 0.8812 - val_loss: 1.1593 - val_acc: 0.8804 - 238ms/epoch - 238ms/step\n",
            "Epoch 56/200\n",
            "1/1 - 0s - loss: 1.1707 - acc: 0.8795 - val_loss: 1.1455 - val_acc: 0.8804 - 292ms/epoch - 292ms/step\n",
            "Epoch 57/200\n",
            "1/1 - 0s - loss: 1.1554 - acc: 0.8767 - val_loss: 1.1315 - val_acc: 0.8804 - 238ms/epoch - 238ms/step\n",
            "Epoch 58/200\n",
            "1/1 - 0s - loss: 1.1409 - acc: 0.8776 - val_loss: 1.1171 - val_acc: 0.8804 - 232ms/epoch - 232ms/step\n",
            "Epoch 59/200\n",
            "1/1 - 0s - loss: 1.1331 - acc: 0.8784 - val_loss: 1.1026 - val_acc: 0.8804 - 244ms/epoch - 244ms/step\n",
            "Epoch 60/200\n",
            "1/1 - 0s - loss: 1.1296 - acc: 0.8736 - val_loss: 1.0880 - val_acc: 0.8804 - 241ms/epoch - 241ms/step\n",
            "Epoch 61/200\n",
            "1/1 - 0s - loss: 1.0966 - acc: 0.8770 - val_loss: 1.0733 - val_acc: 0.8804 - 230ms/epoch - 230ms/step\n",
            "Epoch 62/200\n",
            "1/1 - 0s - loss: 1.0785 - acc: 0.8784 - val_loss: 1.0584 - val_acc: 0.8804 - 251ms/epoch - 251ms/step\n",
            "Epoch 63/200\n",
            "1/1 - 0s - loss: 1.0741 - acc: 0.8801 - val_loss: 1.0435 - val_acc: 0.8804 - 298ms/epoch - 298ms/step\n",
            "Epoch 64/200\n",
            "1/1 - 0s - loss: 1.0408 - acc: 0.8778 - val_loss: 1.0285 - val_acc: 0.8804 - 301ms/epoch - 301ms/step\n",
            "Epoch 65/200\n",
            "1/1 - 0s - loss: 1.0338 - acc: 0.8753 - val_loss: 1.0134 - val_acc: 0.8804 - 275ms/epoch - 275ms/step\n",
            "Epoch 66/200\n",
            "1/1 - 0s - loss: 1.0235 - acc: 0.8778 - val_loss: 0.9978 - val_acc: 0.8804 - 252ms/epoch - 252ms/step\n",
            "Epoch 67/200\n",
            "1/1 - 0s - loss: 1.0060 - acc: 0.8790 - val_loss: 0.9818 - val_acc: 0.8804 - 238ms/epoch - 238ms/step\n",
            "Epoch 68/200\n",
            "1/1 - 0s - loss: 0.9898 - acc: 0.8801 - val_loss: 0.9656 - val_acc: 0.8870 - 308ms/epoch - 308ms/step\n",
            "Epoch 69/200\n",
            "1/1 - 0s - loss: 0.9726 - acc: 0.8891 - val_loss: 0.9493 - val_acc: 0.8870 - 296ms/epoch - 296ms/step\n",
            "Epoch 70/200\n",
            "1/1 - 0s - loss: 0.9605 - acc: 0.8922 - val_loss: 0.9327 - val_acc: 0.8904 - 242ms/epoch - 242ms/step\n",
            "Epoch 71/200\n",
            "1/1 - 0s - loss: 0.9428 - acc: 0.8781 - val_loss: 0.9160 - val_acc: 0.8904 - 290ms/epoch - 290ms/step\n",
            "Epoch 72/200\n",
            "1/1 - 0s - loss: 0.9328 - acc: 0.8891 - val_loss: 0.8993 - val_acc: 0.8904 - 232ms/epoch - 232ms/step\n",
            "Epoch 73/200\n",
            "1/1 - 0s - loss: 0.9037 - acc: 0.8871 - val_loss: 0.8827 - val_acc: 0.8904 - 244ms/epoch - 244ms/step\n",
            "Epoch 74/200\n",
            "1/1 - 0s - loss: 0.8893 - acc: 0.8880 - val_loss: 0.8662 - val_acc: 0.8904 - 243ms/epoch - 243ms/step\n",
            "Epoch 75/200\n",
            "1/1 - 0s - loss: 0.8758 - acc: 0.8928 - val_loss: 0.8497 - val_acc: 0.8904 - 237ms/epoch - 237ms/step\n",
            "Epoch 76/200\n",
            "1/1 - 0s - loss: 0.8689 - acc: 0.8897 - val_loss: 0.8333 - val_acc: 0.8904 - 290ms/epoch - 290ms/step\n",
            "Epoch 77/200\n",
            "1/1 - 0s - loss: 0.8500 - acc: 0.9021 - val_loss: 0.8168 - val_acc: 0.8904 - 232ms/epoch - 232ms/step\n",
            "Epoch 78/200\n",
            "1/1 - 0s - loss: 0.8254 - acc: 0.8880 - val_loss: 0.8003 - val_acc: 0.8904 - 293ms/epoch - 293ms/step\n",
            "Epoch 79/200\n",
            "1/1 - 0s - loss: 0.8100 - acc: 0.8885 - val_loss: 0.7835 - val_acc: 0.8937 - 295ms/epoch - 295ms/step\n",
            "Epoch 80/200\n",
            "1/1 - 0s - loss: 0.7933 - acc: 0.8978 - val_loss: 0.7666 - val_acc: 0.8937 - 419ms/epoch - 419ms/step\n",
            "Epoch 81/200\n",
            "1/1 - 1s - loss: 0.7861 - acc: 0.9493 - val_loss: 0.7498 - val_acc: 0.9867 - 578ms/epoch - 578ms/step\n",
            "Epoch 82/200\n",
            "1/1 - 1s - loss: 0.7584 - acc: 0.8905 - val_loss: 0.7332 - val_acc: 0.9867 - 571ms/epoch - 571ms/step\n",
            "Epoch 83/200\n",
            "1/1 - 0s - loss: 0.7441 - acc: 0.9609 - val_loss: 0.7166 - val_acc: 0.9867 - 424ms/epoch - 424ms/step\n",
            "Epoch 84/200\n",
            "1/1 - 1s - loss: 0.7373 - acc: 0.8902 - val_loss: 0.7002 - val_acc: 0.9867 - 571ms/epoch - 571ms/step\n",
            "Epoch 85/200\n",
            "1/1 - 0s - loss: 0.7031 - acc: 0.9612 - val_loss: 0.6839 - val_acc: 0.9867 - 420ms/epoch - 420ms/step\n",
            "Epoch 86/200\n",
            "1/1 - 0s - loss: 0.6999 - acc: 0.9752 - val_loss: 0.6678 - val_acc: 0.9867 - 385ms/epoch - 385ms/step\n",
            "Epoch 87/200\n",
            "1/1 - 0s - loss: 0.6831 - acc: 0.9716 - val_loss: 0.6518 - val_acc: 0.9867 - 358ms/epoch - 358ms/step\n",
            "Epoch 88/200\n",
            "1/1 - 0s - loss: 0.6562 - acc: 0.9719 - val_loss: 0.6361 - val_acc: 0.9867 - 380ms/epoch - 380ms/step\n",
            "Epoch 89/200\n",
            "1/1 - 0s - loss: 0.6506 - acc: 0.9789 - val_loss: 0.6206 - val_acc: 0.9867 - 355ms/epoch - 355ms/step\n",
            "Epoch 90/200\n",
            "1/1 - 0s - loss: 0.6361 - acc: 0.9659 - val_loss: 0.6054 - val_acc: 0.9867 - 389ms/epoch - 389ms/step\n",
            "Epoch 91/200\n",
            "1/1 - 0s - loss: 0.6097 - acc: 0.9800 - val_loss: 0.5904 - val_acc: 0.9867 - 365ms/epoch - 365ms/step\n",
            "Epoch 92/200\n",
            "1/1 - 0s - loss: 0.6080 - acc: 0.9175 - val_loss: 0.5757 - val_acc: 0.9867 - 239ms/epoch - 239ms/step\n",
            "Epoch 93/200\n",
            "1/1 - 0s - loss: 0.5882 - acc: 0.9606 - val_loss: 0.5611 - val_acc: 0.9867 - 236ms/epoch - 236ms/step\n",
            "Epoch 94/200\n",
            "1/1 - 0s - loss: 0.5719 - acc: 0.9817 - val_loss: 0.5469 - val_acc: 0.9867 - 306ms/epoch - 306ms/step\n",
            "Epoch 95/200\n",
            "1/1 - 0s - loss: 0.5599 - acc: 0.9631 - val_loss: 0.5329 - val_acc: 0.9867 - 229ms/epoch - 229ms/step\n",
            "Epoch 96/200\n",
            "1/1 - 0s - loss: 0.5409 - acc: 0.9668 - val_loss: 0.5192 - val_acc: 0.9900 - 236ms/epoch - 236ms/step\n",
            "Epoch 97/200\n",
            "1/1 - 0s - loss: 0.5368 - acc: 0.9783 - val_loss: 0.5058 - val_acc: 0.9900 - 248ms/epoch - 248ms/step\n",
            "Epoch 98/200\n",
            "1/1 - 0s - loss: 0.5180 - acc: 0.9688 - val_loss: 0.4925 - val_acc: 0.9900 - 295ms/epoch - 295ms/step\n",
            "Epoch 99/200\n",
            "1/1 - 0s - loss: 0.4991 - acc: 0.9778 - val_loss: 0.4796 - val_acc: 0.9900 - 232ms/epoch - 232ms/step\n",
            "Epoch 100/200\n",
            "1/1 - 0s - loss: 0.4954 - acc: 0.9800 - val_loss: 0.4670 - val_acc: 0.9900 - 300ms/epoch - 300ms/step\n",
            "Epoch 101/200\n",
            "1/1 - 0s - loss: 0.4821 - acc: 0.9735 - val_loss: 0.4547 - val_acc: 0.9900 - 226ms/epoch - 226ms/step\n",
            "Epoch 102/200\n",
            "1/1 - 0s - loss: 0.4699 - acc: 0.9792 - val_loss: 0.4426 - val_acc: 0.9900 - 232ms/epoch - 232ms/step\n",
            "Epoch 103/200\n",
            "1/1 - 0s - loss: 0.4583 - acc: 0.9800 - val_loss: 0.4309 - val_acc: 0.9900 - 243ms/epoch - 243ms/step\n",
            "Epoch 104/200\n",
            "1/1 - 0s - loss: 0.4425 - acc: 0.9848 - val_loss: 0.4195 - val_acc: 0.9900 - 293ms/epoch - 293ms/step\n",
            "Epoch 105/200\n",
            "1/1 - 0s - loss: 0.4374 - acc: 0.9783 - val_loss: 0.4084 - val_acc: 0.9900 - 236ms/epoch - 236ms/step\n",
            "Epoch 106/200\n",
            "1/1 - 0s - loss: 0.4208 - acc: 0.9769 - val_loss: 0.3976 - val_acc: 0.9900 - 250ms/epoch - 250ms/step\n",
            "Epoch 107/200\n",
            "1/1 - 0s - loss: 0.4079 - acc: 0.9786 - val_loss: 0.3872 - val_acc: 0.9900 - 290ms/epoch - 290ms/step\n",
            "Epoch 108/200\n",
            "1/1 - 0s - loss: 0.4069 - acc: 0.9741 - val_loss: 0.3770 - val_acc: 0.9900 - 231ms/epoch - 231ms/step\n",
            "Epoch 109/200\n",
            "1/1 - 0s - loss: 0.4014 - acc: 0.9631 - val_loss: 0.3671 - val_acc: 0.9900 - 237ms/epoch - 237ms/step\n",
            "Epoch 110/200\n",
            "1/1 - 0s - loss: 0.3796 - acc: 0.9721 - val_loss: 0.3576 - val_acc: 0.9900 - 294ms/epoch - 294ms/step\n",
            "Epoch 111/200\n",
            "1/1 - 0s - loss: 0.3697 - acc: 0.9780 - val_loss: 0.3483 - val_acc: 0.9900 - 228ms/epoch - 228ms/step\n",
            "Epoch 112/200\n",
            "1/1 - 0s - loss: 0.3584 - acc: 0.9783 - val_loss: 0.3392 - val_acc: 0.9900 - 244ms/epoch - 244ms/step\n",
            "Epoch 113/200\n",
            "1/1 - 0s - loss: 0.3591 - acc: 0.9600 - val_loss: 0.3305 - val_acc: 0.9900 - 232ms/epoch - 232ms/step\n",
            "Epoch 114/200\n",
            "1/1 - 0s - loss: 0.3533 - acc: 0.9699 - val_loss: 0.3220 - val_acc: 0.9900 - 228ms/epoch - 228ms/step\n",
            "Epoch 115/200\n",
            "1/1 - 0s - loss: 0.3399 - acc: 0.9733 - val_loss: 0.3138 - val_acc: 0.9900 - 231ms/epoch - 231ms/step\n",
            "Epoch 116/200\n",
            "1/1 - 0s - loss: 0.3216 - acc: 0.9800 - val_loss: 0.3059 - val_acc: 0.9900 - 297ms/epoch - 297ms/step\n",
            "Epoch 117/200\n",
            "1/1 - 0s - loss: 0.3222 - acc: 0.9783 - val_loss: 0.2982 - val_acc: 0.9900 - 291ms/epoch - 291ms/step\n",
            "Epoch 118/200\n",
            "1/1 - 0s - loss: 0.3145 - acc: 0.9719 - val_loss: 0.2908 - val_acc: 0.9900 - 229ms/epoch - 229ms/step\n",
            "Epoch 119/200\n",
            "1/1 - 0s - loss: 0.3060 - acc: 0.9797 - val_loss: 0.2836 - val_acc: 0.9900 - 296ms/epoch - 296ms/step\n",
            "Epoch 120/200\n",
            "1/1 - 0s - loss: 0.2877 - acc: 0.9817 - val_loss: 0.2767 - val_acc: 0.9900 - 228ms/epoch - 228ms/step\n",
            "Epoch 121/200\n",
            "1/1 - 0s - loss: 0.2911 - acc: 0.9792 - val_loss: 0.2700 - val_acc: 0.9900 - 235ms/epoch - 235ms/step\n",
            "Epoch 122/200\n",
            "1/1 - 0s - loss: 0.2776 - acc: 0.9750 - val_loss: 0.2635 - val_acc: 0.9900 - 226ms/epoch - 226ms/step\n",
            "Epoch 123/200\n",
            "1/1 - 0s - loss: 0.2788 - acc: 0.9817 - val_loss: 0.2573 - val_acc: 0.9900 - 240ms/epoch - 240ms/step\n",
            "Epoch 124/200\n",
            "1/1 - 0s - loss: 0.2762 - acc: 0.9828 - val_loss: 0.2512 - val_acc: 0.9900 - 291ms/epoch - 291ms/step\n",
            "Epoch 125/200\n",
            "1/1 - 1s - loss: 0.2638 - acc: 0.9803 - val_loss: 0.2453 - val_acc: 0.9900 - 541ms/epoch - 541ms/step\n",
            "Epoch 126/200\n",
            "1/1 - 1s - loss: 0.2648 - acc: 0.9803 - val_loss: 0.2396 - val_acc: 0.9900 - 574ms/epoch - 574ms/step\n",
            "Epoch 127/200\n",
            "1/1 - 1s - loss: 0.2673 - acc: 0.9811 - val_loss: 0.2341 - val_acc: 0.9900 - 585ms/epoch - 585ms/step\n",
            "Epoch 128/200\n",
            "1/1 - 1s - loss: 0.2419 - acc: 0.9783 - val_loss: 0.2288 - val_acc: 0.9900 - 580ms/epoch - 580ms/step\n",
            "Epoch 129/200\n",
            "1/1 - 1s - loss: 0.2413 - acc: 0.9811 - val_loss: 0.2237 - val_acc: 0.9900 - 583ms/epoch - 583ms/step\n",
            "Epoch 130/200\n",
            "1/1 - 0s - loss: 0.2332 - acc: 0.9792 - val_loss: 0.2187 - val_acc: 0.9900 - 379ms/epoch - 379ms/step\n",
            "Epoch 131/200\n",
            "1/1 - 0s - loss: 0.2321 - acc: 0.9828 - val_loss: 0.2138 - val_acc: 0.9900 - 377ms/epoch - 377ms/step\n",
            "Epoch 132/200\n",
            "1/1 - 0s - loss: 0.2271 - acc: 0.9809 - val_loss: 0.2091 - val_acc: 0.9900 - 376ms/epoch - 376ms/step\n",
            "Epoch 133/200\n",
            "1/1 - 0s - loss: 0.2273 - acc: 0.9800 - val_loss: 0.2046 - val_acc: 0.9900 - 392ms/epoch - 392ms/step\n",
            "Epoch 134/200\n",
            "1/1 - 0s - loss: 0.2214 - acc: 0.9792 - val_loss: 0.2001 - val_acc: 0.9900 - 368ms/epoch - 368ms/step\n",
            "Epoch 135/200\n",
            "1/1 - 0s - loss: 0.2205 - acc: 0.9789 - val_loss: 0.1958 - val_acc: 0.9900 - 384ms/epoch - 384ms/step\n",
            "Epoch 136/200\n",
            "1/1 - 0s - loss: 0.2233 - acc: 0.9786 - val_loss: 0.1916 - val_acc: 0.9900 - 371ms/epoch - 371ms/step\n",
            "Epoch 137/200\n",
            "1/1 - 0s - loss: 0.2023 - acc: 0.9814 - val_loss: 0.1875 - val_acc: 0.9900 - 226ms/epoch - 226ms/step\n",
            "Epoch 138/200\n",
            "1/1 - 0s - loss: 0.2061 - acc: 0.9811 - val_loss: 0.1835 - val_acc: 0.9900 - 226ms/epoch - 226ms/step\n",
            "Epoch 139/200\n",
            "1/1 - 0s - loss: 0.2006 - acc: 0.9783 - val_loss: 0.1796 - val_acc: 0.9900 - 232ms/epoch - 232ms/step\n",
            "Epoch 140/200\n",
            "1/1 - 0s - loss: 0.1988 - acc: 0.9803 - val_loss: 0.1759 - val_acc: 0.9900 - 297ms/epoch - 297ms/step\n",
            "Epoch 141/200\n",
            "1/1 - 0s - loss: 0.1970 - acc: 0.9786 - val_loss: 0.1722 - val_acc: 0.9834 - 290ms/epoch - 290ms/step\n",
            "Epoch 142/200\n",
            "1/1 - 0s - loss: 0.1913 - acc: 0.9800 - val_loss: 0.1686 - val_acc: 0.9834 - 289ms/epoch - 289ms/step\n",
            "Epoch 143/200\n",
            "1/1 - 0s - loss: 0.1885 - acc: 0.9783 - val_loss: 0.1651 - val_acc: 0.9801 - 239ms/epoch - 239ms/step\n",
            "Epoch 144/200\n",
            "1/1 - 0s - loss: 0.1873 - acc: 0.9786 - val_loss: 0.1617 - val_acc: 0.9801 - 227ms/epoch - 227ms/step\n",
            "Epoch 145/200\n",
            "1/1 - 0s - loss: 0.1821 - acc: 0.9809 - val_loss: 0.1584 - val_acc: 0.9801 - 234ms/epoch - 234ms/step\n",
            "Epoch 146/200\n",
            "1/1 - 0s - loss: 0.1780 - acc: 0.9806 - val_loss: 0.1552 - val_acc: 0.9801 - 229ms/epoch - 229ms/step\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.5242 - acc: 0.9862\n",
            "\n",
            "Test Set Metrics:\n",
            "\tloss: 0.5242\n",
            "\tacc: 0.9862\n",
            "1/1 [==============================] - 0s 225ms/step\n",
            "Using GCN (local pooling) filters...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1/1 - 2s - loss: 1.7781 - acc: 0.1019 - val_loss: 1.7491 - val_acc: 0.1196 - 2s/epoch - 2s/step\n",
            "Epoch 2/200\n",
            "1/1 - 0s - loss: 1.7344 - acc: 0.1084 - val_loss: 1.7154 - val_acc: 0.1196 - 423ms/epoch - 423ms/step\n",
            "Epoch 3/200\n",
            "1/1 - 1s - loss: 1.7122 - acc: 0.1120 - val_loss: 1.6807 - val_acc: 0.1196 - 580ms/epoch - 580ms/step\n",
            "Epoch 4/200\n",
            "1/1 - 1s - loss: 1.6807 - acc: 0.1081 - val_loss: 1.6459 - val_acc: 0.1196 - 591ms/epoch - 591ms/step\n",
            "Epoch 5/200\n",
            "1/1 - 1s - loss: 1.6400 - acc: 0.1084 - val_loss: 1.6104 - val_acc: 0.1196 - 575ms/epoch - 575ms/step\n",
            "Epoch 6/200\n",
            "1/1 - 0s - loss: 1.6180 - acc: 0.1005 - val_loss: 1.5747 - val_acc: 0.1196 - 392ms/epoch - 392ms/step\n",
            "Epoch 7/200\n",
            "1/1 - 0s - loss: 1.5871 - acc: 0.1100 - val_loss: 1.5394 - val_acc: 0.1196 - 356ms/epoch - 356ms/step\n",
            "Epoch 8/200\n",
            "1/1 - 0s - loss: 1.5343 - acc: 0.1100 - val_loss: 1.5043 - val_acc: 0.1196 - 383ms/epoch - 383ms/step\n",
            "Epoch 9/200\n",
            "1/1 - 0s - loss: 1.5071 - acc: 0.1030 - val_loss: 1.4695 - val_acc: 0.1196 - 374ms/epoch - 374ms/step\n",
            "Epoch 10/200\n",
            "1/1 - 0s - loss: 1.4767 - acc: 0.1058 - val_loss: 1.4348 - val_acc: 0.1196 - 378ms/epoch - 378ms/step\n",
            "Epoch 11/200\n",
            "1/1 - 0s - loss: 1.4501 - acc: 0.0988 - val_loss: 1.4004 - val_acc: 0.1163 - 339ms/epoch - 339ms/step\n",
            "Epoch 12/200\n",
            "1/1 - 0s - loss: 1.4117 - acc: 0.0963 - val_loss: 1.3662 - val_acc: 0.1163 - 310ms/epoch - 310ms/step\n",
            "Epoch 13/200\n",
            "1/1 - 0s - loss: 1.3749 - acc: 0.1106 - val_loss: 1.3323 - val_acc: 0.1163 - 258ms/epoch - 258ms/step\n",
            "Epoch 14/200\n",
            "1/1 - 0s - loss: 1.3454 - acc: 0.1010 - val_loss: 1.2988 - val_acc: 0.1163 - 244ms/epoch - 244ms/step\n",
            "Epoch 15/200\n",
            "1/1 - 0s - loss: 1.3063 - acc: 0.1120 - val_loss: 1.2657 - val_acc: 0.1063 - 232ms/epoch - 232ms/step\n",
            "Epoch 16/200\n",
            "1/1 - 0s - loss: 1.2769 - acc: 0.1005 - val_loss: 1.2331 - val_acc: 0.1063 - 293ms/epoch - 293ms/step\n",
            "Epoch 17/200\n",
            "1/1 - 0s - loss: 1.2396 - acc: 0.1117 - val_loss: 1.2008 - val_acc: 0.1030 - 241ms/epoch - 241ms/step\n",
            "Epoch 18/200\n",
            "1/1 - 0s - loss: 1.2080 - acc: 0.1224 - val_loss: 1.1688 - val_acc: 0.1030 - 246ms/epoch - 246ms/step\n",
            "Epoch 19/200\n",
            "1/1 - 0s - loss: 1.1721 - acc: 0.1894 - val_loss: 1.1372 - val_acc: 0.3023 - 300ms/epoch - 300ms/step\n",
            "Epoch 20/200\n",
            "1/1 - 0s - loss: 1.1473 - acc: 0.2260 - val_loss: 1.1061 - val_acc: 0.6113 - 249ms/epoch - 249ms/step\n",
            "Epoch 21/200\n",
            "1/1 - 0s - loss: 1.1147 - acc: 0.5865 - val_loss: 1.0753 - val_acc: 0.8007 - 249ms/epoch - 249ms/step\n",
            "Epoch 22/200\n",
            "1/1 - 0s - loss: 1.0840 - acc: 0.8444 - val_loss: 1.0448 - val_acc: 0.8870 - 294ms/epoch - 294ms/step\n",
            "Epoch 23/200\n",
            "1/1 - 0s - loss: 1.0427 - acc: 0.8545 - val_loss: 1.0148 - val_acc: 0.8904 - 234ms/epoch - 234ms/step\n",
            "Epoch 24/200\n",
            "1/1 - 0s - loss: 1.0260 - acc: 0.9249 - val_loss: 0.9852 - val_acc: 0.9535 - 244ms/epoch - 244ms/step\n",
            "Epoch 25/200\n",
            "1/1 - 0s - loss: 0.9957 - acc: 0.9462 - val_loss: 0.9559 - val_acc: 0.9734 - 252ms/epoch - 252ms/step\n",
            "Epoch 26/200\n",
            "1/1 - 0s - loss: 0.9656 - acc: 0.9443 - val_loss: 0.9271 - val_acc: 0.9734 - 251ms/epoch - 251ms/step\n",
            "Epoch 27/200\n",
            "1/1 - 0s - loss: 0.9290 - acc: 0.9583 - val_loss: 0.8989 - val_acc: 0.9734 - 240ms/epoch - 240ms/step\n",
            "Epoch 28/200\n",
            "1/1 - 0s - loss: 0.9078 - acc: 0.9510 - val_loss: 0.8712 - val_acc: 0.9767 - 257ms/epoch - 257ms/step\n",
            "Epoch 29/200\n",
            "1/1 - 0s - loss: 0.8863 - acc: 0.9674 - val_loss: 0.8441 - val_acc: 0.9767 - 251ms/epoch - 251ms/step\n",
            "Epoch 30/200\n",
            "1/1 - 0s - loss: 0.8595 - acc: 0.9606 - val_loss: 0.8175 - val_acc: 0.9767 - 294ms/epoch - 294ms/step\n",
            "Epoch 31/200\n",
            "1/1 - 0s - loss: 0.8263 - acc: 0.9735 - val_loss: 0.7915 - val_acc: 0.9767 - 292ms/epoch - 292ms/step\n",
            "Epoch 32/200\n",
            "1/1 - 0s - loss: 0.8017 - acc: 0.9603 - val_loss: 0.7661 - val_acc: 0.9767 - 246ms/epoch - 246ms/step\n",
            "Epoch 33/200\n",
            "1/1 - 0s - loss: 0.7600 - acc: 0.9620 - val_loss: 0.7413 - val_acc: 0.9767 - 231ms/epoch - 231ms/step\n",
            "Epoch 34/200\n",
            "1/1 - 0s - loss: 0.7610 - acc: 0.9623 - val_loss: 0.7170 - val_acc: 0.9767 - 239ms/epoch - 239ms/step\n",
            "Epoch 35/200\n",
            "1/1 - 0s - loss: 0.7237 - acc: 0.9603 - val_loss: 0.6935 - val_acc: 0.9767 - 243ms/epoch - 243ms/step\n",
            "Epoch 36/200\n",
            "1/1 - 0s - loss: 0.7117 - acc: 0.9724 - val_loss: 0.6706 - val_acc: 0.9767 - 290ms/epoch - 290ms/step\n",
            "Epoch 37/200\n",
            "1/1 - 0s - loss: 0.6803 - acc: 0.9721 - val_loss: 0.6483 - val_acc: 0.9767 - 289ms/epoch - 289ms/step\n",
            "Epoch 38/200\n",
            "1/1 - 0s - loss: 0.6633 - acc: 0.9716 - val_loss: 0.6266 - val_acc: 0.9767 - 292ms/epoch - 292ms/step\n",
            "Epoch 39/200\n",
            "1/1 - 0s - loss: 0.6386 - acc: 0.9747 - val_loss: 0.6055 - val_acc: 0.9767 - 290ms/epoch - 290ms/step\n",
            "Epoch 40/200\n",
            "1/1 - 1s - loss: 0.6120 - acc: 0.9634 - val_loss: 0.5851 - val_acc: 0.9767 - 514ms/epoch - 514ms/step\n",
            "Epoch 41/200\n",
            "1/1 - 1s - loss: 0.5947 - acc: 0.9741 - val_loss: 0.5653 - val_acc: 0.9767 - 584ms/epoch - 584ms/step\n",
            "Epoch 42/200\n",
            "1/1 - 0s - loss: 0.5810 - acc: 0.9634 - val_loss: 0.5461 - val_acc: 0.9767 - 442ms/epoch - 442ms/step\n",
            "Epoch 43/200\n",
            "1/1 - 1s - loss: 0.5744 - acc: 0.9634 - val_loss: 0.5276 - val_acc: 0.9767 - 559ms/epoch - 559ms/step\n",
            "Epoch 44/200\n",
            "1/1 - 0s - loss: 0.5340 - acc: 0.9735 - val_loss: 0.5097 - val_acc: 0.9767 - 452ms/epoch - 452ms/step\n",
            "Epoch 45/200\n",
            "1/1 - 1s - loss: 0.5145 - acc: 0.9637 - val_loss: 0.4924 - val_acc: 0.9767 - 579ms/epoch - 579ms/step\n",
            "Epoch 46/200\n",
            "1/1 - 0s - loss: 0.4928 - acc: 0.9721 - val_loss: 0.4757 - val_acc: 0.9767 - 430ms/epoch - 430ms/step\n",
            "Epoch 47/200\n",
            "1/1 - 0s - loss: 0.4894 - acc: 0.9750 - val_loss: 0.4596 - val_acc: 0.9767 - 373ms/epoch - 373ms/step\n",
            "Epoch 48/200\n",
            "1/1 - 0s - loss: 0.4753 - acc: 0.9741 - val_loss: 0.4441 - val_acc: 0.9767 - 384ms/epoch - 384ms/step\n",
            "Epoch 49/200\n",
            "1/1 - 0s - loss: 0.4519 - acc: 0.9617 - val_loss: 0.4292 - val_acc: 0.9767 - 384ms/epoch - 384ms/step\n",
            "Epoch 50/200\n",
            "1/1 - 0s - loss: 0.4304 - acc: 0.9772 - val_loss: 0.4148 - val_acc: 0.9801 - 379ms/epoch - 379ms/step\n",
            "Epoch 51/200\n",
            "1/1 - 0s - loss: 0.4229 - acc: 0.9738 - val_loss: 0.4009 - val_acc: 0.9801 - 369ms/epoch - 369ms/step\n",
            "Epoch 52/200\n",
            "1/1 - 0s - loss: 0.4067 - acc: 0.9764 - val_loss: 0.3875 - val_acc: 0.9801 - 388ms/epoch - 388ms/step\n",
            "Epoch 53/200\n",
            "1/1 - 0s - loss: 0.4025 - acc: 0.9752 - val_loss: 0.3746 - val_acc: 0.9801 - 300ms/epoch - 300ms/step\n",
            "Epoch 54/200\n",
            "1/1 - 0s - loss: 0.3889 - acc: 0.9752 - val_loss: 0.3622 - val_acc: 0.9801 - 243ms/epoch - 243ms/step\n",
            "Epoch 55/200\n",
            "1/1 - 0s - loss: 0.3742 - acc: 0.9735 - val_loss: 0.3503 - val_acc: 0.9801 - 255ms/epoch - 255ms/step\n",
            "Epoch 56/200\n",
            "1/1 - 0s - loss: 0.3626 - acc: 0.9735 - val_loss: 0.3389 - val_acc: 0.9801 - 241ms/epoch - 241ms/step\n",
            "Epoch 57/200\n",
            "1/1 - 0s - loss: 0.3460 - acc: 0.9750 - val_loss: 0.3279 - val_acc: 0.9801 - 247ms/epoch - 247ms/step\n",
            "Epoch 58/200\n",
            "1/1 - 0s - loss: 0.3462 - acc: 0.9744 - val_loss: 0.3172 - val_acc: 0.9801 - 248ms/epoch - 248ms/step\n",
            "Epoch 59/200\n",
            "1/1 - 0s - loss: 0.3237 - acc: 0.9758 - val_loss: 0.3069 - val_acc: 0.9801 - 236ms/epoch - 236ms/step\n",
            "Epoch 60/200\n",
            "1/1 - 0s - loss: 0.3123 - acc: 0.9780 - val_loss: 0.2971 - val_acc: 0.9801 - 302ms/epoch - 302ms/step\n",
            "Epoch 61/200\n",
            "1/1 - 0s - loss: 0.3077 - acc: 0.9750 - val_loss: 0.2876 - val_acc: 0.9801 - 256ms/epoch - 256ms/step\n",
            "Epoch 62/200\n",
            "1/1 - 0s - loss: 0.3027 - acc: 0.9752 - val_loss: 0.2785 - val_acc: 0.9801 - 240ms/epoch - 240ms/step\n",
            "Epoch 63/200\n",
            "1/1 - 0s - loss: 0.2854 - acc: 0.9758 - val_loss: 0.2696 - val_acc: 0.9801 - 237ms/epoch - 237ms/step\n",
            "Epoch 64/200\n",
            "1/1 - 0s - loss: 0.2807 - acc: 0.9750 - val_loss: 0.2611 - val_acc: 0.9801 - 239ms/epoch - 239ms/step\n",
            "Epoch 65/200\n",
            "1/1 - 0s - loss: 0.2767 - acc: 0.9766 - val_loss: 0.2529 - val_acc: 0.9801 - 238ms/epoch - 238ms/step\n",
            "Epoch 66/200\n",
            "1/1 - 0s - loss: 0.2577 - acc: 0.9775 - val_loss: 0.2449 - val_acc: 0.9801 - 292ms/epoch - 292ms/step\n",
            "Epoch 67/200\n",
            "1/1 - 0s - loss: 0.2597 - acc: 0.9657 - val_loss: 0.2372 - val_acc: 0.9801 - 298ms/epoch - 298ms/step\n",
            "Epoch 68/200\n",
            "1/1 - 0s - loss: 0.2424 - acc: 0.9761 - val_loss: 0.2298 - val_acc: 0.9801 - 301ms/epoch - 301ms/step\n",
            "Epoch 69/200\n",
            "1/1 - 0s - loss: 0.2335 - acc: 0.9792 - val_loss: 0.2228 - val_acc: 0.9801 - 292ms/epoch - 292ms/step\n",
            "Epoch 70/200\n",
            "1/1 - 0s - loss: 0.2350 - acc: 0.9783 - val_loss: 0.2162 - val_acc: 0.9801 - 247ms/epoch - 247ms/step\n",
            "Epoch 71/200\n",
            "1/1 - 0s - loss: 0.2287 - acc: 0.9797 - val_loss: 0.2099 - val_acc: 0.9801 - 294ms/epoch - 294ms/step\n",
            "Epoch 72/200\n",
            "1/1 - 0s - loss: 0.2220 - acc: 0.9772 - val_loss: 0.2038 - val_acc: 0.9801 - 241ms/epoch - 241ms/step\n",
            "Epoch 73/200\n",
            "1/1 - 0s - loss: 0.2106 - acc: 0.9766 - val_loss: 0.1981 - val_acc: 0.9801 - 239ms/epoch - 239ms/step\n",
            "Epoch 74/200\n",
            "1/1 - 0s - loss: 0.2079 - acc: 0.9764 - val_loss: 0.1927 - val_acc: 0.9801 - 244ms/epoch - 244ms/step\n",
            "Epoch 75/200\n",
            "1/1 - 0s - loss: 0.2107 - acc: 0.9674 - val_loss: 0.1875 - val_acc: 0.9801 - 297ms/epoch - 297ms/step\n",
            "Epoch 76/200\n",
            "1/1 - 0s - loss: 0.1998 - acc: 0.9783 - val_loss: 0.1826 - val_acc: 0.9801 - 292ms/epoch - 292ms/step\n",
            "Epoch 77/200\n",
            "1/1 - 0s - loss: 0.1927 - acc: 0.9789 - val_loss: 0.1779 - val_acc: 0.9801 - 242ms/epoch - 242ms/step\n",
            "Epoch 78/200\n",
            "1/1 - 0s - loss: 0.1959 - acc: 0.9761 - val_loss: 0.1734 - val_acc: 0.9801 - 236ms/epoch - 236ms/step\n",
            "Epoch 79/200\n",
            "1/1 - 0s - loss: 0.1963 - acc: 0.9789 - val_loss: 0.1691 - val_acc: 0.9801 - 250ms/epoch - 250ms/step\n",
            "Epoch 80/200\n",
            "1/1 - 0s - loss: 0.1906 - acc: 0.9752 - val_loss: 0.1650 - val_acc: 0.9801 - 293ms/epoch - 293ms/step\n",
            "Epoch 81/200\n",
            "1/1 - 0s - loss: 0.1805 - acc: 0.9778 - val_loss: 0.1611 - val_acc: 0.9801 - 435ms/epoch - 435ms/step\n",
            "Epoch 82/200\n",
            "1/1 - 1s - loss: 0.1760 - acc: 0.9797 - val_loss: 0.1574 - val_acc: 0.9801 - 586ms/epoch - 586ms/step\n",
            "Epoch 83/200\n",
            "1/1 - 1s - loss: 0.1697 - acc: 0.9772 - val_loss: 0.1538 - val_acc: 0.9801 - 575ms/epoch - 575ms/step\n",
            "Epoch 84/200\n",
            "1/1 - 1s - loss: 0.1646 - acc: 0.9772 - val_loss: 0.1505 - val_acc: 0.9801 - 554ms/epoch - 554ms/step\n",
            "Epoch 85/200\n",
            "1/1 - 0s - loss: 0.1622 - acc: 0.9783 - val_loss: 0.1473 - val_acc: 0.9801 - 455ms/epoch - 455ms/step\n",
            "Epoch 86/200\n",
            "1/1 - 0s - loss: 0.1556 - acc: 0.9789 - val_loss: 0.1442 - val_acc: 0.9801 - 422ms/epoch - 422ms/step\n",
            "Epoch 87/200\n",
            "1/1 - 0s - loss: 0.1606 - acc: 0.9775 - val_loss: 0.1413 - val_acc: 0.9801 - 371ms/epoch - 371ms/step\n",
            "Epoch 88/200\n",
            "1/1 - 0s - loss: 0.1504 - acc: 0.9789 - val_loss: 0.1385 - val_acc: 0.9801 - 369ms/epoch - 369ms/step\n",
            "Epoch 89/200\n",
            "1/1 - 0s - loss: 0.1466 - acc: 0.9780 - val_loss: 0.1359 - val_acc: 0.9801 - 375ms/epoch - 375ms/step\n",
            "Epoch 90/200\n",
            "1/1 - 0s - loss: 0.1468 - acc: 0.9766 - val_loss: 0.1334 - val_acc: 0.9801 - 376ms/epoch - 376ms/step\n",
            "Epoch 91/200\n",
            "1/1 - 0s - loss: 0.1487 - acc: 0.9778 - val_loss: 0.1310 - val_acc: 0.9801 - 379ms/epoch - 379ms/step\n",
            "Epoch 92/200\n",
            "1/1 - 0s - loss: 0.1418 - acc: 0.9786 - val_loss: 0.1287 - val_acc: 0.9801 - 312ms/epoch - 312ms/step\n",
            "Epoch 93/200\n",
            "1/1 - 0s - loss: 0.1397 - acc: 0.9786 - val_loss: 0.1265 - val_acc: 0.9801 - 239ms/epoch - 239ms/step\n",
            "Epoch 94/200\n",
            "1/1 - 0s - loss: 0.1481 - acc: 0.9786 - val_loss: 0.1244 - val_acc: 0.9801 - 304ms/epoch - 304ms/step\n",
            "Epoch 95/200\n",
            "1/1 - 0s - loss: 0.1384 - acc: 0.9789 - val_loss: 0.1224 - val_acc: 0.9801 - 232ms/epoch - 232ms/step\n",
            "Epoch 96/200\n",
            "1/1 - 0s - loss: 0.1302 - acc: 0.9789 - val_loss: 0.1205 - val_acc: 0.9801 - 302ms/epoch - 302ms/step\n",
            "Epoch 97/200\n",
            "1/1 - 0s - loss: 0.1309 - acc: 0.9783 - val_loss: 0.1186 - val_acc: 0.9801 - 292ms/epoch - 292ms/step\n",
            "Epoch 98/200\n",
            "1/1 - 0s - loss: 0.1309 - acc: 0.9783 - val_loss: 0.1169 - val_acc: 0.9801 - 298ms/epoch - 298ms/step\n",
            "Epoch 99/200\n",
            "1/1 - 0s - loss: 0.1379 - acc: 0.9786 - val_loss: 0.1152 - val_acc: 0.9801 - 235ms/epoch - 235ms/step\n",
            "Epoch 100/200\n",
            "1/1 - 0s - loss: 0.1313 - acc: 0.9789 - val_loss: 0.1136 - val_acc: 0.9801 - 300ms/epoch - 300ms/step\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.4192 - acc: 0.9834\n",
            "\n",
            "Test Set Metrics:\n",
            "\tloss: 0.4192\n",
            "\tacc: 0.9834\n",
            "1/1 [==============================] - 0s 235ms/step\n",
            "Using GCN (local pooling) filters...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1/1 - 2s - loss: 1.6869 - acc: 0.0070 - val_loss: 1.6719 - val_acc: 0.0066 - 2s/epoch - 2s/step\n",
            "Epoch 2/200\n",
            "1/1 - 0s - loss: 1.6691 - acc: 0.0068 - val_loss: 1.6609 - val_acc: 0.0066 - 252ms/epoch - 252ms/step\n",
            "Epoch 3/200\n",
            "1/1 - 1s - loss: 1.6564 - acc: 0.0084 - val_loss: 1.6499 - val_acc: 0.0066 - 529ms/epoch - 529ms/step\n",
            "Epoch 4/200\n",
            "1/1 - 0s - loss: 1.6552 - acc: 0.0087 - val_loss: 1.6389 - val_acc: 0.0066 - 450ms/epoch - 450ms/step\n",
            "Epoch 5/200\n",
            "1/1 - 1s - loss: 1.6395 - acc: 0.0093 - val_loss: 1.6279 - val_acc: 0.0066 - 577ms/epoch - 577ms/step\n",
            "Epoch 6/200\n",
            "1/1 - 1s - loss: 1.6238 - acc: 0.0070 - val_loss: 1.6170 - val_acc: 0.0066 - 583ms/epoch - 583ms/step\n",
            "Epoch 7/200\n",
            "1/1 - 0s - loss: 1.6117 - acc: 0.0079 - val_loss: 1.6060 - val_acc: 0.0066 - 426ms/epoch - 426ms/step\n",
            "Epoch 8/200\n",
            "1/1 - 1s - loss: 1.6072 - acc: 0.0068 - val_loss: 1.5951 - val_acc: 0.0066 - 573ms/epoch - 573ms/step\n",
            "Epoch 9/200\n",
            "1/1 - 0s - loss: 1.5952 - acc: 0.0110 - val_loss: 1.5841 - val_acc: 0.0066 - 417ms/epoch - 417ms/step\n",
            "Epoch 10/200\n",
            "1/1 - 0s - loss: 1.5828 - acc: 0.0079 - val_loss: 1.5730 - val_acc: 0.0066 - 383ms/epoch - 383ms/step\n",
            "Epoch 11/200\n",
            "1/1 - 0s - loss: 1.5721 - acc: 0.0076 - val_loss: 1.5619 - val_acc: 0.0066 - 380ms/epoch - 380ms/step\n",
            "Epoch 12/200\n",
            "1/1 - 0s - loss: 1.5659 - acc: 0.0124 - val_loss: 1.5508 - val_acc: 0.0066 - 370ms/epoch - 370ms/step\n",
            "Epoch 13/200\n",
            "1/1 - 0s - loss: 1.5439 - acc: 0.0079 - val_loss: 1.5396 - val_acc: 0.0066 - 359ms/epoch - 359ms/step\n",
            "Epoch 14/200\n",
            "1/1 - 0s - loss: 1.5380 - acc: 0.0121 - val_loss: 1.5283 - val_acc: 0.0066 - 366ms/epoch - 366ms/step\n",
            "Epoch 15/200\n",
            "1/1 - 0s - loss: 1.5194 - acc: 0.0104 - val_loss: 1.5171 - val_acc: 0.0066 - 319ms/epoch - 319ms/step\n",
            "Epoch 16/200\n",
            "1/1 - 0s - loss: 1.5143 - acc: 0.0121 - val_loss: 1.5058 - val_acc: 0.0066 - 243ms/epoch - 243ms/step\n",
            "Epoch 17/200\n",
            "1/1 - 0s - loss: 1.5009 - acc: 0.0152 - val_loss: 1.4946 - val_acc: 0.0066 - 294ms/epoch - 294ms/step\n",
            "Epoch 18/200\n",
            "1/1 - 0s - loss: 1.4952 - acc: 0.0110 - val_loss: 1.4832 - val_acc: 0.0066 - 242ms/epoch - 242ms/step\n",
            "Epoch 19/200\n",
            "1/1 - 0s - loss: 1.4811 - acc: 0.0180 - val_loss: 1.4716 - val_acc: 0.0066 - 244ms/epoch - 244ms/step\n",
            "Epoch 20/200\n",
            "1/1 - 0s - loss: 1.4683 - acc: 0.0180 - val_loss: 1.4601 - val_acc: 0.0066 - 308ms/epoch - 308ms/step\n",
            "Epoch 21/200\n",
            "1/1 - 0s - loss: 1.4547 - acc: 0.0234 - val_loss: 1.4485 - val_acc: 0.0066 - 243ms/epoch - 243ms/step\n",
            "Epoch 22/200\n",
            "1/1 - 0s - loss: 1.4476 - acc: 0.0259 - val_loss: 1.4368 - val_acc: 0.0066 - 244ms/epoch - 244ms/step\n",
            "Epoch 23/200\n",
            "1/1 - 0s - loss: 1.4389 - acc: 0.0250 - val_loss: 1.4250 - val_acc: 0.0066 - 269ms/epoch - 269ms/step\n",
            "Epoch 24/200\n",
            "1/1 - 0s - loss: 1.4112 - acc: 0.0239 - val_loss: 1.4129 - val_acc: 0.0199 - 296ms/epoch - 296ms/step\n",
            "Epoch 25/200\n",
            "1/1 - 0s - loss: 1.4113 - acc: 0.0290 - val_loss: 1.4007 - val_acc: 0.0199 - 244ms/epoch - 244ms/step\n",
            "Epoch 26/200\n",
            "1/1 - 0s - loss: 1.3943 - acc: 0.0391 - val_loss: 1.3882 - val_acc: 0.1196 - 238ms/epoch - 238ms/step\n",
            "Epoch 27/200\n",
            "1/1 - 0s - loss: 1.3851 - acc: 0.1058 - val_loss: 1.3755 - val_acc: 0.1196 - 243ms/epoch - 243ms/step\n",
            "Epoch 28/200\n",
            "1/1 - 0s - loss: 1.3688 - acc: 0.1193 - val_loss: 1.3618 - val_acc: 0.1429 - 252ms/epoch - 252ms/step\n",
            "Epoch 29/200\n",
            "1/1 - 0s - loss: 1.3580 - acc: 0.1210 - val_loss: 1.3472 - val_acc: 0.1429 - 293ms/epoch - 293ms/step\n",
            "Epoch 30/200\n",
            "1/1 - 0s - loss: 1.3380 - acc: 0.1393 - val_loss: 1.3320 - val_acc: 0.1462 - 274ms/epoch - 274ms/step\n",
            "Epoch 31/200\n",
            "1/1 - 0s - loss: 1.3312 - acc: 0.2223 - val_loss: 1.3165 - val_acc: 0.1595 - 297ms/epoch - 297ms/step\n",
            "Epoch 32/200\n",
            "1/1 - 0s - loss: 1.3109 - acc: 0.1599 - val_loss: 1.3008 - val_acc: 0.3056 - 301ms/epoch - 301ms/step\n",
            "Epoch 33/200\n",
            "1/1 - 0s - loss: 1.3010 - acc: 0.1635 - val_loss: 1.2848 - val_acc: 0.4585 - 240ms/epoch - 240ms/step\n",
            "Epoch 34/200\n",
            "1/1 - 0s - loss: 1.2684 - acc: 0.5297 - val_loss: 1.2687 - val_acc: 0.7076 - 245ms/epoch - 245ms/step\n",
            "Epoch 35/200\n",
            "1/1 - 0s - loss: 1.2682 - acc: 0.7267 - val_loss: 1.2520 - val_acc: 0.8738 - 295ms/epoch - 295ms/step\n",
            "Epoch 36/200\n",
            "1/1 - 0s - loss: 1.2430 - acc: 0.8182 - val_loss: 1.2351 - val_acc: 0.8738 - 299ms/epoch - 299ms/step\n",
            "Epoch 37/200\n",
            "1/1 - 0s - loss: 1.2313 - acc: 0.8489 - val_loss: 1.2177 - val_acc: 0.8738 - 296ms/epoch - 296ms/step\n",
            "Epoch 38/200\n",
            "1/1 - 0s - loss: 1.2125 - acc: 0.8663 - val_loss: 1.1995 - val_acc: 0.8738 - 309ms/epoch - 309ms/step\n",
            "Epoch 39/200\n",
            "1/1 - 0s - loss: 1.1925 - acc: 0.8711 - val_loss: 1.1801 - val_acc: 0.8738 - 238ms/epoch - 238ms/step\n",
            "Epoch 40/200\n",
            "1/1 - 0s - loss: 1.1831 - acc: 0.8725 - val_loss: 1.1601 - val_acc: 0.8738 - 290ms/epoch - 290ms/step\n",
            "Epoch 41/200\n",
            "1/1 - 0s - loss: 1.1455 - acc: 0.8714 - val_loss: 1.1385 - val_acc: 0.8738 - 242ms/epoch - 242ms/step\n",
            "Epoch 42/200\n",
            "1/1 - 0s - loss: 1.1310 - acc: 0.8717 - val_loss: 1.1158 - val_acc: 0.8738 - 246ms/epoch - 246ms/step\n",
            "Epoch 43/200\n",
            "1/1 - 0s - loss: 1.1004 - acc: 0.8705 - val_loss: 1.0905 - val_acc: 0.8738 - 263ms/epoch - 263ms/step\n",
            "Epoch 44/200\n",
            "1/1 - 1s - loss: 1.1015 - acc: 0.8725 - val_loss: 1.0631 - val_acc: 0.8738 - 537ms/epoch - 537ms/step\n",
            "Epoch 45/200\n",
            "1/1 - 0s - loss: 1.0531 - acc: 0.8728 - val_loss: 1.0337 - val_acc: 0.8738 - 430ms/epoch - 430ms/step\n",
            "Epoch 46/200\n",
            "1/1 - 0s - loss: 1.0423 - acc: 0.8725 - val_loss: 1.0026 - val_acc: 0.8738 - 425ms/epoch - 425ms/step\n",
            "Epoch 47/200\n",
            "1/1 - 0s - loss: 1.0160 - acc: 0.8722 - val_loss: 0.9714 - val_acc: 0.8738 - 441ms/epoch - 441ms/step\n",
            "Epoch 48/200\n",
            "1/1 - 0s - loss: 0.9635 - acc: 0.8725 - val_loss: 0.9402 - val_acc: 0.8738 - 438ms/epoch - 438ms/step\n",
            "Epoch 49/200\n",
            "1/1 - 0s - loss: 0.9325 - acc: 0.8725 - val_loss: 0.9092 - val_acc: 0.8738 - 432ms/epoch - 432ms/step\n",
            "Epoch 50/200\n",
            "1/1 - 0s - loss: 0.9156 - acc: 0.8725 - val_loss: 0.8786 - val_acc: 0.8738 - 438ms/epoch - 438ms/step\n",
            "Epoch 51/200\n",
            "1/1 - 0s - loss: 0.8731 - acc: 0.8725 - val_loss: 0.8486 - val_acc: 0.8738 - 390ms/epoch - 390ms/step\n",
            "Epoch 52/200\n",
            "1/1 - 0s - loss: 0.8494 - acc: 0.8725 - val_loss: 0.8189 - val_acc: 0.8738 - 372ms/epoch - 372ms/step\n",
            "Epoch 53/200\n",
            "1/1 - 0s - loss: 0.8220 - acc: 0.8725 - val_loss: 0.7899 - val_acc: 0.8738 - 382ms/epoch - 382ms/step\n",
            "Epoch 54/200\n",
            "1/1 - 0s - loss: 0.7927 - acc: 0.8725 - val_loss: 0.7614 - val_acc: 0.8738 - 379ms/epoch - 379ms/step\n",
            "Epoch 55/200\n",
            "1/1 - 0s - loss: 0.7646 - acc: 0.8725 - val_loss: 0.7336 - val_acc: 0.8738 - 363ms/epoch - 363ms/step\n",
            "Epoch 56/200\n",
            "1/1 - 0s - loss: 0.7386 - acc: 0.8725 - val_loss: 0.7065 - val_acc: 0.8738 - 375ms/epoch - 375ms/step\n",
            "Epoch 57/200\n",
            "1/1 - 0s - loss: 0.7122 - acc: 0.8725 - val_loss: 0.6803 - val_acc: 0.8738 - 289ms/epoch - 289ms/step\n",
            "Epoch 58/200\n",
            "1/1 - 0s - loss: 0.6925 - acc: 0.8725 - val_loss: 0.6551 - val_acc: 0.8738 - 299ms/epoch - 299ms/step\n",
            "Epoch 59/200\n",
            "1/1 - 0s - loss: 0.6620 - acc: 0.8725 - val_loss: 0.6308 - val_acc: 0.8738 - 247ms/epoch - 247ms/step\n",
            "Epoch 60/200\n",
            "1/1 - 0s - loss: 0.6412 - acc: 0.8725 - val_loss: 0.6070 - val_acc: 0.8738 - 297ms/epoch - 297ms/step\n",
            "Epoch 61/200\n",
            "1/1 - 0s - loss: 0.5989 - acc: 0.8725 - val_loss: 0.5840 - val_acc: 0.8738 - 300ms/epoch - 300ms/step\n",
            "Epoch 62/200\n",
            "1/1 - 0s - loss: 0.5815 - acc: 0.8725 - val_loss: 0.5620 - val_acc: 0.8738 - 311ms/epoch - 311ms/step\n",
            "Epoch 63/200\n",
            "1/1 - 0s - loss: 0.5561 - acc: 0.8725 - val_loss: 0.5411 - val_acc: 0.8738 - 298ms/epoch - 298ms/step\n",
            "Epoch 64/200\n",
            "1/1 - 0s - loss: 0.5500 - acc: 0.8725 - val_loss: 0.5213 - val_acc: 0.8738 - 242ms/epoch - 242ms/step\n",
            "Epoch 65/200\n",
            "1/1 - 0s - loss: 0.5357 - acc: 0.8725 - val_loss: 0.5025 - val_acc: 0.8738 - 292ms/epoch - 292ms/step\n",
            "Epoch 66/200\n",
            "1/1 - 0s - loss: 0.5222 - acc: 0.8725 - val_loss: 0.4846 - val_acc: 0.8738 - 289ms/epoch - 289ms/step\n",
            "Epoch 67/200\n",
            "1/1 - 0s - loss: 0.4833 - acc: 0.8725 - val_loss: 0.4677 - val_acc: 0.8738 - 293ms/epoch - 293ms/step\n",
            "Epoch 68/200\n",
            "1/1 - 0s - loss: 0.4745 - acc: 0.8725 - val_loss: 0.4516 - val_acc: 0.8738 - 238ms/epoch - 238ms/step\n",
            "Epoch 69/200\n",
            "1/1 - 0s - loss: 0.4629 - acc: 0.8725 - val_loss: 0.4366 - val_acc: 0.8738 - 235ms/epoch - 235ms/step\n",
            "Epoch 70/200\n",
            "1/1 - 0s - loss: 0.4476 - acc: 0.8725 - val_loss: 0.4224 - val_acc: 0.8738 - 307ms/epoch - 307ms/step\n",
            "Epoch 71/200\n",
            "1/1 - 0s - loss: 0.4428 - acc: 0.8725 - val_loss: 0.4091 - val_acc: 0.8738 - 242ms/epoch - 242ms/step\n",
            "Epoch 72/200\n",
            "1/1 - 0s - loss: 0.4213 - acc: 0.8725 - val_loss: 0.3966 - val_acc: 0.8738 - 246ms/epoch - 246ms/step\n",
            "Epoch 73/200\n",
            "1/1 - 0s - loss: 0.4009 - acc: 0.8725 - val_loss: 0.3850 - val_acc: 0.8738 - 291ms/epoch - 291ms/step\n",
            "Epoch 74/200\n",
            "1/1 - 0s - loss: 0.3834 - acc: 0.8725 - val_loss: 0.3743 - val_acc: 0.8738 - 293ms/epoch - 293ms/step\n",
            "Epoch 75/200\n",
            "1/1 - 0s - loss: 0.3823 - acc: 0.8725 - val_loss: 0.3642 - val_acc: 0.8738 - 289ms/epoch - 289ms/step\n",
            "Epoch 76/200\n",
            "1/1 - 0s - loss: 0.3728 - acc: 0.8725 - val_loss: 0.3548 - val_acc: 0.8738 - 236ms/epoch - 236ms/step\n",
            "Epoch 77/200\n",
            "1/1 - 0s - loss: 0.3636 - acc: 0.8725 - val_loss: 0.3460 - val_acc: 0.8738 - 302ms/epoch - 302ms/step\n",
            "Epoch 78/200\n",
            "1/1 - 0s - loss: 0.3480 - acc: 0.8725 - val_loss: 0.3378 - val_acc: 0.8738 - 289ms/epoch - 289ms/step\n",
            "Epoch 79/200\n",
            "1/1 - 0s - loss: 0.3520 - acc: 0.8725 - val_loss: 0.3301 - val_acc: 0.8738 - 246ms/epoch - 246ms/step\n",
            "Epoch 80/200\n",
            "1/1 - 0s - loss: 0.3465 - acc: 0.8725 - val_loss: 0.3230 - val_acc: 0.8738 - 296ms/epoch - 296ms/step\n",
            "Epoch 81/200\n",
            "1/1 - 0s - loss: 0.3221 - acc: 0.8725 - val_loss: 0.3162 - val_acc: 0.8738 - 304ms/epoch - 304ms/step\n",
            "Epoch 82/200\n",
            "1/1 - 0s - loss: 0.3180 - acc: 0.8725 - val_loss: 0.3100 - val_acc: 0.8738 - 238ms/epoch - 238ms/step\n",
            "Epoch 83/200\n",
            "1/1 - 1s - loss: 0.3133 - acc: 0.8725 - val_loss: 0.3041 - val_acc: 0.8738 - 532ms/epoch - 532ms/step\n",
            "Epoch 84/200\n",
            "1/1 - 1s - loss: 0.3053 - acc: 0.8725 - val_loss: 0.2986 - val_acc: 0.8738 - 580ms/epoch - 580ms/step\n",
            "Epoch 85/200\n",
            "1/1 - 0s - loss: 0.2996 - acc: 0.8725 - val_loss: 0.2934 - val_acc: 0.8738 - 431ms/epoch - 431ms/step\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 1.2488 - acc: 0.8729\n",
            "\n",
            "Test Set Metrics:\n",
            "\tloss: 1.2488\n",
            "\tacc: 0.8729\n",
            "1/1 [==============================] - 0s 360ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GCN (local pooling) filters...\n",
            "Epoch 1/200\n",
            "1/1 - 2s - loss: 1.6882 - acc: 0.0183 - val_loss: 1.6703 - val_acc: 0.0166 - 2s/epoch - 2s/step\n",
            "Epoch 2/200\n",
            "1/1 - 0s - loss: 1.6741 - acc: 0.0270 - val_loss: 1.6592 - val_acc: 0.0166 - 150ms/epoch - 150ms/step\n",
            "Epoch 3/200\n",
            "1/1 - 0s - loss: 1.6670 - acc: 0.0239 - val_loss: 1.6482 - val_acc: 0.0166 - 170ms/epoch - 170ms/step\n",
            "Epoch 4/200\n",
            "1/1 - 0s - loss: 1.6533 - acc: 0.0304 - val_loss: 1.6376 - val_acc: 0.0166 - 165ms/epoch - 165ms/step\n",
            "Epoch 5/200\n",
            "1/1 - 0s - loss: 1.6419 - acc: 0.0217 - val_loss: 1.6273 - val_acc: 0.0166 - 172ms/epoch - 172ms/step\n",
            "Epoch 6/200\n",
            "1/1 - 0s - loss: 1.6336 - acc: 0.0236 - val_loss: 1.6172 - val_acc: 0.0166 - 170ms/epoch - 170ms/step\n",
            "Epoch 7/200\n",
            "1/1 - 0s - loss: 1.6187 - acc: 0.0701 - val_loss: 1.6071 - val_acc: 0.0166 - 147ms/epoch - 147ms/step\n",
            "Epoch 8/200\n",
            "1/1 - 0s - loss: 1.6093 - acc: 0.0245 - val_loss: 1.5970 - val_acc: 0.0199 - 162ms/epoch - 162ms/step\n",
            "Epoch 9/200\n",
            "1/1 - 0s - loss: 1.5972 - acc: 0.0256 - val_loss: 1.5871 - val_acc: 0.0199 - 163ms/epoch - 163ms/step\n",
            "Epoch 10/200\n",
            "1/1 - 0s - loss: 1.5926 - acc: 0.0205 - val_loss: 1.5774 - val_acc: 0.0199 - 159ms/epoch - 159ms/step\n",
            "Epoch 11/200\n",
            "1/1 - 0s - loss: 1.5783 - acc: 0.0205 - val_loss: 1.5677 - val_acc: 0.0199 - 150ms/epoch - 150ms/step\n",
            "Epoch 12/200\n",
            "1/1 - 0s - loss: 1.5706 - acc: 0.0329 - val_loss: 1.5581 - val_acc: 0.0299 - 156ms/epoch - 156ms/step\n",
            "Epoch 13/200\n",
            "1/1 - 0s - loss: 1.5589 - acc: 0.0464 - val_loss: 1.5486 - val_acc: 0.0365 - 171ms/epoch - 171ms/step\n",
            "Epoch 14/200\n",
            "1/1 - 0s - loss: 1.5515 - acc: 0.0284 - val_loss: 1.5390 - val_acc: 0.0365 - 155ms/epoch - 155ms/step\n",
            "Epoch 15/200\n",
            "1/1 - 0s - loss: 1.5381 - acc: 0.0895 - val_loss: 1.5292 - val_acc: 0.0764 - 157ms/epoch - 157ms/step\n",
            "Epoch 16/200\n",
            "1/1 - 0s - loss: 1.5325 - acc: 0.0861 - val_loss: 1.5195 - val_acc: 0.1096 - 168ms/epoch - 168ms/step\n",
            "Epoch 17/200\n",
            "1/1 - 0s - loss: 1.5240 - acc: 0.1146 - val_loss: 1.5097 - val_acc: 0.1130 - 165ms/epoch - 165ms/step\n",
            "Epoch 18/200\n",
            "1/1 - 0s - loss: 1.5152 - acc: 0.1331 - val_loss: 1.5001 - val_acc: 0.1163 - 172ms/epoch - 172ms/step\n",
            "Epoch 19/200\n",
            "1/1 - 0s - loss: 1.5015 - acc: 0.0991 - val_loss: 1.4905 - val_acc: 0.1595 - 154ms/epoch - 154ms/step\n",
            "Epoch 20/200\n",
            "1/1 - 0s - loss: 1.4897 - acc: 0.1520 - val_loss: 1.4809 - val_acc: 0.2757 - 162ms/epoch - 162ms/step\n",
            "Epoch 21/200\n",
            "1/1 - 0s - loss: 1.4784 - acc: 0.2426 - val_loss: 1.4716 - val_acc: 0.2857 - 168ms/epoch - 168ms/step\n",
            "Epoch 22/200\n",
            "1/1 - 0s - loss: 1.4733 - acc: 0.2798 - val_loss: 1.4623 - val_acc: 0.3090 - 156ms/epoch - 156ms/step\n",
            "Epoch 23/200\n",
            "1/1 - 0s - loss: 1.4627 - acc: 0.2902 - val_loss: 1.4530 - val_acc: 0.3223 - 149ms/epoch - 149ms/step\n",
            "Epoch 24/200\n",
            "1/1 - 0s - loss: 1.4498 - acc: 0.3054 - val_loss: 1.4437 - val_acc: 0.3322 - 153ms/epoch - 153ms/step\n",
            "Epoch 25/200\n",
            "1/1 - 0s - loss: 1.4389 - acc: 0.2983 - val_loss: 1.4344 - val_acc: 0.3355 - 162ms/epoch - 162ms/step\n",
            "Epoch 26/200\n",
            "1/1 - 0s - loss: 1.4331 - acc: 0.3363 - val_loss: 1.4250 - val_acc: 0.3355 - 162ms/epoch - 162ms/step\n",
            "Epoch 27/200\n",
            "1/1 - 0s - loss: 1.4206 - acc: 0.3552 - val_loss: 1.4157 - val_acc: 0.3355 - 165ms/epoch - 165ms/step\n",
            "Epoch 28/200\n",
            "1/1 - 0s - loss: 1.4071 - acc: 0.3400 - val_loss: 1.4063 - val_acc: 0.4385 - 152ms/epoch - 152ms/step\n",
            "Epoch 29/200\n",
            "1/1 - 0s - loss: 1.4060 - acc: 0.4796 - val_loss: 1.3964 - val_acc: 0.4452 - 162ms/epoch - 162ms/step\n",
            "Epoch 30/200\n",
            "1/1 - 0s - loss: 1.3841 - acc: 0.4911 - val_loss: 1.3854 - val_acc: 0.4452 - 156ms/epoch - 156ms/step\n",
            "Epoch 31/200\n",
            "1/1 - 0s - loss: 1.3766 - acc: 0.5086 - val_loss: 1.3733 - val_acc: 0.4452 - 152ms/epoch - 152ms/step\n",
            "Epoch 32/200\n",
            "1/1 - 0s - loss: 1.3632 - acc: 0.5663 - val_loss: 1.3601 - val_acc: 0.4684 - 152ms/epoch - 152ms/step\n",
            "Epoch 33/200\n",
            "1/1 - 0s - loss: 1.3496 - acc: 0.6026 - val_loss: 1.3450 - val_acc: 0.6246 - 167ms/epoch - 167ms/step\n",
            "Epoch 34/200\n",
            "1/1 - 0s - loss: 1.3410 - acc: 0.6243 - val_loss: 1.3292 - val_acc: 0.9701 - 293ms/epoch - 293ms/step\n",
            "Epoch 35/200\n",
            "1/1 - 0s - loss: 1.3257 - acc: 0.8278 - val_loss: 1.3130 - val_acc: 0.9701 - 312ms/epoch - 312ms/step\n",
            "Epoch 36/200\n",
            "1/1 - 0s - loss: 1.3138 - acc: 0.8705 - val_loss: 1.2965 - val_acc: 0.9701 - 318ms/epoch - 318ms/step\n",
            "Epoch 37/200\n",
            "1/1 - 0s - loss: 1.2879 - acc: 0.9167 - val_loss: 1.2797 - val_acc: 0.9701 - 334ms/epoch - 334ms/step\n",
            "Epoch 38/200\n",
            "1/1 - 0s - loss: 1.2784 - acc: 0.9403 - val_loss: 1.2615 - val_acc: 0.9701 - 289ms/epoch - 289ms/step\n",
            "Epoch 39/200\n",
            "1/1 - 0s - loss: 1.2560 - acc: 0.9341 - val_loss: 1.2425 - val_acc: 0.9701 - 316ms/epoch - 316ms/step\n",
            "Epoch 40/200\n",
            "1/1 - 0s - loss: 1.2316 - acc: 0.9550 - val_loss: 1.2230 - val_acc: 0.9701 - 321ms/epoch - 321ms/step\n",
            "Epoch 41/200\n",
            "1/1 - 0s - loss: 1.2265 - acc: 0.9524 - val_loss: 1.2032 - val_acc: 0.9701 - 313ms/epoch - 313ms/step\n",
            "Epoch 42/200\n",
            "1/1 - 0s - loss: 1.2035 - acc: 0.9544 - val_loss: 1.1832 - val_acc: 0.9701 - 296ms/epoch - 296ms/step\n",
            "Epoch 43/200\n",
            "1/1 - 0s - loss: 1.1907 - acc: 0.9696 - val_loss: 1.1626 - val_acc: 0.9701 - 291ms/epoch - 291ms/step\n",
            "Epoch 44/200\n",
            "1/1 - 0s - loss: 1.1659 - acc: 0.9572 - val_loss: 1.1414 - val_acc: 0.9668 - 294ms/epoch - 294ms/step\n",
            "Epoch 45/200\n",
            "1/1 - 0s - loss: 1.1464 - acc: 0.9552 - val_loss: 1.1200 - val_acc: 0.9668 - 300ms/epoch - 300ms/step\n",
            "Epoch 46/200\n",
            "1/1 - 0s - loss: 1.1234 - acc: 0.9592 - val_loss: 1.0984 - val_acc: 0.9668 - 185ms/epoch - 185ms/step\n",
            "Epoch 47/200\n",
            "1/1 - 0s - loss: 1.0966 - acc: 0.9589 - val_loss: 1.0766 - val_acc: 0.9668 - 159ms/epoch - 159ms/step\n",
            "Epoch 48/200\n",
            "1/1 - 0s - loss: 1.0836 - acc: 0.9614 - val_loss: 1.0549 - val_acc: 0.9668 - 160ms/epoch - 160ms/step\n",
            "Epoch 49/200\n",
            "1/1 - 0s - loss: 1.0661 - acc: 0.9589 - val_loss: 1.0330 - val_acc: 0.9668 - 155ms/epoch - 155ms/step\n",
            "Epoch 50/200\n",
            "1/1 - 0s - loss: 1.0253 - acc: 0.9623 - val_loss: 1.0112 - val_acc: 0.9668 - 157ms/epoch - 157ms/step\n",
            "Epoch 51/200\n",
            "1/1 - 0s - loss: 1.0133 - acc: 0.9575 - val_loss: 0.9894 - val_acc: 0.9668 - 149ms/epoch - 149ms/step\n",
            "Epoch 52/200\n",
            "1/1 - 0s - loss: 1.0071 - acc: 0.9598 - val_loss: 0.9677 - val_acc: 0.9668 - 163ms/epoch - 163ms/step\n",
            "Epoch 53/200\n",
            "1/1 - 0s - loss: 0.9829 - acc: 0.9578 - val_loss: 0.9462 - val_acc: 0.9668 - 151ms/epoch - 151ms/step\n",
            "Epoch 54/200\n",
            "1/1 - 0s - loss: 0.9631 - acc: 0.9598 - val_loss: 0.9247 - val_acc: 0.9668 - 147ms/epoch - 147ms/step\n",
            "Epoch 55/200\n",
            "1/1 - 0s - loss: 0.9343 - acc: 0.9583 - val_loss: 0.9033 - val_acc: 0.9668 - 154ms/epoch - 154ms/step\n",
            "Epoch 56/200\n",
            "1/1 - 0s - loss: 0.9193 - acc: 0.9598 - val_loss: 0.8821 - val_acc: 0.9668 - 164ms/epoch - 164ms/step\n",
            "Epoch 57/200\n",
            "1/1 - 0s - loss: 0.8913 - acc: 0.9592 - val_loss: 0.8611 - val_acc: 0.9668 - 154ms/epoch - 154ms/step\n",
            "Epoch 58/200\n",
            "1/1 - 0s - loss: 0.8701 - acc: 0.9609 - val_loss: 0.8402 - val_acc: 0.9668 - 147ms/epoch - 147ms/step\n",
            "Epoch 59/200\n",
            "1/1 - 0s - loss: 0.8522 - acc: 0.9592 - val_loss: 0.8197 - val_acc: 0.9668 - 160ms/epoch - 160ms/step\n",
            "Epoch 60/200\n",
            "1/1 - 0s - loss: 0.8364 - acc: 0.9589 - val_loss: 0.7993 - val_acc: 0.9668 - 150ms/epoch - 150ms/step\n",
            "Epoch 61/200\n",
            "1/1 - 0s - loss: 0.8228 - acc: 0.9589 - val_loss: 0.7792 - val_acc: 0.9668 - 156ms/epoch - 156ms/step\n",
            "Epoch 62/200\n",
            "1/1 - 0s - loss: 0.7959 - acc: 0.9589 - val_loss: 0.7593 - val_acc: 0.9668 - 151ms/epoch - 151ms/step\n",
            "Epoch 63/200\n",
            "1/1 - 0s - loss: 0.7695 - acc: 0.9583 - val_loss: 0.7398 - val_acc: 0.9668 - 145ms/epoch - 145ms/step\n",
            "Epoch 64/200\n",
            "1/1 - 0s - loss: 0.7587 - acc: 0.9586 - val_loss: 0.7206 - val_acc: 0.9668 - 154ms/epoch - 154ms/step\n",
            "Epoch 65/200\n",
            "1/1 - 0s - loss: 0.7372 - acc: 0.9583 - val_loss: 0.7015 - val_acc: 0.9668 - 153ms/epoch - 153ms/step\n",
            "Epoch 66/200\n",
            "1/1 - 0s - loss: 0.7237 - acc: 0.9600 - val_loss: 0.6828 - val_acc: 0.9668 - 158ms/epoch - 158ms/step\n",
            "Epoch 67/200\n",
            "1/1 - 0s - loss: 0.7048 - acc: 0.9595 - val_loss: 0.6643 - val_acc: 0.9668 - 155ms/epoch - 155ms/step\n",
            "Epoch 68/200\n",
            "1/1 - 0s - loss: 0.6868 - acc: 0.9589 - val_loss: 0.6461 - val_acc: 0.9668 - 162ms/epoch - 162ms/step\n",
            "Epoch 69/200\n",
            "1/1 - 0s - loss: 0.6646 - acc: 0.9606 - val_loss: 0.6283 - val_acc: 0.9668 - 163ms/epoch - 163ms/step\n",
            "Epoch 70/200\n",
            "1/1 - 0s - loss: 0.6337 - acc: 0.9598 - val_loss: 0.6109 - val_acc: 0.9668 - 165ms/epoch - 165ms/step\n",
            "Epoch 71/200\n",
            "1/1 - 0s - loss: 0.6256 - acc: 0.9614 - val_loss: 0.5939 - val_acc: 0.9668 - 150ms/epoch - 150ms/step\n",
            "Epoch 72/200\n",
            "1/1 - 0s - loss: 0.6089 - acc: 0.9600 - val_loss: 0.5771 - val_acc: 0.9668 - 155ms/epoch - 155ms/step\n",
            "Epoch 73/200\n",
            "1/1 - 0s - loss: 0.5820 - acc: 0.9589 - val_loss: 0.5606 - val_acc: 0.9668 - 164ms/epoch - 164ms/step\n",
            "Epoch 74/200\n",
            "1/1 - 0s - loss: 0.5754 - acc: 0.9592 - val_loss: 0.5444 - val_acc: 0.9668 - 162ms/epoch - 162ms/step\n",
            "Epoch 75/200\n",
            "1/1 - 0s - loss: 0.5558 - acc: 0.9595 - val_loss: 0.5282 - val_acc: 0.9668 - 151ms/epoch - 151ms/step\n",
            "Epoch 76/200\n",
            "1/1 - 0s - loss: 0.5498 - acc: 0.9592 - val_loss: 0.5124 - val_acc: 0.9668 - 169ms/epoch - 169ms/step\n",
            "Epoch 77/200\n",
            "1/1 - 0s - loss: 0.5243 - acc: 0.9586 - val_loss: 0.4971 - val_acc: 0.9668 - 151ms/epoch - 151ms/step\n",
            "Epoch 78/200\n",
            "1/1 - 0s - loss: 0.5152 - acc: 0.9657 - val_loss: 0.4822 - val_acc: 0.9668 - 198ms/epoch - 198ms/step\n",
            "Epoch 79/200\n",
            "1/1 - 0s - loss: 0.4939 - acc: 0.9598 - val_loss: 0.4676 - val_acc: 0.9668 - 168ms/epoch - 168ms/step\n",
            "Epoch 80/200\n",
            "1/1 - 0s - loss: 0.4814 - acc: 0.9592 - val_loss: 0.4535 - val_acc: 0.9668 - 167ms/epoch - 167ms/step\n",
            "Epoch 81/200\n",
            "1/1 - 0s - loss: 0.4598 - acc: 0.9595 - val_loss: 0.4398 - val_acc: 0.9668 - 152ms/epoch - 152ms/step\n",
            "Epoch 82/200\n",
            "1/1 - 0s - loss: 0.4619 - acc: 0.9600 - val_loss: 0.4265 - val_acc: 0.9668 - 148ms/epoch - 148ms/step\n",
            "Epoch 83/200\n",
            "1/1 - 0s - loss: 0.4358 - acc: 0.9614 - val_loss: 0.4137 - val_acc: 0.9668 - 262ms/epoch - 262ms/step\n",
            "Epoch 84/200\n",
            "1/1 - 0s - loss: 0.4357 - acc: 0.9721 - val_loss: 0.4014 - val_acc: 0.9668 - 243ms/epoch - 243ms/step\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 1.3293 - acc: 0.9724\n",
            "\n",
            "Test Set Metrics:\n",
            "\tloss: 1.3293\n",
            "\tacc: 0.9724\n",
            "1/1 [==============================] - 0s 356ms/step\n",
            "Using GCN (local pooling) filters...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1/1 - 2s - loss: 1.7112 - acc: 0.0039 - val_loss: 1.6891 - val_acc: 0.0000e+00 - 2s/epoch - 2s/step\n",
            "Epoch 2/200\n",
            "1/1 - 0s - loss: 1.7028 - acc: 0.0039 - val_loss: 1.6792 - val_acc: 0.0000e+00 - 164ms/epoch - 164ms/step\n",
            "Epoch 3/200\n",
            "1/1 - 0s - loss: 1.6895 - acc: 0.0031 - val_loss: 1.6693 - val_acc: 0.0000e+00 - 166ms/epoch - 166ms/step\n",
            "Epoch 4/200\n",
            "1/1 - 0s - loss: 1.6760 - acc: 0.0065 - val_loss: 1.6593 - val_acc: 0.0000e+00 - 164ms/epoch - 164ms/step\n",
            "Epoch 5/200\n",
            "1/1 - 0s - loss: 1.6630 - acc: 0.0096 - val_loss: 1.6492 - val_acc: 0.0000e+00 - 154ms/epoch - 154ms/step\n",
            "Epoch 6/200\n",
            "1/1 - 0s - loss: 1.6542 - acc: 0.0152 - val_loss: 1.6391 - val_acc: 0.0000e+00 - 159ms/epoch - 159ms/step\n",
            "Epoch 7/200\n",
            "1/1 - 0s - loss: 1.6464 - acc: 0.0160 - val_loss: 1.6291 - val_acc: 0.0000e+00 - 159ms/epoch - 159ms/step\n",
            "Epoch 8/200\n",
            "1/1 - 0s - loss: 1.6374 - acc: 0.0338 - val_loss: 1.6192 - val_acc: 0.0000e+00 - 154ms/epoch - 154ms/step\n",
            "Epoch 9/200\n",
            "1/1 - 0s - loss: 1.6248 - acc: 0.0231 - val_loss: 1.6094 - val_acc: 0.0000e+00 - 159ms/epoch - 159ms/step\n",
            "Epoch 10/200\n",
            "1/1 - 0s - loss: 1.6128 - acc: 0.0540 - val_loss: 1.5999 - val_acc: 0.0000e+00 - 156ms/epoch - 156ms/step\n",
            "Epoch 11/200\n",
            "1/1 - 0s - loss: 1.6077 - acc: 0.1970 - val_loss: 1.5904 - val_acc: 0.1794 - 165ms/epoch - 165ms/step\n",
            "Epoch 12/200\n",
            "1/1 - 0s - loss: 1.6026 - acc: 0.1596 - val_loss: 1.5810 - val_acc: 0.3688 - 153ms/epoch - 153ms/step\n",
            "Epoch 13/200\n",
            "1/1 - 0s - loss: 1.5863 - acc: 0.1646 - val_loss: 1.5718 - val_acc: 0.3887 - 157ms/epoch - 157ms/step\n",
            "Epoch 14/200\n",
            "1/1 - 0s - loss: 1.5673 - acc: 0.4450 - val_loss: 1.5627 - val_acc: 0.4684 - 164ms/epoch - 164ms/step\n",
            "Epoch 15/200\n",
            "1/1 - 0s - loss: 1.5732 - acc: 0.4728 - val_loss: 1.5536 - val_acc: 0.4917 - 167ms/epoch - 167ms/step\n",
            "Epoch 16/200\n",
            "1/1 - 0s - loss: 1.5668 - acc: 0.5111 - val_loss: 1.5447 - val_acc: 0.5249 - 167ms/epoch - 167ms/step\n",
            "Epoch 17/200\n",
            "1/1 - 0s - loss: 1.5498 - acc: 0.5525 - val_loss: 1.5358 - val_acc: 0.5382 - 163ms/epoch - 163ms/step\n",
            "Epoch 18/200\n",
            "1/1 - 0s - loss: 1.5359 - acc: 0.5885 - val_loss: 1.5270 - val_acc: 0.7143 - 165ms/epoch - 165ms/step\n",
            "Epoch 19/200\n",
            "1/1 - 0s - loss: 1.5324 - acc: 0.5840 - val_loss: 1.5183 - val_acc: 0.8704 - 173ms/epoch - 173ms/step\n",
            "Epoch 20/200\n",
            "1/1 - 0s - loss: 1.5232 - acc: 0.7371 - val_loss: 1.5100 - val_acc: 0.8704 - 168ms/epoch - 168ms/step\n",
            "Epoch 21/200\n",
            "1/1 - 0s - loss: 1.5151 - acc: 0.7152 - val_loss: 1.5018 - val_acc: 0.8704 - 155ms/epoch - 155ms/step\n",
            "Epoch 22/200\n",
            "1/1 - 0s - loss: 1.5130 - acc: 0.7878 - val_loss: 1.4937 - val_acc: 0.8704 - 170ms/epoch - 170ms/step\n",
            "Epoch 23/200\n",
            "1/1 - 0s - loss: 1.5057 - acc: 0.7931 - val_loss: 1.4857 - val_acc: 0.8704 - 161ms/epoch - 161ms/step\n",
            "Epoch 24/200\n",
            "1/1 - 0s - loss: 1.4947 - acc: 0.8069 - val_loss: 1.4777 - val_acc: 0.8704 - 170ms/epoch - 170ms/step\n",
            "Epoch 25/200\n",
            "1/1 - 0s - loss: 1.4832 - acc: 0.8055 - val_loss: 1.4699 - val_acc: 0.8704 - 156ms/epoch - 156ms/step\n",
            "Epoch 26/200\n",
            "1/1 - 0s - loss: 1.4704 - acc: 0.8469 - val_loss: 1.4624 - val_acc: 0.8704 - 165ms/epoch - 165ms/step\n",
            "Epoch 27/200\n",
            "1/1 - 0s - loss: 1.4644 - acc: 0.8494 - val_loss: 1.4550 - val_acc: 0.8704 - 155ms/epoch - 155ms/step\n",
            "Epoch 28/200\n",
            "1/1 - 0s - loss: 1.4643 - acc: 0.8269 - val_loss: 1.4477 - val_acc: 0.8738 - 161ms/epoch - 161ms/step\n",
            "Epoch 29/200\n",
            "1/1 - 0s - loss: 1.4534 - acc: 0.8390 - val_loss: 1.4404 - val_acc: 0.8771 - 165ms/epoch - 165ms/step\n",
            "Epoch 30/200\n",
            "1/1 - 0s - loss: 1.4410 - acc: 0.8542 - val_loss: 1.4331 - val_acc: 0.8771 - 158ms/epoch - 158ms/step\n",
            "Epoch 31/200\n",
            "1/1 - 0s - loss: 1.4354 - acc: 0.8536 - val_loss: 1.4257 - val_acc: 0.8771 - 162ms/epoch - 162ms/step\n",
            "Epoch 32/200\n",
            "1/1 - 0s - loss: 1.4305 - acc: 0.8604 - val_loss: 1.4183 - val_acc: 0.8771 - 165ms/epoch - 165ms/step\n",
            "Epoch 33/200\n",
            "1/1 - 0s - loss: 1.4182 - acc: 0.8598 - val_loss: 1.4109 - val_acc: 0.8771 - 158ms/epoch - 158ms/step\n",
            "Epoch 34/200\n",
            "1/1 - 0s - loss: 1.4168 - acc: 0.8688 - val_loss: 1.4035 - val_acc: 0.8771 - 169ms/epoch - 169ms/step\n",
            "Epoch 35/200\n",
            "1/1 - 0s - loss: 1.4026 - acc: 0.8686 - val_loss: 1.3961 - val_acc: 0.8771 - 153ms/epoch - 153ms/step\n",
            "Epoch 36/200\n",
            "1/1 - 0s - loss: 1.3999 - acc: 0.8641 - val_loss: 1.3888 - val_acc: 0.8771 - 166ms/epoch - 166ms/step\n",
            "Epoch 37/200\n",
            "1/1 - 0s - loss: 1.3902 - acc: 0.8849 - val_loss: 1.3817 - val_acc: 0.8771 - 206ms/epoch - 206ms/step\n",
            "Epoch 38/200\n",
            "1/1 - 0s - loss: 1.3861 - acc: 0.8672 - val_loss: 1.3745 - val_acc: 0.8738 - 273ms/epoch - 273ms/step\n",
            "Epoch 39/200\n",
            "1/1 - 0s - loss: 1.3806 - acc: 0.8621 - val_loss: 1.3674 - val_acc: 0.8738 - 279ms/epoch - 279ms/step\n",
            "Epoch 40/200\n",
            "1/1 - 0s - loss: 1.3804 - acc: 0.8691 - val_loss: 1.3602 - val_acc: 0.8738 - 324ms/epoch - 324ms/step\n",
            "Epoch 41/200\n",
            "1/1 - 0s - loss: 1.3618 - acc: 0.8714 - val_loss: 1.3530 - val_acc: 0.8738 - 272ms/epoch - 272ms/step\n",
            "Epoch 42/200\n",
            "1/1 - 0s - loss: 1.3555 - acc: 0.8708 - val_loss: 1.3457 - val_acc: 0.8738 - 273ms/epoch - 273ms/step\n",
            "Epoch 43/200\n",
            "1/1 - 0s - loss: 1.3467 - acc: 0.8756 - val_loss: 1.3384 - val_acc: 0.8738 - 323ms/epoch - 323ms/step\n",
            "Epoch 44/200\n",
            "1/1 - 0s - loss: 1.3339 - acc: 0.8776 - val_loss: 1.3310 - val_acc: 0.8738 - 275ms/epoch - 275ms/step\n",
            "Epoch 45/200\n",
            "1/1 - 0s - loss: 1.3307 - acc: 0.8748 - val_loss: 1.3236 - val_acc: 0.8738 - 260ms/epoch - 260ms/step\n",
            "Epoch 46/200\n",
            "1/1 - 0s - loss: 1.3303 - acc: 0.8742 - val_loss: 1.3161 - val_acc: 0.8837 - 274ms/epoch - 274ms/step\n",
            "Epoch 47/200\n",
            "1/1 - 0s - loss: 1.3171 - acc: 0.8826 - val_loss: 1.3086 - val_acc: 0.8870 - 239ms/epoch - 239ms/step\n",
            "Epoch 48/200\n",
            "1/1 - 0s - loss: 1.3122 - acc: 0.8756 - val_loss: 1.3010 - val_acc: 0.8870 - 295ms/epoch - 295ms/step\n",
            "Epoch 49/200\n",
            "1/1 - 0s - loss: 1.3067 - acc: 0.8807 - val_loss: 1.2934 - val_acc: 0.8870 - 227ms/epoch - 227ms/step\n",
            "Epoch 50/200\n",
            "1/1 - 0s - loss: 1.2934 - acc: 0.8880 - val_loss: 1.2858 - val_acc: 0.8870 - 307ms/epoch - 307ms/step\n",
            "Epoch 51/200\n",
            "1/1 - 0s - loss: 1.2824 - acc: 0.8762 - val_loss: 1.2781 - val_acc: 0.8870 - 305ms/epoch - 305ms/step\n",
            "Epoch 52/200\n",
            "1/1 - 0s - loss: 1.2844 - acc: 0.8860 - val_loss: 1.2704 - val_acc: 0.8870 - 294ms/epoch - 294ms/step\n",
            "Epoch 53/200\n",
            "1/1 - 0s - loss: 1.2725 - acc: 0.8919 - val_loss: 1.2626 - val_acc: 0.8870 - 252ms/epoch - 252ms/step\n",
            "Epoch 54/200\n",
            "1/1 - 0s - loss: 1.2615 - acc: 0.8914 - val_loss: 1.2548 - val_acc: 0.8870 - 240ms/epoch - 240ms/step\n",
            "Epoch 55/200\n",
            "1/1 - 0s - loss: 1.2564 - acc: 0.8846 - val_loss: 1.2468 - val_acc: 0.8870 - 185ms/epoch - 185ms/step\n",
            "Epoch 56/200\n",
            "1/1 - 0s - loss: 1.2419 - acc: 0.8871 - val_loss: 1.2388 - val_acc: 0.8870 - 157ms/epoch - 157ms/step\n",
            "Epoch 57/200\n",
            "1/1 - 0s - loss: 1.2431 - acc: 0.8871 - val_loss: 1.2307 - val_acc: 0.8870 - 166ms/epoch - 166ms/step\n",
            "Epoch 58/200\n",
            "1/1 - 0s - loss: 1.2421 - acc: 0.8908 - val_loss: 1.2225 - val_acc: 0.8870 - 156ms/epoch - 156ms/step\n",
            "Epoch 59/200\n",
            "1/1 - 0s - loss: 1.2336 - acc: 0.8914 - val_loss: 1.2143 - val_acc: 0.8870 - 157ms/epoch - 157ms/step\n",
            "Epoch 60/200\n",
            "1/1 - 0s - loss: 1.2160 - acc: 0.8891 - val_loss: 1.2060 - val_acc: 0.8870 - 165ms/epoch - 165ms/step\n",
            "Epoch 61/200\n",
            "1/1 - 0s - loss: 1.2030 - acc: 0.8942 - val_loss: 1.1977 - val_acc: 0.8870 - 162ms/epoch - 162ms/step\n",
            "Epoch 62/200\n",
            "1/1 - 0s - loss: 1.1877 - acc: 0.8781 - val_loss: 1.1892 - val_acc: 0.8870 - 160ms/epoch - 160ms/step\n",
            "Epoch 63/200\n",
            "1/1 - 0s - loss: 1.1849 - acc: 0.8919 - val_loss: 1.1807 - val_acc: 0.8870 - 156ms/epoch - 156ms/step\n",
            "Epoch 64/200\n",
            "1/1 - 0s - loss: 1.1889 - acc: 0.8914 - val_loss: 1.1721 - val_acc: 0.8870 - 164ms/epoch - 164ms/step\n",
            "Epoch 65/200\n",
            "1/1 - 0s - loss: 1.1794 - acc: 0.8883 - val_loss: 1.1634 - val_acc: 0.8870 - 162ms/epoch - 162ms/step\n",
            "Epoch 66/200\n",
            "1/1 - 0s - loss: 1.1720 - acc: 0.8888 - val_loss: 1.1547 - val_acc: 0.8870 - 167ms/epoch - 167ms/step\n",
            "Epoch 67/200\n",
            "1/1 - 0s - loss: 1.1581 - acc: 0.8897 - val_loss: 1.1459 - val_acc: 0.8870 - 165ms/epoch - 165ms/step\n",
            "Epoch 68/200\n",
            "1/1 - 0s - loss: 1.1605 - acc: 0.8967 - val_loss: 1.1371 - val_acc: 0.8870 - 172ms/epoch - 172ms/step\n",
            "Epoch 69/200\n",
            "1/1 - 0s - loss: 1.1456 - acc: 0.8928 - val_loss: 1.1281 - val_acc: 0.8870 - 164ms/epoch - 164ms/step\n",
            "Epoch 70/200\n",
            "1/1 - 0s - loss: 1.1301 - acc: 0.8911 - val_loss: 1.1191 - val_acc: 0.8870 - 155ms/epoch - 155ms/step\n",
            "Epoch 71/200\n",
            "1/1 - 0s - loss: 1.1119 - acc: 0.8914 - val_loss: 1.1100 - val_acc: 0.8870 - 163ms/epoch - 163ms/step\n",
            "Epoch 72/200\n",
            "1/1 - 0s - loss: 1.1007 - acc: 0.8866 - val_loss: 1.1007 - val_acc: 0.8870 - 170ms/epoch - 170ms/step\n",
            "Epoch 73/200\n",
            "1/1 - 0s - loss: 1.0967 - acc: 0.8905 - val_loss: 1.0914 - val_acc: 0.8870 - 154ms/epoch - 154ms/step\n",
            "Epoch 74/200\n",
            "1/1 - 0s - loss: 1.0955 - acc: 0.8916 - val_loss: 1.0819 - val_acc: 0.8870 - 156ms/epoch - 156ms/step\n",
            "Epoch 75/200\n",
            "1/1 - 0s - loss: 1.0789 - acc: 0.8905 - val_loss: 1.0716 - val_acc: 0.8870 - 172ms/epoch - 172ms/step\n",
            "Epoch 76/200\n",
            "1/1 - 0s - loss: 1.0698 - acc: 0.8914 - val_loss: 1.0608 - val_acc: 0.8870 - 160ms/epoch - 160ms/step\n",
            "Epoch 77/200\n",
            "1/1 - 0s - loss: 1.0683 - acc: 0.8933 - val_loss: 1.0496 - val_acc: 0.8870 - 172ms/epoch - 172ms/step\n",
            "Epoch 78/200\n",
            "1/1 - 0s - loss: 1.0580 - acc: 0.8947 - val_loss: 1.0382 - val_acc: 0.8870 - 167ms/epoch - 167ms/step\n",
            "Epoch 79/200\n",
            "1/1 - 0s - loss: 1.0413 - acc: 0.8919 - val_loss: 1.0266 - val_acc: 0.8870 - 157ms/epoch - 157ms/step\n",
            "Epoch 80/200\n",
            "1/1 - 0s - loss: 1.0253 - acc: 0.8922 - val_loss: 1.0149 - val_acc: 0.8870 - 163ms/epoch - 163ms/step\n",
            "Epoch 81/200\n",
            "1/1 - 0s - loss: 1.0084 - acc: 0.8922 - val_loss: 1.0031 - val_acc: 0.8870 - 160ms/epoch - 160ms/step\n",
            "Epoch 82/200\n",
            "1/1 - 0s - loss: 1.0000 - acc: 0.8919 - val_loss: 0.9911 - val_acc: 0.8870 - 164ms/epoch - 164ms/step\n",
            "Epoch 83/200\n",
            "1/1 - 0s - loss: 0.9920 - acc: 0.8928 - val_loss: 0.9790 - val_acc: 0.8870 - 156ms/epoch - 156ms/step\n",
            "Epoch 84/200\n",
            "1/1 - 0s - loss: 1.0000 - acc: 0.8936 - val_loss: 0.9665 - val_acc: 0.8870 - 176ms/epoch - 176ms/step\n",
            "Epoch 85/200\n",
            "1/1 - 0s - loss: 0.9779 - acc: 0.8911 - val_loss: 0.9536 - val_acc: 0.8870 - 159ms/epoch - 159ms/step\n",
            "Epoch 86/200\n",
            "1/1 - 0s - loss: 0.9631 - acc: 0.8933 - val_loss: 0.9405 - val_acc: 0.8870 - 167ms/epoch - 167ms/step\n",
            "Epoch 87/200\n",
            "1/1 - 0s - loss: 0.9498 - acc: 0.8922 - val_loss: 0.9273 - val_acc: 0.8870 - 158ms/epoch - 158ms/step\n",
            "Epoch 88/200\n",
            "1/1 - 0s - loss: 0.9209 - acc: 0.9015 - val_loss: 0.9139 - val_acc: 0.8870 - 156ms/epoch - 156ms/step\n",
            "Epoch 89/200\n",
            "1/1 - 0s - loss: 0.9273 - acc: 0.8978 - val_loss: 0.9004 - val_acc: 0.8904 - 174ms/epoch - 174ms/step\n",
            "Epoch 90/200\n",
            "1/1 - 0s - loss: 0.9087 - acc: 0.8925 - val_loss: 0.8869 - val_acc: 0.8904 - 164ms/epoch - 164ms/step\n",
            "Epoch 91/200\n",
            "1/1 - 0s - loss: 0.8861 - acc: 0.8967 - val_loss: 0.8732 - val_acc: 0.8904 - 154ms/epoch - 154ms/step\n",
            "Epoch 92/200\n",
            "1/1 - 0s - loss: 0.8805 - acc: 0.8916 - val_loss: 0.8593 - val_acc: 0.8904 - 158ms/epoch - 158ms/step\n",
            "Epoch 93/200\n",
            "1/1 - 0s - loss: 0.8601 - acc: 0.9023 - val_loss: 0.8454 - val_acc: 0.8904 - 190ms/epoch - 190ms/step\n",
            "Epoch 94/200\n",
            "1/1 - 0s - loss: 0.8444 - acc: 0.8930 - val_loss: 0.8315 - val_acc: 0.8904 - 240ms/epoch - 240ms/step\n",
            "Epoch 95/200\n",
            "1/1 - 0s - loss: 0.8462 - acc: 0.8925 - val_loss: 0.8176 - val_acc: 0.8904 - 280ms/epoch - 280ms/step\n",
            "Epoch 96/200\n",
            "1/1 - 0s - loss: 0.8282 - acc: 0.8947 - val_loss: 0.8035 - val_acc: 0.8904 - 293ms/epoch - 293ms/step\n",
            "Epoch 97/200\n",
            "1/1 - 0s - loss: 0.8062 - acc: 0.8984 - val_loss: 0.7891 - val_acc: 0.8904 - 268ms/epoch - 268ms/step\n",
            "Epoch 98/200\n",
            "1/1 - 0s - loss: 0.7839 - acc: 0.9004 - val_loss: 0.7748 - val_acc: 0.8904 - 270ms/epoch - 270ms/step\n",
            "Epoch 99/200\n",
            "1/1 - 0s - loss: 0.7957 - acc: 0.8922 - val_loss: 0.7605 - val_acc: 0.8904 - 312ms/epoch - 312ms/step\n",
            "Epoch 100/200\n",
            "1/1 - 0s - loss: 0.7672 - acc: 0.9246 - val_loss: 0.7462 - val_acc: 0.8904 - 277ms/epoch - 277ms/step\n",
            "Epoch 101/200\n",
            "1/1 - 0s - loss: 0.7482 - acc: 0.9189 - val_loss: 0.7319 - val_acc: 0.8904 - 269ms/epoch - 269ms/step\n",
            "Epoch 102/200\n",
            "1/1 - 0s - loss: 0.7391 - acc: 0.9009 - val_loss: 0.7177 - val_acc: 0.8904 - 346ms/epoch - 346ms/step\n",
            "Epoch 103/200\n",
            "1/1 - 0s - loss: 0.7274 - acc: 0.9012 - val_loss: 0.7036 - val_acc: 0.8904 - 305ms/epoch - 305ms/step\n",
            "Epoch 104/200\n",
            "1/1 - 0s - loss: 0.7150 - acc: 0.9074 - val_loss: 0.6895 - val_acc: 0.8904 - 258ms/epoch - 258ms/step\n",
            "Epoch 105/200\n",
            "1/1 - 0s - loss: 0.7081 - acc: 0.9401 - val_loss: 0.6755 - val_acc: 0.8904 - 321ms/epoch - 321ms/step\n",
            "Epoch 106/200\n",
            "1/1 - 0s - loss: 0.6704 - acc: 0.9023 - val_loss: 0.6616 - val_acc: 0.9568 - 301ms/epoch - 301ms/step\n",
            "Epoch 107/200\n",
            "1/1 - 0s - loss: 0.6679 - acc: 0.9401 - val_loss: 0.6477 - val_acc: 0.9568 - 298ms/epoch - 298ms/step\n",
            "Epoch 108/200\n",
            "1/1 - 0s - loss: 0.6410 - acc: 0.9474 - val_loss: 0.6337 - val_acc: 0.9767 - 309ms/epoch - 309ms/step\n",
            "Epoch 109/200\n",
            "1/1 - 0s - loss: 0.6583 - acc: 0.9071 - val_loss: 0.6198 - val_acc: 0.9767 - 308ms/epoch - 308ms/step\n",
            "Epoch 110/200\n",
            "1/1 - 0s - loss: 0.6369 - acc: 0.9417 - val_loss: 0.6060 - val_acc: 0.9767 - 231ms/epoch - 231ms/step\n",
            "Epoch 111/200\n",
            "1/1 - 0s - loss: 0.6008 - acc: 0.9496 - val_loss: 0.5923 - val_acc: 0.9767 - 152ms/epoch - 152ms/step\n",
            "Epoch 112/200\n",
            "1/1 - 0s - loss: 0.6053 - acc: 0.9721 - val_loss: 0.5788 - val_acc: 0.9767 - 173ms/epoch - 173ms/step\n",
            "Epoch 113/200\n",
            "1/1 - 0s - loss: 0.5787 - acc: 0.9811 - val_loss: 0.5655 - val_acc: 0.9767 - 162ms/epoch - 162ms/step\n",
            "Epoch 114/200\n",
            "1/1 - 0s - loss: 0.5917 - acc: 0.9012 - val_loss: 0.5524 - val_acc: 0.9767 - 154ms/epoch - 154ms/step\n",
            "Epoch 115/200\n",
            "1/1 - 0s - loss: 0.5814 - acc: 0.9488 - val_loss: 0.5395 - val_acc: 0.9767 - 153ms/epoch - 153ms/step\n",
            "Epoch 116/200\n",
            "1/1 - 0s - loss: 0.5526 - acc: 0.9229 - val_loss: 0.5269 - val_acc: 0.9767 - 168ms/epoch - 168ms/step\n",
            "Epoch 117/200\n",
            "1/1 - 0s - loss: 0.5390 - acc: 0.9347 - val_loss: 0.5145 - val_acc: 0.9801 - 156ms/epoch - 156ms/step\n",
            "Epoch 118/200\n",
            "1/1 - 0s - loss: 0.5360 - acc: 0.9724 - val_loss: 0.5025 - val_acc: 0.9801 - 155ms/epoch - 155ms/step\n",
            "Epoch 119/200\n",
            "1/1 - 0s - loss: 0.5280 - acc: 0.9780 - val_loss: 0.4907 - val_acc: 0.9801 - 166ms/epoch - 166ms/step\n",
            "Epoch 120/200\n",
            "1/1 - 0s - loss: 0.5081 - acc: 0.9735 - val_loss: 0.4792 - val_acc: 0.9801 - 162ms/epoch - 162ms/step\n",
            "Epoch 121/200\n",
            "1/1 - 0s - loss: 0.4895 - acc: 0.9741 - val_loss: 0.4680 - val_acc: 0.9801 - 167ms/epoch - 167ms/step\n",
            "Epoch 122/200\n",
            "1/1 - 0s - loss: 0.4832 - acc: 0.9789 - val_loss: 0.4570 - val_acc: 0.9801 - 167ms/epoch - 167ms/step\n",
            "Epoch 123/200\n",
            "1/1 - 0s - loss: 0.4728 - acc: 0.9792 - val_loss: 0.4464 - val_acc: 0.9801 - 163ms/epoch - 163ms/step\n",
            "Epoch 124/200\n",
            "1/1 - 0s - loss: 0.4547 - acc: 0.9809 - val_loss: 0.4359 - val_acc: 0.9801 - 166ms/epoch - 166ms/step\n",
            "Epoch 125/200\n",
            "1/1 - 0s - loss: 0.4475 - acc: 0.9735 - val_loss: 0.4255 - val_acc: 0.9801 - 160ms/epoch - 160ms/step\n",
            "Epoch 126/200\n",
            "1/1 - 0s - loss: 0.4219 - acc: 0.9811 - val_loss: 0.4147 - val_acc: 0.9801 - 163ms/epoch - 163ms/step\n",
            "Epoch 127/200\n",
            "1/1 - 0s - loss: 0.4287 - acc: 0.9589 - val_loss: 0.4041 - val_acc: 0.9801 - 150ms/epoch - 150ms/step\n",
            "Epoch 128/200\n",
            "1/1 - 0s - loss: 0.4132 - acc: 0.9800 - val_loss: 0.3936 - val_acc: 0.9801 - 152ms/epoch - 152ms/step\n",
            "Epoch 129/200\n",
            "1/1 - 0s - loss: 0.3987 - acc: 0.9814 - val_loss: 0.3834 - val_acc: 0.9801 - 151ms/epoch - 151ms/step\n",
            "Epoch 130/200\n",
            "1/1 - 0s - loss: 0.3961 - acc: 0.9803 - val_loss: 0.3734 - val_acc: 0.9801 - 164ms/epoch - 164ms/step\n",
            "Epoch 131/200\n",
            "1/1 - 0s - loss: 0.3790 - acc: 0.9755 - val_loss: 0.3637 - val_acc: 0.9801 - 162ms/epoch - 162ms/step\n",
            "Epoch 132/200\n",
            "1/1 - 0s - loss: 0.3761 - acc: 0.9806 - val_loss: 0.3542 - val_acc: 0.9801 - 170ms/epoch - 170ms/step\n",
            "Epoch 133/200\n",
            "1/1 - 0s - loss: 0.3542 - acc: 0.9797 - val_loss: 0.3449 - val_acc: 0.9801 - 149ms/epoch - 149ms/step\n",
            "Epoch 134/200\n",
            "1/1 - 0s - loss: 0.3470 - acc: 0.9795 - val_loss: 0.3359 - val_acc: 0.9801 - 153ms/epoch - 153ms/step\n",
            "Epoch 135/200\n",
            "1/1 - 0s - loss: 0.3584 - acc: 0.9783 - val_loss: 0.3272 - val_acc: 0.9801 - 161ms/epoch - 161ms/step\n",
            "Epoch 136/200\n",
            "1/1 - 0s - loss: 0.3469 - acc: 0.9792 - val_loss: 0.3186 - val_acc: 0.9801 - 168ms/epoch - 168ms/step\n",
            "Epoch 137/200\n",
            "1/1 - 0s - loss: 0.3293 - acc: 0.9778 - val_loss: 0.3103 - val_acc: 0.9801 - 163ms/epoch - 163ms/step\n",
            "Epoch 138/200\n",
            "1/1 - 0s - loss: 0.3296 - acc: 0.9764 - val_loss: 0.3022 - val_acc: 0.9801 - 161ms/epoch - 161ms/step\n",
            "Epoch 139/200\n",
            "1/1 - 0s - loss: 0.3217 - acc: 0.9730 - val_loss: 0.2944 - val_acc: 0.9801 - 153ms/epoch - 153ms/step\n",
            "Epoch 140/200\n",
            "1/1 - 0s - loss: 0.3134 - acc: 0.9806 - val_loss: 0.2867 - val_acc: 0.9801 - 168ms/epoch - 168ms/step\n",
            "Epoch 141/200\n",
            "1/1 - 0s - loss: 0.3101 - acc: 0.9800 - val_loss: 0.2793 - val_acc: 0.9801 - 163ms/epoch - 163ms/step\n",
            "Epoch 142/200\n",
            "1/1 - 0s - loss: 0.2982 - acc: 0.9789 - val_loss: 0.2722 - val_acc: 0.9801 - 163ms/epoch - 163ms/step\n",
            "Epoch 143/200\n",
            "1/1 - 0s - loss: 0.2913 - acc: 0.9806 - val_loss: 0.2653 - val_acc: 0.9801 - 165ms/epoch - 165ms/step\n",
            "Epoch 144/200\n",
            "1/1 - 0s - loss: 0.2724 - acc: 0.9800 - val_loss: 0.2586 - val_acc: 0.9801 - 165ms/epoch - 165ms/step\n",
            "Epoch 145/200\n",
            "1/1 - 0s - loss: 0.2702 - acc: 0.9789 - val_loss: 0.2522 - val_acc: 0.9801 - 180ms/epoch - 180ms/step\n",
            "Epoch 146/200\n",
            "1/1 - 0s - loss: 0.2549 - acc: 0.9792 - val_loss: 0.2459 - val_acc: 0.9801 - 170ms/epoch - 170ms/step\n",
            "Epoch 147/200\n",
            "1/1 - 0s - loss: 0.2618 - acc: 0.9792 - val_loss: 0.2398 - val_acc: 0.9801 - 166ms/epoch - 166ms/step\n",
            "Epoch 148/200\n",
            "1/1 - 0s - loss: 0.2642 - acc: 0.9800 - val_loss: 0.2339 - val_acc: 0.9801 - 167ms/epoch - 167ms/step\n",
            "Epoch 149/200\n",
            "1/1 - 0s - loss: 0.2486 - acc: 0.9800 - val_loss: 0.2282 - val_acc: 0.9801 - 161ms/epoch - 161ms/step\n",
            "Epoch 150/200\n",
            "1/1 - 0s - loss: 0.2355 - acc: 0.9783 - val_loss: 0.2227 - val_acc: 0.9801 - 236ms/epoch - 236ms/step\n",
            "Epoch 151/200\n",
            "1/1 - 0s - loss: 0.2302 - acc: 0.9792 - val_loss: 0.2174 - val_acc: 0.9801 - 276ms/epoch - 276ms/step\n",
            "Epoch 152/200\n",
            "1/1 - 0s - loss: 0.2268 - acc: 0.9789 - val_loss: 0.2123 - val_acc: 0.9801 - 315ms/epoch - 315ms/step\n",
            "Epoch 153/200\n",
            "1/1 - 0s - loss: 0.2318 - acc: 0.9797 - val_loss: 0.2073 - val_acc: 0.9801 - 334ms/epoch - 334ms/step\n",
            "Epoch 154/200\n",
            "1/1 - 0s - loss: 0.2177 - acc: 0.9783 - val_loss: 0.2025 - val_acc: 0.9801 - 269ms/epoch - 269ms/step\n",
            "Epoch 155/200\n",
            "1/1 - 0s - loss: 0.2202 - acc: 0.9803 - val_loss: 0.1977 - val_acc: 0.9801 - 319ms/epoch - 319ms/step\n",
            "Epoch 156/200\n",
            "1/1 - 0s - loss: 0.2107 - acc: 0.9795 - val_loss: 0.1931 - val_acc: 0.9801 - 332ms/epoch - 332ms/step\n",
            "Epoch 157/200\n",
            "1/1 - 0s - loss: 0.2008 - acc: 0.9795 - val_loss: 0.1886 - val_acc: 0.9801 - 280ms/epoch - 280ms/step\n",
            "Epoch 158/200\n",
            "1/1 - 0s - loss: 0.2032 - acc: 0.9806 - val_loss: 0.1843 - val_acc: 0.9801 - 320ms/epoch - 320ms/step\n",
            "Epoch 159/200\n",
            "1/1 - 0s - loss: 0.2068 - acc: 0.9806 - val_loss: 0.1801 - val_acc: 0.9801 - 322ms/epoch - 322ms/step\n",
            "Epoch 160/200\n",
            "1/1 - 0s - loss: 0.1982 - acc: 0.9792 - val_loss: 0.1761 - val_acc: 0.9801 - 231ms/epoch - 231ms/step\n",
            "Epoch 161/200\n",
            "1/1 - 0s - loss: 0.1814 - acc: 0.9806 - val_loss: 0.1722 - val_acc: 0.9801 - 241ms/epoch - 241ms/step\n",
            "Epoch 162/200\n",
            "1/1 - 0s - loss: 0.1931 - acc: 0.9792 - val_loss: 0.1685 - val_acc: 0.9801 - 300ms/epoch - 300ms/step\n",
            "Epoch 163/200\n",
            "1/1 - 0s - loss: 0.1807 - acc: 0.9820 - val_loss: 0.1648 - val_acc: 0.9801 - 239ms/epoch - 239ms/step\n",
            "Epoch 164/200\n",
            "1/1 - 0s - loss: 0.1826 - acc: 0.9795 - val_loss: 0.1613 - val_acc: 0.9801 - 307ms/epoch - 307ms/step\n",
            "Epoch 165/200\n",
            "1/1 - 0s - loss: 0.1807 - acc: 0.9797 - val_loss: 0.1578 - val_acc: 0.9801 - 299ms/epoch - 299ms/step\n",
            "Epoch 166/200\n",
            "1/1 - 0s - loss: 0.1688 - acc: 0.9803 - val_loss: 0.1545 - val_acc: 0.9801 - 319ms/epoch - 319ms/step\n",
            "Epoch 167/200\n",
            "1/1 - 0s - loss: 0.1678 - acc: 0.9809 - val_loss: 0.1513 - val_acc: 0.9801 - 161ms/epoch - 161ms/step\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.5209 - acc: 0.9779\n",
            "\n",
            "Test Set Metrics:\n",
            "\tloss: 0.5209\n",
            "\tacc: 0.9779\n",
            "1/1 [==============================] - 0s 195ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GCN (local pooling) filters...\n",
            "Epoch 1/200\n",
            "1/1 - 2s - loss: 1.4510 - acc: 0.3954 - val_loss: 1.4701 - val_acc: 0.3256 - 2s/epoch - 2s/step\n",
            "Epoch 2/200\n",
            "1/1 - 0s - loss: 1.4502 - acc: 0.4011 - val_loss: 1.4467 - val_acc: 0.3289 - 172ms/epoch - 172ms/step\n",
            "Epoch 3/200\n",
            "1/1 - 0s - loss: 1.4168 - acc: 0.4346 - val_loss: 1.4234 - val_acc: 0.3455 - 168ms/epoch - 168ms/step\n",
            "Epoch 4/200\n",
            "1/1 - 0s - loss: 1.4066 - acc: 0.4230 - val_loss: 1.4003 - val_acc: 0.3455 - 166ms/epoch - 166ms/step\n",
            "Epoch 5/200\n",
            "1/1 - 0s - loss: 1.3803 - acc: 0.4430 - val_loss: 1.3772 - val_acc: 0.3621 - 171ms/epoch - 171ms/step\n",
            "Epoch 6/200\n",
            "1/1 - 0s - loss: 1.3669 - acc: 0.4861 - val_loss: 1.3539 - val_acc: 0.3987 - 152ms/epoch - 152ms/step\n",
            "Epoch 7/200\n",
            "1/1 - 0s - loss: 1.3310 - acc: 0.5277 - val_loss: 1.3308 - val_acc: 0.4817 - 156ms/epoch - 156ms/step\n",
            "Epoch 8/200\n",
            "1/1 - 0s - loss: 1.3158 - acc: 0.6473 - val_loss: 1.3078 - val_acc: 0.7375 - 159ms/epoch - 159ms/step\n",
            "Epoch 9/200\n",
            "1/1 - 0s - loss: 1.3041 - acc: 0.6364 - val_loss: 1.2849 - val_acc: 0.8804 - 165ms/epoch - 165ms/step\n",
            "Epoch 10/200\n",
            "1/1 - 0s - loss: 1.2685 - acc: 0.7391 - val_loss: 1.2622 - val_acc: 0.8804 - 161ms/epoch - 161ms/step\n",
            "Epoch 11/200\n",
            "1/1 - 0s - loss: 1.2440 - acc: 0.8134 - val_loss: 1.2396 - val_acc: 0.8804 - 152ms/epoch - 152ms/step\n",
            "Epoch 12/200\n",
            "1/1 - 0s - loss: 1.2258 - acc: 0.7650 - val_loss: 1.2172 - val_acc: 0.8870 - 153ms/epoch - 153ms/step\n",
            "Epoch 13/200\n",
            "1/1 - 0s - loss: 1.2133 - acc: 0.8489 - val_loss: 1.1950 - val_acc: 0.8870 - 164ms/epoch - 164ms/step\n",
            "Epoch 14/200\n",
            "1/1 - 0s - loss: 1.1897 - acc: 0.8415 - val_loss: 1.1730 - val_acc: 0.8870 - 169ms/epoch - 169ms/step\n",
            "Epoch 15/200\n",
            "1/1 - 0s - loss: 1.1624 - acc: 0.8610 - val_loss: 1.1513 - val_acc: 0.8870 - 163ms/epoch - 163ms/step\n",
            "Epoch 16/200\n",
            "1/1 - 0s - loss: 1.1457 - acc: 0.8697 - val_loss: 1.1296 - val_acc: 0.8870 - 163ms/epoch - 163ms/step\n",
            "Epoch 17/200\n",
            "1/1 - 0s - loss: 1.1092 - acc: 0.8646 - val_loss: 1.1082 - val_acc: 0.8870 - 157ms/epoch - 157ms/step\n",
            "Epoch 18/200\n",
            "1/1 - 0s - loss: 1.0969 - acc: 0.8612 - val_loss: 1.0868 - val_acc: 0.8870 - 154ms/epoch - 154ms/step\n",
            "Epoch 19/200\n",
            "1/1 - 0s - loss: 1.0799 - acc: 0.8694 - val_loss: 1.0655 - val_acc: 0.8870 - 171ms/epoch - 171ms/step\n",
            "Epoch 20/200\n",
            "1/1 - 0s - loss: 1.0571 - acc: 0.8646 - val_loss: 1.0442 - val_acc: 0.8804 - 159ms/epoch - 159ms/step\n",
            "Epoch 21/200\n",
            "1/1 - 0s - loss: 1.0534 - acc: 0.8705 - val_loss: 1.0233 - val_acc: 0.8804 - 161ms/epoch - 161ms/step\n",
            "Epoch 22/200\n",
            "1/1 - 0s - loss: 1.0206 - acc: 0.8714 - val_loss: 1.0025 - val_acc: 0.8804 - 294ms/epoch - 294ms/step\n",
            "Epoch 23/200\n",
            "1/1 - 0s - loss: 0.9849 - acc: 0.8643 - val_loss: 0.9820 - val_acc: 0.8771 - 313ms/epoch - 313ms/step\n",
            "Epoch 24/200\n",
            "1/1 - 0s - loss: 0.9663 - acc: 0.8688 - val_loss: 0.9617 - val_acc: 0.8738 - 323ms/epoch - 323ms/step\n",
            "Epoch 25/200\n",
            "1/1 - 0s - loss: 0.9692 - acc: 0.8750 - val_loss: 0.9415 - val_acc: 0.8738 - 311ms/epoch - 311ms/step\n",
            "Epoch 26/200\n",
            "1/1 - 0s - loss: 0.9398 - acc: 0.8731 - val_loss: 0.9217 - val_acc: 0.8738 - 315ms/epoch - 315ms/step\n",
            "Epoch 27/200\n",
            "1/1 - 0s - loss: 0.9227 - acc: 0.8736 - val_loss: 0.9020 - val_acc: 0.8738 - 280ms/epoch - 280ms/step\n",
            "Epoch 28/200\n",
            "1/1 - 0s - loss: 0.9045 - acc: 0.8739 - val_loss: 0.8825 - val_acc: 0.8738 - 319ms/epoch - 319ms/step\n",
            "Epoch 29/200\n",
            "1/1 - 0s - loss: 0.8872 - acc: 0.8854 - val_loss: 0.8634 - val_acc: 0.8738 - 312ms/epoch - 312ms/step\n",
            "Epoch 30/200\n",
            "1/1 - 0s - loss: 0.8688 - acc: 0.8711 - val_loss: 0.8445 - val_acc: 0.8738 - 339ms/epoch - 339ms/step\n",
            "Epoch 31/200\n",
            "1/1 - 0s - loss: 0.8333 - acc: 0.8731 - val_loss: 0.8258 - val_acc: 0.8738 - 226ms/epoch - 226ms/step\n",
            "Epoch 32/200\n",
            "1/1 - 0s - loss: 0.8307 - acc: 0.8739 - val_loss: 0.8075 - val_acc: 0.8738 - 308ms/epoch - 308ms/step\n",
            "Epoch 33/200\n",
            "1/1 - 0s - loss: 0.8050 - acc: 0.8742 - val_loss: 0.7894 - val_acc: 0.8738 - 251ms/epoch - 251ms/step\n",
            "Epoch 34/200\n",
            "1/1 - 0s - loss: 0.7909 - acc: 0.8739 - val_loss: 0.7716 - val_acc: 0.8738 - 231ms/epoch - 231ms/step\n",
            "Epoch 35/200\n",
            "1/1 - 0s - loss: 0.7710 - acc: 0.8849 - val_loss: 0.7539 - val_acc: 0.8738 - 295ms/epoch - 295ms/step\n",
            "Epoch 36/200\n",
            "1/1 - 0s - loss: 0.7642 - acc: 0.8711 - val_loss: 0.7365 - val_acc: 0.8738 - 299ms/epoch - 299ms/step\n",
            "Epoch 37/200\n",
            "1/1 - 0s - loss: 0.7411 - acc: 0.8764 - val_loss: 0.7195 - val_acc: 0.8738 - 300ms/epoch - 300ms/step\n",
            "Epoch 38/200\n",
            "1/1 - 0s - loss: 0.7124 - acc: 0.8736 - val_loss: 0.7023 - val_acc: 0.8738 - 310ms/epoch - 310ms/step\n",
            "Epoch 39/200\n",
            "1/1 - 0s - loss: 0.7060 - acc: 0.8742 - val_loss: 0.6856 - val_acc: 0.8738 - 208ms/epoch - 208ms/step\n",
            "Epoch 40/200\n",
            "1/1 - 0s - loss: 0.6764 - acc: 0.8863 - val_loss: 0.6691 - val_acc: 0.8738 - 165ms/epoch - 165ms/step\n",
            "Epoch 41/200\n",
            "1/1 - 0s - loss: 0.6746 - acc: 0.8753 - val_loss: 0.6529 - val_acc: 0.8738 - 151ms/epoch - 151ms/step\n",
            "Epoch 42/200\n",
            "1/1 - 0s - loss: 0.6631 - acc: 0.8731 - val_loss: 0.6370 - val_acc: 0.8738 - 170ms/epoch - 170ms/step\n",
            "Epoch 43/200\n",
            "1/1 - 0s - loss: 0.6350 - acc: 0.8745 - val_loss: 0.6213 - val_acc: 0.8738 - 157ms/epoch - 157ms/step\n",
            "Epoch 44/200\n",
            "1/1 - 0s - loss: 0.6203 - acc: 0.8722 - val_loss: 0.6060 - val_acc: 0.8738 - 161ms/epoch - 161ms/step\n",
            "Epoch 45/200\n",
            "1/1 - 0s - loss: 0.6159 - acc: 0.8739 - val_loss: 0.5910 - val_acc: 0.8738 - 162ms/epoch - 162ms/step\n",
            "Epoch 46/200\n",
            "1/1 - 0s - loss: 0.5953 - acc: 0.8739 - val_loss: 0.5762 - val_acc: 0.8738 - 251ms/epoch - 251ms/step\n",
            "Epoch 47/200\n",
            "1/1 - 0s - loss: 0.5797 - acc: 0.8731 - val_loss: 0.5616 - val_acc: 0.8738 - 167ms/epoch - 167ms/step\n",
            "Epoch 48/200\n",
            "1/1 - 0s - loss: 0.5572 - acc: 0.8725 - val_loss: 0.5475 - val_acc: 0.8738 - 166ms/epoch - 166ms/step\n",
            "Epoch 49/200\n",
            "1/1 - 0s - loss: 0.5488 - acc: 0.8733 - val_loss: 0.5338 - val_acc: 0.8738 - 152ms/epoch - 152ms/step\n",
            "Epoch 50/200\n",
            "1/1 - 0s - loss: 0.5467 - acc: 0.8731 - val_loss: 0.5203 - val_acc: 0.8738 - 156ms/epoch - 156ms/step\n",
            "Epoch 51/200\n",
            "1/1 - 0s - loss: 0.5249 - acc: 0.8725 - val_loss: 0.5073 - val_acc: 0.8738 - 161ms/epoch - 161ms/step\n",
            "Epoch 52/200\n",
            "1/1 - 0s - loss: 0.5037 - acc: 0.8854 - val_loss: 0.4946 - val_acc: 0.8738 - 163ms/epoch - 163ms/step\n",
            "Epoch 53/200\n",
            "1/1 - 0s - loss: 0.5073 - acc: 0.8748 - val_loss: 0.4823 - val_acc: 0.8738 - 151ms/epoch - 151ms/step\n",
            "Epoch 54/200\n",
            "1/1 - 0s - loss: 0.4960 - acc: 0.8725 - val_loss: 0.4703 - val_acc: 0.8738 - 154ms/epoch - 154ms/step\n",
            "Epoch 55/200\n",
            "1/1 - 0s - loss: 0.4779 - acc: 0.8728 - val_loss: 0.4586 - val_acc: 0.8738 - 165ms/epoch - 165ms/step\n",
            "Epoch 56/200\n",
            "1/1 - 0s - loss: 0.4655 - acc: 0.8731 - val_loss: 0.4474 - val_acc: 0.8738 - 152ms/epoch - 152ms/step\n",
            "Epoch 57/200\n",
            "1/1 - 0s - loss: 0.4550 - acc: 0.8728 - val_loss: 0.4365 - val_acc: 0.8738 - 150ms/epoch - 150ms/step\n",
            "Epoch 58/200\n",
            "1/1 - 0s - loss: 0.4491 - acc: 0.8762 - val_loss: 0.4259 - val_acc: 0.8738 - 168ms/epoch - 168ms/step\n",
            "Epoch 59/200\n",
            "1/1 - 0s - loss: 0.4399 - acc: 0.8869 - val_loss: 0.4158 - val_acc: 0.8738 - 165ms/epoch - 165ms/step\n",
            "Epoch 60/200\n",
            "1/1 - 0s - loss: 0.4319 - acc: 0.8750 - val_loss: 0.4059 - val_acc: 0.8738 - 151ms/epoch - 151ms/step\n",
            "Epoch 61/200\n",
            "1/1 - 0s - loss: 0.4044 - acc: 0.8739 - val_loss: 0.3965 - val_acc: 0.8738 - 171ms/epoch - 171ms/step\n",
            "Epoch 62/200\n",
            "1/1 - 0s - loss: 0.4059 - acc: 0.8756 - val_loss: 0.3874 - val_acc: 0.8771 - 166ms/epoch - 166ms/step\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 1.2151 - acc: 0.9033\n",
            "\n",
            "Test Set Metrics:\n",
            "\tloss: 1.2151\n",
            "\tacc: 0.9033\n",
            "1/1 [==============================] - 0s 185ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GCN (local pooling) filters...\n",
            "Epoch 1/200\n",
            "1/1 - 2s - loss: 1.9884 - acc: 0.0110 - val_loss: 2.0030 - val_acc: 0.0100 - 2s/epoch - 2s/step\n",
            "Epoch 2/200\n",
            "1/1 - 0s - loss: 1.9948 - acc: 0.0115 - val_loss: 1.9830 - val_acc: 0.0100 - 283ms/epoch - 283ms/step\n",
            "Epoch 3/200\n",
            "1/1 - 0s - loss: 1.9443 - acc: 0.0113 - val_loss: 1.9634 - val_acc: 0.0100 - 318ms/epoch - 318ms/step\n",
            "Epoch 4/200\n",
            "1/1 - 0s - loss: 1.9252 - acc: 0.0124 - val_loss: 1.9440 - val_acc: 0.0100 - 280ms/epoch - 280ms/step\n",
            "Epoch 5/200\n",
            "1/1 - 0s - loss: 1.9193 - acc: 0.0090 - val_loss: 1.9248 - val_acc: 0.0100 - 275ms/epoch - 275ms/step\n",
            "Epoch 6/200\n",
            "1/1 - 0s - loss: 1.9012 - acc: 0.0099 - val_loss: 1.9056 - val_acc: 0.0100 - 316ms/epoch - 316ms/step\n",
            "Epoch 7/200\n",
            "1/1 - 0s - loss: 1.8942 - acc: 0.0096 - val_loss: 1.8866 - val_acc: 0.0100 - 319ms/epoch - 319ms/step\n",
            "Epoch 8/200\n",
            "1/1 - 0s - loss: 1.8617 - acc: 0.0110 - val_loss: 1.8680 - val_acc: 0.0100 - 316ms/epoch - 316ms/step\n",
            "Epoch 9/200\n",
            "1/1 - 0s - loss: 1.8380 - acc: 0.0107 - val_loss: 1.8489 - val_acc: 0.0100 - 314ms/epoch - 314ms/step\n",
            "Epoch 10/200\n",
            "1/1 - 0s - loss: 1.8202 - acc: 0.0104 - val_loss: 1.8293 - val_acc: 0.0100 - 310ms/epoch - 310ms/step\n",
            "Epoch 11/200\n",
            "1/1 - 0s - loss: 1.8184 - acc: 0.0115 - val_loss: 1.8096 - val_acc: 0.0100 - 293ms/epoch - 293ms/step\n",
            "Epoch 12/200\n",
            "1/1 - 0s - loss: 1.7974 - acc: 0.0132 - val_loss: 1.7899 - val_acc: 0.0100 - 295ms/epoch - 295ms/step\n",
            "Epoch 13/200\n",
            "1/1 - 0s - loss: 1.7729 - acc: 0.0146 - val_loss: 1.7703 - val_acc: 0.0100 - 252ms/epoch - 252ms/step\n",
            "Epoch 14/200\n",
            "1/1 - 0s - loss: 1.7525 - acc: 0.0132 - val_loss: 1.7504 - val_acc: 0.0100 - 291ms/epoch - 291ms/step\n",
            "Epoch 15/200\n",
            "1/1 - 0s - loss: 1.7308 - acc: 0.0183 - val_loss: 1.7305 - val_acc: 0.0100 - 297ms/epoch - 297ms/step\n",
            "Epoch 16/200\n",
            "1/1 - 0s - loss: 1.7113 - acc: 0.0124 - val_loss: 1.7106 - val_acc: 0.0100 - 241ms/epoch - 241ms/step\n",
            "Epoch 17/200\n",
            "1/1 - 0s - loss: 1.6790 - acc: 0.0113 - val_loss: 1.6907 - val_acc: 0.0100 - 305ms/epoch - 305ms/step\n",
            "Epoch 18/200\n",
            "1/1 - 0s - loss: 1.6704 - acc: 0.0172 - val_loss: 1.6711 - val_acc: 0.0100 - 206ms/epoch - 206ms/step\n",
            "Epoch 19/200\n",
            "1/1 - 0s - loss: 1.6632 - acc: 0.0135 - val_loss: 1.6516 - val_acc: 0.0100 - 156ms/epoch - 156ms/step\n",
            "Epoch 20/200\n",
            "1/1 - 0s - loss: 1.6397 - acc: 0.0175 - val_loss: 1.6321 - val_acc: 0.0100 - 158ms/epoch - 158ms/step\n",
            "Epoch 21/200\n",
            "1/1 - 0s - loss: 1.6191 - acc: 0.0242 - val_loss: 1.6126 - val_acc: 0.0100 - 163ms/epoch - 163ms/step\n",
            "Epoch 22/200\n",
            "1/1 - 0s - loss: 1.6057 - acc: 0.0197 - val_loss: 1.5932 - val_acc: 0.0100 - 154ms/epoch - 154ms/step\n",
            "Epoch 23/200\n",
            "1/1 - 0s - loss: 1.5869 - acc: 0.0256 - val_loss: 1.5738 - val_acc: 0.0100 - 164ms/epoch - 164ms/step\n",
            "Epoch 24/200\n",
            "1/1 - 0s - loss: 1.5712 - acc: 0.0954 - val_loss: 1.5548 - val_acc: 0.0831 - 159ms/epoch - 159ms/step\n",
            "Epoch 25/200\n",
            "1/1 - 0s - loss: 1.5495 - acc: 0.1013 - val_loss: 1.5361 - val_acc: 0.1063 - 156ms/epoch - 156ms/step\n",
            "Epoch 26/200\n",
            "1/1 - 0s - loss: 1.5305 - acc: 0.1489 - val_loss: 1.5185 - val_acc: 0.1130 - 155ms/epoch - 155ms/step\n",
            "Epoch 27/200\n",
            "1/1 - 0s - loss: 1.5137 - acc: 0.1618 - val_loss: 1.5013 - val_acc: 0.1130 - 158ms/epoch - 158ms/step\n",
            "Epoch 28/200\n",
            "1/1 - 0s - loss: 1.5013 - acc: 0.1787 - val_loss: 1.4843 - val_acc: 0.1130 - 170ms/epoch - 170ms/step\n",
            "Epoch 29/200\n",
            "1/1 - 0s - loss: 1.4962 - acc: 0.2443 - val_loss: 1.4675 - val_acc: 0.1262 - 158ms/epoch - 158ms/step\n",
            "Epoch 30/200\n",
            "1/1 - 0s - loss: 1.4741 - acc: 0.2722 - val_loss: 1.4509 - val_acc: 0.6877 - 159ms/epoch - 159ms/step\n",
            "Epoch 31/200\n",
            "1/1 - 0s - loss: 1.4613 - acc: 0.4835 - val_loss: 1.4342 - val_acc: 0.7043 - 164ms/epoch - 164ms/step\n",
            "Epoch 32/200\n",
            "1/1 - 0s - loss: 1.4321 - acc: 0.6082 - val_loss: 1.4176 - val_acc: 0.7641 - 180ms/epoch - 180ms/step\n",
            "Epoch 33/200\n",
            "1/1 - 0s - loss: 1.4184 - acc: 0.7183 - val_loss: 1.4021 - val_acc: 0.8505 - 167ms/epoch - 167ms/step\n",
            "Epoch 34/200\n",
            "1/1 - 0s - loss: 1.4028 - acc: 0.7864 - val_loss: 1.3876 - val_acc: 0.8837 - 160ms/epoch - 160ms/step\n",
            "Epoch 35/200\n",
            "1/1 - 0s - loss: 1.3921 - acc: 0.8306 - val_loss: 1.3730 - val_acc: 0.8837 - 150ms/epoch - 150ms/step\n",
            "Epoch 36/200\n",
            "1/1 - 0s - loss: 1.3774 - acc: 0.8244 - val_loss: 1.3580 - val_acc: 0.8837 - 160ms/epoch - 160ms/step\n",
            "Epoch 37/200\n",
            "1/1 - 0s - loss: 1.3630 - acc: 0.8303 - val_loss: 1.3424 - val_acc: 0.8837 - 155ms/epoch - 155ms/step\n",
            "Epoch 38/200\n",
            "1/1 - 0s - loss: 1.3424 - acc: 0.8604 - val_loss: 1.3265 - val_acc: 0.8837 - 152ms/epoch - 152ms/step\n",
            "Epoch 39/200\n",
            "1/1 - 0s - loss: 1.3301 - acc: 0.8646 - val_loss: 1.3104 - val_acc: 0.8837 - 166ms/epoch - 166ms/step\n",
            "Epoch 40/200\n",
            "1/1 - 0s - loss: 1.3127 - acc: 0.8646 - val_loss: 1.2940 - val_acc: 0.8837 - 165ms/epoch - 165ms/step\n",
            "Epoch 41/200\n",
            "1/1 - 0s - loss: 1.2851 - acc: 0.8666 - val_loss: 1.2775 - val_acc: 0.8837 - 150ms/epoch - 150ms/step\n",
            "Epoch 42/200\n",
            "1/1 - 0s - loss: 1.2764 - acc: 0.8733 - val_loss: 1.2609 - val_acc: 0.8837 - 166ms/epoch - 166ms/step\n",
            "Epoch 43/200\n",
            "1/1 - 0s - loss: 1.2606 - acc: 0.8745 - val_loss: 1.2441 - val_acc: 0.8837 - 153ms/epoch - 153ms/step\n",
            "Epoch 44/200\n",
            "1/1 - 0s - loss: 1.2365 - acc: 0.8742 - val_loss: 1.2273 - val_acc: 0.8837 - 159ms/epoch - 159ms/step\n",
            "Epoch 45/200\n",
            "1/1 - 0s - loss: 1.2189 - acc: 0.8731 - val_loss: 1.2103 - val_acc: 0.8837 - 170ms/epoch - 170ms/step\n",
            "Epoch 46/200\n",
            "1/1 - 0s - loss: 1.2103 - acc: 0.8748 - val_loss: 1.1927 - val_acc: 0.8738 - 159ms/epoch - 159ms/step\n",
            "Epoch 47/200\n",
            "1/1 - 0s - loss: 1.1954 - acc: 0.8711 - val_loss: 1.1750 - val_acc: 0.8738 - 149ms/epoch - 149ms/step\n",
            "Epoch 48/200\n",
            "1/1 - 0s - loss: 1.1698 - acc: 0.8767 - val_loss: 1.1569 - val_acc: 0.8738 - 156ms/epoch - 156ms/step\n",
            "Epoch 49/200\n",
            "1/1 - 0s - loss: 1.1604 - acc: 0.8762 - val_loss: 1.1386 - val_acc: 0.8738 - 153ms/epoch - 153ms/step\n",
            "Epoch 50/200\n",
            "1/1 - 0s - loss: 1.1374 - acc: 0.8725 - val_loss: 1.1203 - val_acc: 0.8738 - 151ms/epoch - 151ms/step\n",
            "Epoch 51/200\n",
            "1/1 - 0s - loss: 1.1287 - acc: 0.8725 - val_loss: 1.1018 - val_acc: 0.8738 - 171ms/epoch - 171ms/step\n",
            "Epoch 52/200\n",
            "1/1 - 0s - loss: 1.0978 - acc: 0.8753 - val_loss: 1.0830 - val_acc: 0.8738 - 160ms/epoch - 160ms/step\n",
            "Epoch 53/200\n",
            "1/1 - 0s - loss: 1.0740 - acc: 0.8742 - val_loss: 1.0641 - val_acc: 0.8738 - 159ms/epoch - 159ms/step\n",
            "Epoch 54/200\n",
            "1/1 - 0s - loss: 1.0684 - acc: 0.8739 - val_loss: 1.0451 - val_acc: 0.8738 - 153ms/epoch - 153ms/step\n",
            "Epoch 55/200\n",
            "1/1 - 0s - loss: 1.0548 - acc: 0.8759 - val_loss: 1.0261 - val_acc: 0.8738 - 161ms/epoch - 161ms/step\n",
            "Epoch 56/200\n",
            "1/1 - 0s - loss: 1.0238 - acc: 0.8778 - val_loss: 1.0072 - val_acc: 0.8738 - 156ms/epoch - 156ms/step\n",
            "Epoch 57/200\n",
            "1/1 - 0s - loss: 1.0134 - acc: 0.8750 - val_loss: 0.9884 - val_acc: 0.8738 - 152ms/epoch - 152ms/step\n",
            "Epoch 58/200\n",
            "1/1 - 0s - loss: 0.9879 - acc: 0.8762 - val_loss: 0.9695 - val_acc: 0.8738 - 167ms/epoch - 167ms/step\n",
            "Epoch 59/200\n",
            "1/1 - 0s - loss: 0.9810 - acc: 0.8759 - val_loss: 0.9506 - val_acc: 0.8738 - 165ms/epoch - 165ms/step\n",
            "Epoch 60/200\n",
            "1/1 - 0s - loss: 0.9575 - acc: 0.8756 - val_loss: 0.9319 - val_acc: 0.8738 - 233ms/epoch - 233ms/step\n",
            "Epoch 61/200\n",
            "1/1 - 0s - loss: 0.9299 - acc: 0.8770 - val_loss: 0.9135 - val_acc: 0.8738 - 278ms/epoch - 278ms/step\n",
            "Epoch 62/200\n",
            "1/1 - 0s - loss: 0.9304 - acc: 0.8759 - val_loss: 0.8953 - val_acc: 0.8738 - 274ms/epoch - 274ms/step\n",
            "Epoch 63/200\n",
            "1/1 - 0s - loss: 0.8984 - acc: 0.8756 - val_loss: 0.8773 - val_acc: 0.8738 - 308ms/epoch - 308ms/step\n",
            "Epoch 64/200\n",
            "1/1 - 0s - loss: 0.8781 - acc: 0.8750 - val_loss: 0.8594 - val_acc: 0.8738 - 322ms/epoch - 322ms/step\n",
            "Epoch 65/200\n",
            "1/1 - 0s - loss: 0.8633 - acc: 0.8745 - val_loss: 0.8416 - val_acc: 0.8738 - 284ms/epoch - 284ms/step\n",
            "Epoch 66/200\n",
            "1/1 - 0s - loss: 0.8488 - acc: 0.8762 - val_loss: 0.8239 - val_acc: 0.8738 - 294ms/epoch - 294ms/step\n",
            "Epoch 67/200\n",
            "1/1 - 0s - loss: 0.8367 - acc: 0.8753 - val_loss: 0.8064 - val_acc: 0.8738 - 325ms/epoch - 325ms/step\n",
            "Epoch 68/200\n",
            "1/1 - 0s - loss: 0.8211 - acc: 0.8733 - val_loss: 0.7890 - val_acc: 0.8738 - 319ms/epoch - 319ms/step\n",
            "Epoch 69/200\n",
            "1/1 - 0s - loss: 0.7900 - acc: 0.8745 - val_loss: 0.7715 - val_acc: 0.8738 - 291ms/epoch - 291ms/step\n",
            "Epoch 70/200\n",
            "1/1 - 0s - loss: 0.7845 - acc: 0.8736 - val_loss: 0.7541 - val_acc: 0.8738 - 303ms/epoch - 303ms/step\n",
            "Epoch 71/200\n",
            "1/1 - 0s - loss: 0.7590 - acc: 0.8776 - val_loss: 0.7372 - val_acc: 0.8738 - 295ms/epoch - 295ms/step\n",
            "Epoch 72/200\n",
            "1/1 - 0s - loss: 0.7461 - acc: 0.8733 - val_loss: 0.7204 - val_acc: 0.8738 - 291ms/epoch - 291ms/step\n",
            "Epoch 73/200\n",
            "1/1 - 0s - loss: 0.7255 - acc: 0.8745 - val_loss: 0.7039 - val_acc: 0.8738 - 302ms/epoch - 302ms/step\n",
            "Epoch 74/200\n",
            "1/1 - 0s - loss: 0.7044 - acc: 0.8745 - val_loss: 0.6876 - val_acc: 0.8738 - 164ms/epoch - 164ms/step\n",
            "Epoch 75/200\n",
            "1/1 - 0s - loss: 0.7108 - acc: 0.8728 - val_loss: 0.6715 - val_acc: 0.8738 - 167ms/epoch - 167ms/step\n",
            "Epoch 76/200\n",
            "1/1 - 0s - loss: 0.6810 - acc: 0.8745 - val_loss: 0.6557 - val_acc: 0.8738 - 163ms/epoch - 163ms/step\n",
            "Epoch 77/200\n",
            "1/1 - 0s - loss: 0.6728 - acc: 0.8742 - val_loss: 0.6402 - val_acc: 0.8738 - 152ms/epoch - 152ms/step\n",
            "Epoch 78/200\n",
            "1/1 - 0s - loss: 0.6571 - acc: 0.8750 - val_loss: 0.6250 - val_acc: 0.8738 - 164ms/epoch - 164ms/step\n",
            "Epoch 79/200\n",
            "1/1 - 0s - loss: 0.6499 - acc: 0.8742 - val_loss: 0.6100 - val_acc: 0.8738 - 155ms/epoch - 155ms/step\n",
            "Epoch 80/200\n",
            "1/1 - 0s - loss: 0.6225 - acc: 0.8748 - val_loss: 0.5951 - val_acc: 0.8738 - 165ms/epoch - 165ms/step\n",
            "Epoch 81/200\n",
            "1/1 - 0s - loss: 0.6245 - acc: 0.8764 - val_loss: 0.5800 - val_acc: 0.8738 - 166ms/epoch - 166ms/step\n",
            "Epoch 82/200\n",
            "1/1 - 0s - loss: 0.5835 - acc: 0.8742 - val_loss: 0.5652 - val_acc: 0.8738 - 153ms/epoch - 153ms/step\n",
            "Epoch 83/200\n",
            "1/1 - 0s - loss: 0.5821 - acc: 0.8736 - val_loss: 0.5508 - val_acc: 0.8804 - 165ms/epoch - 165ms/step\n",
            "Epoch 84/200\n",
            "1/1 - 0s - loss: 0.5603 - acc: 0.8739 - val_loss: 0.5366 - val_acc: 0.8804 - 159ms/epoch - 159ms/step\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 1.3913 - acc: 0.8812\n",
            "\n",
            "Test Set Metrics:\n",
            "\tloss: 1.3913\n",
            "\tacc: 0.8812\n",
            "1/1 [==============================] - 0s 212ms/step\n"
          ]
        }
      ],
      "source": [
        "well_list = [\"16a\",\"4\",\"5a\"]\n",
        "well_name = ''\n",
        "for j in range(len(well_list)):\n",
        "  well_name = well_list[j]\n",
        "  print(well_name)\n",
        "  if well_name != '':\n",
        "    for i in range(5, 11):\n",
        "      cls_name = i\n",
        "      for k in range(1, 5):\n",
        "        time = k\n",
        "        # the base_directory property tells us where it was downloaded to:\n",
        "        cora_cites_file = os.path.join(\"/content/drive/MyDrive/NewZealand_data/mckee-\"+str(well_name)+\"/\"+str(cls_name)+\"/\", \"mckee-\"+str(well_name)+\" BF node for \"+str(cls_name)+\".csv\")\n",
        "        cora_content_file = os.path.join(\"/content/drive/MyDrive/NewZealand_data/mckee-\"+str(well_name)+\"/\"+str(cls_name)+\"/\", \"mckee-\"+str(well_name)+\" BF_full_edge \"+str(cls_name)+\".txt\")\n",
        "\n",
        "        cora_cites_cont = pd.read_csv(\n",
        "            cora_cites_file)\n",
        "\n",
        "        cora_cites = pd.DataFrame()\n",
        "        #cora_cites['target'] = cora_cites['target'].str.replace('.', '')\n",
        "        cora_cites['target'] = cora_cites_cont['full'].map(lambda x: str(x).replace('.', ''))\n",
        "        cora_cites['source'] = cora_cites_cont['full1'].map(lambda x: str(x).replace('.', ''))\n",
        "\n",
        "        cora_feature_names = [f\"w{i}\" for i in range(14)]\n",
        "\n",
        "        cora_raw_content = pd.read_csv(\n",
        "            cora_content_file,\n",
        "            sep=\"\\t\",  # tab-separated\n",
        "            header=None,  # no heading row\n",
        "            names=[\"DEPTH\", *cora_feature_names, \"RESULTS\"],  # set our own names for the columns\n",
        "        )\n",
        "        cora_raw_content['DEPTH'] = cora_raw_content['DEPTH'].map(lambda x: str(x).replace('.', ''))\n",
        "\n",
        "        cora_content_str_subject = cora_raw_content.set_index(\"DEPTH\")\n",
        "\n",
        "        cora_content_no_subject = cora_content_str_subject.drop(columns=\"RESULTS\")\n",
        "        G = StellarGraph({\"paper\": cora_content_no_subject}, {\"cites\": cora_cites})\n",
        "\n",
        "        node_subjects = cora_content_str_subject[\"RESULTS\"]\n",
        "\n",
        "        trainS = ''\n",
        "        trainT = ''\n",
        "\n",
        "        if well_name == \"16a\":\n",
        "          trainS = 320\n",
        "          trainT =143\n",
        "        elif well_name == \"4\":\n",
        "          trainS = 4506\n",
        "          trainT =520\n",
        "        elif well_name == \"5a\":\n",
        "          trainS = 3553\n",
        "          trainT =301\n",
        "\n",
        "        train_subjects, test_subjects = model_selection.train_test_split(\n",
        "            node_subjects, train_size=trainS, test_size=None, stratify=node_subjects\n",
        "        )\n",
        "        val_subjects, test_subjects = model_selection.train_test_split(\n",
        "            test_subjects, train_size=trainT, test_size=None, stratify=test_subjects\n",
        "        )\n",
        "\n",
        "        target_encoding = preprocessing.LabelBinarizer()\n",
        "\n",
        "        train_targets = target_encoding.fit_transform(train_subjects)\n",
        "        val_targets = target_encoding.transform(val_subjects)\n",
        "        test_targets = target_encoding.transform(test_subjects)\n",
        "\n",
        "        generator = FullBatchNodeGenerator(G, method=\"gcn\")\n",
        "\n",
        "        train_gen = generator.flow(train_subjects.index, train_targets)\n",
        "\n",
        "        gcn = GCN(\n",
        "            layer_sizes=[16, 16], activations=[\"relu\", \"relu\"], generator=generator, dropout=0.5\n",
        "        )\n",
        "\n",
        "        x_inp, x_out = gcn.in_out_tensors()\n",
        "\n",
        "        predictions = layers.Dense(units=train_targets.shape[1], activation=\"softmax\")(x_out)\n",
        "\n",
        "        model = Model(inputs=x_inp, outputs=predictions)\n",
        "        model.compile(\n",
        "            optimizer=optimizers.Adam(lr=0.01),\n",
        "            loss=losses.categorical_crossentropy,\n",
        "            metrics=[\"acc\"],\n",
        "        )\n",
        "\n",
        "        from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "        es_callback = EarlyStopping(monitor=\"val_acc\", patience=50, restore_best_weights=True)\n",
        "\n",
        "        val_gen = generator.flow(val_subjects.index, val_targets)\n",
        "\n",
        "        history = model.fit(\n",
        "            train_gen,\n",
        "            epochs=200,\n",
        "            validation_data=val_gen,\n",
        "            verbose=2,\n",
        "            shuffle=False,  # this should be False, since shuffling data means shuffling the whole graph\n",
        "            callbacks=[es_callback],\n",
        "        )\n",
        "\n",
        "        hist_df = pd.DataFrame(history.history)\n",
        "\n",
        "        if time == 1:\n",
        "          hist_df.to_excel(\"/content/drive/MyDrive/NewZealand_data/mckee-\"+str(well_name)+\"/\"+str(cls_name)+\"/results/History_\"+str(cls_name)+\".xlsx\")\n",
        "        else:\n",
        "          hist_df.to_excel(\"/content/drive/MyDrive/NewZealand_data/mckee-\"+str(well_name)+\"/\"+str(cls_name)+\"/results/History_\"+str(cls_name)+\"new\"+str(time)+\".xlsx\")\n",
        "\n",
        "        test_gen = generator.flow(test_subjects.index, test_targets)\n",
        "\n",
        "        test_metrics = model.evaluate(test_gen)\n",
        "        print(\"\\nTest Set Metrics:\")\n",
        "        for name, val in zip(model.metrics_names, test_metrics):\n",
        "            print(\"\\t{}: {:0.4f}\".format(name, val))\n",
        "\n",
        "        all_nodes = node_subjects.index\n",
        "        all_gen = generator.flow(all_nodes)\n",
        "        all_predictions = model.predict(all_gen)\n",
        "\n",
        "        node_predictions = target_encoding.inverse_transform(all_predictions.squeeze())\n",
        "\n",
        "        df = pd.DataFrame({\"Predicted\": node_predictions, \"True\": node_subjects})\n",
        "\n",
        "        if time == 1:\n",
        "          df.to_excel(\"/content/drive/MyDrive/NewZealand_data/mckee-\"+str(well_name)+\"/\"+str(cls_name)+\"/results/Results_\"+str(cls_name)+\".xlsx\")\n",
        "        else:\n",
        "          df.to_excel(\"/content/drive/MyDrive/NewZealand_data/mckee-\"+str(well_name)+\"/\"+str(cls_name)+\"/results/Results_\"+str(cls_name)+\" new\"+str(time)+\".xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYHNxMjxeDgu"
      },
      "outputs": [],
      "source": [
        "cls_name = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXH1-Fe351Na"
      },
      "outputs": [],
      "source": [
        "well_name = \"5a\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNVSSiwt1Haz"
      },
      "outputs": [],
      "source": [
        "time = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aL0j1hPIeMSl"
      },
      "outputs": [],
      "source": [
        "# the base_directory property tells us where it was downloaded to:\n",
        "cora_cites_file = os.path.join(\"/content/drive/MyDrive/NewZealand_data/mckee-\"+str(well_name)+\"/\"+str(cls_name)+\"/\", \"mckee-\"+str(well_name)+\" BF node for \"+str(cls_name)+\".csv\")\n",
        "cora_content_file = os.path.join(\"/content/drive/MyDrive/NewZealand_data/mckee-\"+str(well_name)+\"/\"+str(cls_name)+\"/\", \"mckee-\"+str(well_name)+\" BF_full_edge \"+str(cls_name)+\".txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPhab3zWA2-D"
      },
      "outputs": [],
      "source": [
        "cora_cites = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1wcgtksC02H"
      },
      "outputs": [],
      "source": [
        "cora_cites_cont = pd.read_csv(\n",
        "    cora_cites_file)\n",
        "\n",
        "\n",
        "#cora_cites['target'] = cora_cites['target'].str.replace('.', '')\n",
        "cora_cites['target'] = cora_cites_cont['full'].map(lambda x: str(x).replace('.', ''))\n",
        "cora_cites['source'] = cora_cites_cont['full1'].map(lambda x: str(x).replace('.', ''))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "7RO9WhL5V2fL",
        "outputId": "7d30ddaa-a5a4-41c8-a101-fc532a274f91"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-fc3b491a-0044-4fff-a0a3-60a71f9abb58\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>170307</td>\n",
              "      <td>17032224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>170307</td>\n",
              "      <td>17033748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>170307</td>\n",
              "      <td>17035272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>170307</td>\n",
              "      <td>17036796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>170307</td>\n",
              "      <td>1703832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3441565</th>\n",
              "      <td>239649</td>\n",
              "      <td>2395728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3441566</th>\n",
              "      <td>239649</td>\n",
              "      <td>23958804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3441567</th>\n",
              "      <td>239649</td>\n",
              "      <td>23960328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3441568</th>\n",
              "      <td>239649</td>\n",
              "      <td>23961852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3441569</th>\n",
              "      <td>239649</td>\n",
              "      <td>23963376</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3441570 rows  2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fc3b491a-0044-4fff-a0a3-60a71f9abb58')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fc3b491a-0044-4fff-a0a3-60a71f9abb58 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fc3b491a-0044-4fff-a0a3-60a71f9abb58');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         target    source\n",
              "0        170307  17032224\n",
              "1        170307  17033748\n",
              "2        170307  17035272\n",
              "3        170307  17036796\n",
              "4        170307   1703832\n",
              "...         ...       ...\n",
              "3441565  239649   2395728\n",
              "3441566  239649  23958804\n",
              "3441567  239649  23960328\n",
              "3441568  239649  23961852\n",
              "3441569  239649  23963376\n",
              "\n",
              "[3441570 rows x 2 columns]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cora_cites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cy3VmUygRdDV"
      },
      "outputs": [],
      "source": [
        "cora_feature_names = [f\"w{i}\" for i in range(14)]\n",
        "\n",
        "cora_raw_content = pd.read_csv(\n",
        "    cora_content_file,\n",
        "    sep=\"\\t\",  # tab-separated\n",
        "    header=None,  # no heading row\n",
        "    names=[\"DEPTH\", *cora_feature_names, \"RESULTS\"],  # set our own names for the columns\n",
        ")\n",
        "cora_raw_content['DEPTH'] = cora_raw_content['DEPTH'].map(lambda x: str(x).replace('.', ''))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9Chc8OXSv5S"
      },
      "outputs": [],
      "source": [
        "cora_content_str_subject = cora_raw_content.set_index(\"DEPTH\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8xZEIlaS3In"
      },
      "outputs": [],
      "source": [
        "cora_content_no_subject = cora_content_str_subject.drop(columns=\"RESULTS\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wby9enqJS9RK",
        "outputId": "96948c41-eafd-4320-a0c2-ab06bb42dee9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "StellarGraph: Undirected multigraph\n",
            " Nodes: 4216, Edges: 3441570\n",
            "\n",
            " Node types:\n",
            "  paper: [4216]\n",
            "    Features: float32 vector, length 14\n",
            "    Edge types: paper-cites->paper\n",
            "\n",
            " Edge types:\n",
            "    paper-cites->paper: [3441570]\n",
            "        Weights: all 1 (default)\n",
            "        Features: none\n"
          ]
        }
      ],
      "source": [
        "G = StellarGraph({\"paper\": cora_content_no_subject}, {\"cites\": cora_cites})\n",
        "print(G.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIPcXOHZTsyV"
      },
      "outputs": [],
      "source": [
        "node_subjects = cora_content_str_subject[\"RESULTS\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghRV6PKBfiCf",
        "outputId": "6269dc82-1215-4b48-eb9f-76fe96ee4fa9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DEPTH\n",
              "170307      Very_Low\n",
              "17032224    Very_Low\n",
              "17033748    Very_Low\n",
              "17035272    Very_Low\n",
              "17036796    Very_Low\n",
              "              ...   \n",
              "23958804    Very_Low\n",
              "23960328    Very_Low\n",
              "23961852    Very_Low\n",
              "23963376    Very_Low\n",
              "239649      Very_Low\n",
              "Name: RESULTS, Length: 4216, dtype: object"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "node_subjects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "zXcYcUBW0Hqm",
        "outputId": "120c0347-e47a-48cd-d1b8-d11464a8d366"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-45c52ec7-8eb6-4722-9a38-16320442bd23\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RESULTS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Very_Low</th>\n",
              "      <td>3679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>High</th>\n",
              "      <td>464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Very_High</th>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Moderate</th>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Low</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-45c52ec7-8eb6-4722-9a38-16320442bd23')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-45c52ec7-8eb6-4722-9a38-16320442bd23 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-45c52ec7-8eb6-4722-9a38-16320442bd23');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           RESULTS\n",
              "Very_Low      3679\n",
              "High           464\n",
              "Very_High       41\n",
              "Moderate        29\n",
              "Low              3"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "node_subjects.value_counts().to_frame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "jFDkWQwv0KQJ",
        "outputId": "d221f34d-69fa-4883-c4bd-587f2f5575f0"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-2741394056d2>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mnode_subjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_subjects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m )\n\u001b[0;32m---> 17\u001b[0;31m val_subjects, test_subjects = model_selection.train_test_split(\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mtest_subjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_subjects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2581\u001b[0m         \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCVClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2583\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstratify\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2585\u001b[0m     return list(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   1687\u001b[0m         \"\"\"\n\u001b[1;32m   1688\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1689\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1690\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_iter_indices\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   2076\u001b[0m         \u001b[0mclass_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2077\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_counts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2078\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   2079\u001b[0m                 \u001b[0;34m\"The least populated class in y has only 1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2080\u001b[0m                 \u001b[0;34m\" member, which is too few. The minimum\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."
          ]
        }
      ],
      "source": [
        "trainS = ''\n",
        "trainT = ''\n",
        "\n",
        "if well_name == \"16a\":\n",
        "  trainS = 320\n",
        "  trainT =143\n",
        "elif well_name == \"4\":\n",
        "  trainS = 4506\n",
        "  trainT =520\n",
        "elif well_name == \"5a\":\n",
        "  trainS = 3553\n",
        "  trainT =301\n",
        "\n",
        "train_subjects, test_subjects = model_selection.train_test_split(\n",
        "    node_subjects, train_size=trainS, test_size=None, stratify=node_subjects\n",
        ")\n",
        "val_subjects, test_subjects = model_selection.train_test_split(\n",
        "    test_subjects, train_size=trainT, test_size=None, stratify=test_subjects\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Be3dMipz0NnL",
        "outputId": "86de3ea7-0578-49da-e38e-2794c79ac635"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3fc4fa9e-7a49-4414-9b66-8b8cac24eda0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RESULTS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>High</th>\n",
              "      <td>101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Very_Low</th>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Moderate</th>\n",
              "      <td>69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Low</th>\n",
              "      <td>51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Very_High</th>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3fc4fa9e-7a49-4414-9b66-8b8cac24eda0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3fc4fa9e-7a49-4414-9b66-8b8cac24eda0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3fc4fa9e-7a49-4414-9b66-8b8cac24eda0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           RESULTS\n",
              "High           101\n",
              "Very_Low        82\n",
              "Moderate        69\n",
              "Low             51\n",
              "Very_High       17"
            ]
          },
          "execution_count": 930,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_subjects.value_counts().to_frame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mg6h6TY20Qug"
      },
      "outputs": [],
      "source": [
        "target_encoding = preprocessing.LabelBinarizer()\n",
        "\n",
        "train_targets = target_encoding.fit_transform(train_subjects)\n",
        "val_targets = target_encoding.transform(val_subjects)\n",
        "test_targets = target_encoding.transform(test_subjects)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xi7coR8xgwsj",
        "outputId": "131385d2-84e5-4be4-991f-60e40b13047a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(320, 5)"
            ]
          },
          "execution_count": 932,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_targets.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYRZx-5H0TQM",
        "outputId": "8f052fb0-cbe1-42ad-d292-8682fe762b8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GCN (local pooling) filters...\n"
          ]
        }
      ],
      "source": [
        "generator = FullBatchNodeGenerator(G, method=\"gcn\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEmI3iKnikna",
        "outputId": "725a7733-33b1-448c-a166-84cbd36b78f6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(320, 5)"
            ]
          },
          "execution_count": 934,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_targets.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LG5A8Vgphs5X",
        "outputId": "11fcc278-5d0f-4099-92be-e6b3706c003e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(124, 5)"
            ]
          },
          "execution_count": 935,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_targets.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKWcAJAIhwLi",
        "outputId": "47d9b377-0998-45fd-e334-9edb95bde032"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(143, 5)"
            ]
          },
          "execution_count": 936,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_targets.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pduTMjJiib0h",
        "outputId": "ba505914-0af8-47aa-e1fe-99a6908a0f71"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['23402544', '232791', '22960584', '22835616', '22904196', '23396448',\n",
              "       '233553', '22995636', '23391876', '2321052',\n",
              "       ...\n",
              "       '23525988', '22735032', '23440644', '22992588', '22750272', '23321772',\n",
              "       '23565612', '22719792', '23266908', '23163276'],\n",
              "      dtype='object', name='DEPTH', length=320)"
            ]
          },
          "execution_count": 937,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_subjects.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQIwUxDW0V8e"
      },
      "outputs": [],
      "source": [
        "train_gen = generator.flow(train_subjects.index, train_targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzC_4HBq0YhR"
      },
      "outputs": [],
      "source": [
        "gcn = GCN(\n",
        "    layer_sizes=[16, 16], activations=[\"relu\", \"relu\"], generator=generator, dropout=0.5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkK2WJmf0bzj",
        "outputId": "a857373e-dd98-48b0-c6fe-8918b52391b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(1, None, 16) dtype=float32 (created by layer 'gather_indices_20')>"
            ]
          },
          "execution_count": 940,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_inp, x_out = gcn.in_out_tensors()\n",
        "\n",
        "x_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xen5kVLYhqPA",
        "outputId": "10a953b7-3871-44ad-c8fd-aff85e9d0fad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<KerasTensor: shape=(1, 587, 14) dtype=float32 (created by layer 'input_81')>,\n",
              " <KerasTensor: shape=(1, None) dtype=int32 (created by layer 'input_82')>,\n",
              " <KerasTensor: shape=(1, None, 2) dtype=int64 (created by layer 'input_83')>,\n",
              " <KerasTensor: shape=(1, None) dtype=float32 (created by layer 'input_84')>]"
            ]
          },
          "execution_count": 941,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_inp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvJQhysS0dwI"
      },
      "outputs": [],
      "source": [
        "predictions = layers.Dense(units=train_targets.shape[1], activation=\"softmax\")(x_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDL8OpRCvcZU",
        "outputId": "d28825e7-d1b4-4e4b-86a1-4b8688b0ab56"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(1, None, 5) dtype=float32 (created by layer 'dense_20')>"
            ]
          },
          "execution_count": 943,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Xn9gZ8O0hEf",
        "outputId": "c7dd346f-81e1-4426-cc18-e40c97c16e70"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        }
      ],
      "source": [
        "model = Model(inputs=x_inp, outputs=predictions)\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(lr=0.01),\n",
        "    loss=losses.categorical_crossentropy,\n",
        "    metrics=[\"acc\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqbyKXEJusxm"
      },
      "outputs": [],
      "source": [
        "inp = model.input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWvOBhysuv9D",
        "outputId": "f204174c-6c93-4818-c63b-0dd560125cd1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<KerasTensor: shape=(1, 587, 14) dtype=float32 (created by layer 'input_81')>,\n",
              " <KerasTensor: shape=(1, None) dtype=int32 (created by layer 'input_82')>,\n",
              " <KerasTensor: shape=(1, None, 2) dtype=int64 (created by layer 'input_83')>,\n",
              " <KerasTensor: shape=(1, None) dtype=float32 (created by layer 'input_84')>]"
            ]
          },
          "execution_count": 946,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ab3JTghCBkjS",
        "outputId": "10648e19-94f2-4f0c-9ca9-f3cfc51c122e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_20\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_81 (InputLayer)          [(1, 587, 14)]       0           []                               \n",
            "                                                                                                  \n",
            " input_83 (InputLayer)          [(1, None, 2)]       0           []                               \n",
            "                                                                                                  \n",
            " input_84 (InputLayer)          [(1, None)]          0           []                               \n",
            "                                                                                                  \n",
            " dropout_40 (Dropout)           (1, 587, 14)         0           ['input_81[0][0]']               \n",
            "                                                                                                  \n",
            " squeezed_sparse_conversion_20   (587, 587)          0           ['input_83[0][0]',               \n",
            " (SqueezedSparseConversion)                                       'input_84[0][0]']               \n",
            "                                                                                                  \n",
            " graph_convolution_40 (GraphCon  (1, None, 16)       240         ['dropout_40[0][0]',             \n",
            " volution)                                                        'squeezed_sparse_conversion_20[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " dropout_41 (Dropout)           (1, None, 16)        0           ['graph_convolution_40[0][0]']   \n",
            "                                                                                                  \n",
            " graph_convolution_41 (GraphCon  (1, None, 16)       272         ['dropout_41[0][0]',             \n",
            " volution)                                                        'squeezed_sparse_conversion_20[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " input_82 (InputLayer)          [(1, None)]          0           []                               \n",
            "                                                                                                  \n",
            " gather_indices_20 (GatherIndic  (1, None, 16)       0           ['graph_convolution_41[0][0]',   \n",
            " es)                                                              'input_82[0][0]']               \n",
            "                                                                                                  \n",
            " dense_20 (Dense)               (1, None, 5)         85          ['gather_indices_20[0][0]']      \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 597\n",
            "Trainable params: 597\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yY6tu0g10kEq"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "es_callback = EarlyStopping(monitor=\"val_acc\", patience=50, restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYpoLJwS0u4j"
      },
      "outputs": [],
      "source": [
        "val_gen = generator.flow(val_subjects.index, val_targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAzypgpb0m2l",
        "outputId": "1f3da37a-ceae-41c1-a4ae-d513997dc79b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1/1 - 2s - loss: 1.9689 - acc: 0.0625 - val_loss: 1.9361 - val_acc: 0.0559 - 2s/epoch - 2s/step\n",
            "Epoch 2/200\n",
            "1/1 - 0s - loss: 1.9529 - acc: 0.0719 - val_loss: 1.9236 - val_acc: 0.0559 - 66ms/epoch - 66ms/step\n",
            "Epoch 3/200\n",
            "1/1 - 0s - loss: 1.9093 - acc: 0.0594 - val_loss: 1.9114 - val_acc: 0.0559 - 65ms/epoch - 65ms/step\n",
            "Epoch 4/200\n",
            "1/1 - 0s - loss: 1.9424 - acc: 0.0531 - val_loss: 1.8995 - val_acc: 0.0559 - 60ms/epoch - 60ms/step\n",
            "Epoch 5/200\n",
            "1/1 - 0s - loss: 1.9153 - acc: 0.0719 - val_loss: 1.8878 - val_acc: 0.0559 - 64ms/epoch - 64ms/step\n",
            "Epoch 6/200\n",
            "1/1 - 0s - loss: 1.8752 - acc: 0.0594 - val_loss: 1.8764 - val_acc: 0.0559 - 70ms/epoch - 70ms/step\n",
            "Epoch 7/200\n",
            "1/1 - 0s - loss: 1.8935 - acc: 0.0625 - val_loss: 1.8653 - val_acc: 0.0559 - 66ms/epoch - 66ms/step\n",
            "Epoch 8/200\n",
            "1/1 - 0s - loss: 1.8757 - acc: 0.0719 - val_loss: 1.8545 - val_acc: 0.0559 - 62ms/epoch - 62ms/step\n",
            "Epoch 9/200\n",
            "1/1 - 0s - loss: 1.8453 - acc: 0.1000 - val_loss: 1.8439 - val_acc: 0.0559 - 70ms/epoch - 70ms/step\n",
            "Epoch 10/200\n",
            "1/1 - 0s - loss: 1.8396 - acc: 0.0656 - val_loss: 1.8336 - val_acc: 0.0559 - 69ms/epoch - 69ms/step\n",
            "Epoch 11/200\n",
            "1/1 - 0s - loss: 1.8358 - acc: 0.0688 - val_loss: 1.8235 - val_acc: 0.0559 - 152ms/epoch - 152ms/step\n",
            "Epoch 12/200\n",
            "1/1 - 0s - loss: 1.8230 - acc: 0.0375 - val_loss: 1.8136 - val_acc: 0.0559 - 63ms/epoch - 63ms/step\n",
            "Epoch 13/200\n",
            "1/1 - 0s - loss: 1.8298 - acc: 0.0250 - val_loss: 1.8039 - val_acc: 0.0559 - 144ms/epoch - 144ms/step\n",
            "Epoch 14/200\n",
            "1/1 - 0s - loss: 1.8342 - acc: 0.0406 - val_loss: 1.7945 - val_acc: 0.0559 - 118ms/epoch - 118ms/step\n",
            "Epoch 15/200\n",
            "1/1 - 0s - loss: 1.7808 - acc: 0.0625 - val_loss: 1.7852 - val_acc: 0.0490 - 172ms/epoch - 172ms/step\n",
            "Epoch 16/200\n",
            "1/1 - 0s - loss: 1.7678 - acc: 0.0594 - val_loss: 1.7760 - val_acc: 0.0490 - 145ms/epoch - 145ms/step\n",
            "Epoch 17/200\n",
            "1/1 - 0s - loss: 1.7860 - acc: 0.0719 - val_loss: 1.7670 - val_acc: 0.0490 - 143ms/epoch - 143ms/step\n",
            "Epoch 18/200\n",
            "1/1 - 0s - loss: 1.7768 - acc: 0.0656 - val_loss: 1.7581 - val_acc: 0.0490 - 60ms/epoch - 60ms/step\n",
            "Epoch 19/200\n",
            "1/1 - 0s - loss: 1.7512 - acc: 0.0562 - val_loss: 1.7496 - val_acc: 0.0490 - 63ms/epoch - 63ms/step\n",
            "Epoch 20/200\n",
            "1/1 - 0s - loss: 1.7590 - acc: 0.0688 - val_loss: 1.7413 - val_acc: 0.0490 - 131ms/epoch - 131ms/step\n",
            "Epoch 21/200\n",
            "1/1 - 0s - loss: 1.7388 - acc: 0.0656 - val_loss: 1.7335 - val_acc: 0.0490 - 63ms/epoch - 63ms/step\n",
            "Epoch 22/200\n",
            "1/1 - 0s - loss: 1.7390 - acc: 0.0469 - val_loss: 1.7257 - val_acc: 0.0490 - 135ms/epoch - 135ms/step\n",
            "Epoch 23/200\n",
            "1/1 - 0s - loss: 1.7142 - acc: 0.0656 - val_loss: 1.7181 - val_acc: 0.0490 - 280ms/epoch - 280ms/step\n",
            "Epoch 24/200\n",
            "1/1 - 0s - loss: 1.7145 - acc: 0.0688 - val_loss: 1.7106 - val_acc: 0.0490 - 296ms/epoch - 296ms/step\n",
            "Epoch 25/200\n",
            "1/1 - 0s - loss: 1.7107 - acc: 0.0969 - val_loss: 1.7033 - val_acc: 0.0490 - 224ms/epoch - 224ms/step\n",
            "Epoch 26/200\n",
            "1/1 - 0s - loss: 1.7080 - acc: 0.0781 - val_loss: 1.6962 - val_acc: 0.0629 - 187ms/epoch - 187ms/step\n",
            "Epoch 27/200\n",
            "1/1 - 0s - loss: 1.6692 - acc: 0.0875 - val_loss: 1.6893 - val_acc: 0.0629 - 197ms/epoch - 197ms/step\n",
            "Epoch 28/200\n",
            "1/1 - 0s - loss: 1.6868 - acc: 0.0688 - val_loss: 1.6825 - val_acc: 0.0629 - 207ms/epoch - 207ms/step\n",
            "Epoch 29/200\n",
            "1/1 - 0s - loss: 1.6658 - acc: 0.0781 - val_loss: 1.6759 - val_acc: 0.0629 - 173ms/epoch - 173ms/step\n",
            "Epoch 30/200\n",
            "1/1 - 0s - loss: 1.6764 - acc: 0.0625 - val_loss: 1.6694 - val_acc: 0.0629 - 205ms/epoch - 205ms/step\n",
            "Epoch 31/200\n",
            "1/1 - 0s - loss: 1.6715 - acc: 0.0812 - val_loss: 1.6631 - val_acc: 0.0629 - 231ms/epoch - 231ms/step\n",
            "Epoch 32/200\n",
            "1/1 - 0s - loss: 1.6666 - acc: 0.0656 - val_loss: 1.6565 - val_acc: 0.0629 - 169ms/epoch - 169ms/step\n",
            "Epoch 33/200\n",
            "1/1 - 0s - loss: 1.6375 - acc: 0.0781 - val_loss: 1.6504 - val_acc: 0.0629 - 191ms/epoch - 191ms/step\n",
            "Epoch 34/200\n",
            "1/1 - 0s - loss: 1.6694 - acc: 0.1719 - val_loss: 1.6444 - val_acc: 0.0350 - 91ms/epoch - 91ms/step\n",
            "Epoch 35/200\n",
            "1/1 - 0s - loss: 1.6307 - acc: 0.1469 - val_loss: 1.6385 - val_acc: 0.0490 - 104ms/epoch - 104ms/step\n",
            "Epoch 36/200\n",
            "1/1 - 0s - loss: 1.6323 - acc: 0.1375 - val_loss: 1.6326 - val_acc: 0.0629 - 135ms/epoch - 135ms/step\n",
            "Epoch 37/200\n",
            "1/1 - 0s - loss: 1.6170 - acc: 0.1813 - val_loss: 1.6268 - val_acc: 0.1538 - 92ms/epoch - 92ms/step\n",
            "Epoch 38/200\n",
            "1/1 - 0s - loss: 1.6225 - acc: 0.1437 - val_loss: 1.6211 - val_acc: 0.1888 - 108ms/epoch - 108ms/step\n",
            "Epoch 39/200\n",
            "1/1 - 0s - loss: 1.6212 - acc: 0.1219 - val_loss: 1.6154 - val_acc: 0.1888 - 102ms/epoch - 102ms/step\n",
            "Epoch 40/200\n",
            "1/1 - 0s - loss: 1.6073 - acc: 0.1469 - val_loss: 1.6098 - val_acc: 0.1888 - 128ms/epoch - 128ms/step\n",
            "Epoch 41/200\n",
            "1/1 - 0s - loss: 1.6032 - acc: 0.2281 - val_loss: 1.6043 - val_acc: 0.2098 - 134ms/epoch - 134ms/step\n",
            "Epoch 42/200\n",
            "1/1 - 0s - loss: 1.5991 - acc: 0.1937 - val_loss: 1.5988 - val_acc: 0.2028 - 104ms/epoch - 104ms/step\n",
            "Epoch 43/200\n",
            "1/1 - 0s - loss: 1.6011 - acc: 0.1813 - val_loss: 1.5933 - val_acc: 0.2028 - 101ms/epoch - 101ms/step\n",
            "Epoch 44/200\n",
            "1/1 - 0s - loss: 1.5815 - acc: 0.2156 - val_loss: 1.5878 - val_acc: 0.3287 - 139ms/epoch - 139ms/step\n",
            "Epoch 45/200\n",
            "1/1 - 0s - loss: 1.6044 - acc: 0.2562 - val_loss: 1.5823 - val_acc: 0.3357 - 99ms/epoch - 99ms/step\n",
            "Epoch 46/200\n",
            "1/1 - 0s - loss: 1.5742 - acc: 0.3719 - val_loss: 1.5770 - val_acc: 0.3357 - 135ms/epoch - 135ms/step\n",
            "Epoch 47/200\n",
            "1/1 - 0s - loss: 1.5807 - acc: 0.2562 - val_loss: 1.5717 - val_acc: 0.3357 - 144ms/epoch - 144ms/step\n",
            "Epoch 48/200\n",
            "1/1 - 0s - loss: 1.5556 - acc: 0.3625 - val_loss: 1.5664 - val_acc: 0.3427 - 100ms/epoch - 100ms/step\n",
            "Epoch 49/200\n",
            "1/1 - 0s - loss: 1.5678 - acc: 0.3812 - val_loss: 1.5611 - val_acc: 0.3427 - 137ms/epoch - 137ms/step\n",
            "Epoch 50/200\n",
            "1/1 - 0s - loss: 1.5783 - acc: 0.2438 - val_loss: 1.5559 - val_acc: 0.4266 - 131ms/epoch - 131ms/step\n",
            "Epoch 51/200\n",
            "1/1 - 0s - loss: 1.5455 - acc: 0.4812 - val_loss: 1.5507 - val_acc: 0.4825 - 139ms/epoch - 139ms/step\n",
            "Epoch 52/200\n",
            "1/1 - 0s - loss: 1.5576 - acc: 0.3219 - val_loss: 1.5455 - val_acc: 0.5035 - 93ms/epoch - 93ms/step\n",
            "Epoch 53/200\n",
            "1/1 - 0s - loss: 1.5410 - acc: 0.3906 - val_loss: 1.5403 - val_acc: 0.5035 - 96ms/epoch - 96ms/step\n",
            "Epoch 54/200\n",
            "1/1 - 0s - loss: 1.5329 - acc: 0.4187 - val_loss: 1.5351 - val_acc: 0.5035 - 97ms/epoch - 97ms/step\n",
            "Epoch 55/200\n",
            "1/1 - 0s - loss: 1.5308 - acc: 0.4437 - val_loss: 1.5298 - val_acc: 0.5035 - 113ms/epoch - 113ms/step\n",
            "Epoch 56/200\n",
            "1/1 - 0s - loss: 1.5145 - acc: 0.5156 - val_loss: 1.5247 - val_acc: 0.5175 - 134ms/epoch - 134ms/step\n",
            "Epoch 57/200\n",
            "1/1 - 0s - loss: 1.5173 - acc: 0.5156 - val_loss: 1.5195 - val_acc: 0.5734 - 131ms/epoch - 131ms/step\n",
            "Epoch 58/200\n",
            "1/1 - 0s - loss: 1.5170 - acc: 0.5000 - val_loss: 1.5144 - val_acc: 0.5734 - 97ms/epoch - 97ms/step\n",
            "Epoch 59/200\n",
            "1/1 - 0s - loss: 1.5179 - acc: 0.4313 - val_loss: 1.5093 - val_acc: 0.5734 - 136ms/epoch - 136ms/step\n",
            "Epoch 60/200\n",
            "1/1 - 0s - loss: 1.5005 - acc: 0.5094 - val_loss: 1.5043 - val_acc: 0.5734 - 82ms/epoch - 82ms/step\n",
            "Epoch 61/200\n",
            "1/1 - 0s - loss: 1.4949 - acc: 0.4563 - val_loss: 1.4994 - val_acc: 0.5524 - 63ms/epoch - 63ms/step\n",
            "Epoch 62/200\n",
            "1/1 - 0s - loss: 1.4871 - acc: 0.4906 - val_loss: 1.4946 - val_acc: 0.5524 - 63ms/epoch - 63ms/step\n",
            "Epoch 63/200\n",
            "1/1 - 0s - loss: 1.4806 - acc: 0.5688 - val_loss: 1.4900 - val_acc: 0.5524 - 64ms/epoch - 64ms/step\n",
            "Epoch 64/200\n",
            "1/1 - 0s - loss: 1.4918 - acc: 0.5562 - val_loss: 1.4853 - val_acc: 0.5524 - 64ms/epoch - 64ms/step\n",
            "Epoch 65/200\n",
            "1/1 - 0s - loss: 1.4850 - acc: 0.4844 - val_loss: 1.4807 - val_acc: 0.5524 - 60ms/epoch - 60ms/step\n",
            "Epoch 66/200\n",
            "1/1 - 0s - loss: 1.4756 - acc: 0.5344 - val_loss: 1.4761 - val_acc: 0.5524 - 62ms/epoch - 62ms/step\n",
            "Epoch 67/200\n",
            "1/1 - 0s - loss: 1.4734 - acc: 0.4625 - val_loss: 1.4716 - val_acc: 0.5175 - 76ms/epoch - 76ms/step\n",
            "Epoch 68/200\n",
            "1/1 - 0s - loss: 1.4587 - acc: 0.5531 - val_loss: 1.4672 - val_acc: 0.5175 - 63ms/epoch - 63ms/step\n",
            "Epoch 69/200\n",
            "1/1 - 0s - loss: 1.4697 - acc: 0.4938 - val_loss: 1.4628 - val_acc: 0.5175 - 119ms/epoch - 119ms/step\n",
            "Epoch 70/200\n",
            "1/1 - 0s - loss: 1.4632 - acc: 0.4906 - val_loss: 1.4584 - val_acc: 0.5175 - 63ms/epoch - 63ms/step\n",
            "Epoch 71/200\n",
            "1/1 - 0s - loss: 1.4467 - acc: 0.5938 - val_loss: 1.4541 - val_acc: 0.5245 - 70ms/epoch - 70ms/step\n",
            "Epoch 72/200\n",
            "1/1 - 0s - loss: 1.4600 - acc: 0.5063 - val_loss: 1.4497 - val_acc: 0.5245 - 64ms/epoch - 64ms/step\n",
            "Epoch 73/200\n",
            "1/1 - 0s - loss: 1.4493 - acc: 0.5250 - val_loss: 1.4454 - val_acc: 0.5245 - 72ms/epoch - 72ms/step\n",
            "Epoch 74/200\n",
            "1/1 - 0s - loss: 1.4221 - acc: 0.5188 - val_loss: 1.4410 - val_acc: 0.5245 - 63ms/epoch - 63ms/step\n",
            "Epoch 75/200\n",
            "1/1 - 0s - loss: 1.4284 - acc: 0.4969 - val_loss: 1.4366 - val_acc: 0.5455 - 61ms/epoch - 61ms/step\n",
            "Epoch 76/200\n",
            "1/1 - 0s - loss: 1.4163 - acc: 0.5813 - val_loss: 1.4322 - val_acc: 0.5455 - 62ms/epoch - 62ms/step\n",
            "Epoch 77/200\n",
            "1/1 - 0s - loss: 1.4080 - acc: 0.5562 - val_loss: 1.4278 - val_acc: 0.5455 - 64ms/epoch - 64ms/step\n",
            "Epoch 78/200\n",
            "1/1 - 0s - loss: 1.4390 - acc: 0.4844 - val_loss: 1.4233 - val_acc: 0.5524 - 69ms/epoch - 69ms/step\n",
            "Epoch 79/200\n",
            "1/1 - 0s - loss: 1.4156 - acc: 0.5063 - val_loss: 1.4188 - val_acc: 0.5524 - 65ms/epoch - 65ms/step\n",
            "Epoch 80/200\n",
            "1/1 - 0s - loss: 1.4125 - acc: 0.5781 - val_loss: 1.4142 - val_acc: 0.5524 - 64ms/epoch - 64ms/step\n",
            "Epoch 81/200\n",
            "1/1 - 0s - loss: 1.4123 - acc: 0.5281 - val_loss: 1.4097 - val_acc: 0.5524 - 66ms/epoch - 66ms/step\n",
            "Epoch 82/200\n",
            "1/1 - 0s - loss: 1.4070 - acc: 0.4969 - val_loss: 1.4050 - val_acc: 0.5524 - 65ms/epoch - 65ms/step\n",
            "Epoch 83/200\n",
            "1/1 - 0s - loss: 1.3921 - acc: 0.5312 - val_loss: 1.4004 - val_acc: 0.5524 - 57ms/epoch - 57ms/step\n",
            "Epoch 84/200\n",
            "1/1 - 0s - loss: 1.3870 - acc: 0.5562 - val_loss: 1.3957 - val_acc: 0.5524 - 61ms/epoch - 61ms/step\n",
            "Epoch 85/200\n",
            "1/1 - 0s - loss: 1.4155 - acc: 0.5094 - val_loss: 1.3910 - val_acc: 0.5524 - 70ms/epoch - 70ms/step\n",
            "Epoch 86/200\n",
            "1/1 - 0s - loss: 1.3717 - acc: 0.5312 - val_loss: 1.3863 - val_acc: 0.5524 - 64ms/epoch - 64ms/step\n",
            "Epoch 87/200\n",
            "1/1 - 0s - loss: 1.3588 - acc: 0.5531 - val_loss: 1.3816 - val_acc: 0.5524 - 61ms/epoch - 61ms/step\n",
            "Epoch 88/200\n",
            "1/1 - 0s - loss: 1.3668 - acc: 0.5906 - val_loss: 1.3768 - val_acc: 0.5524 - 63ms/epoch - 63ms/step\n",
            "Epoch 89/200\n",
            "1/1 - 0s - loss: 1.3652 - acc: 0.5469 - val_loss: 1.3721 - val_acc: 0.5524 - 65ms/epoch - 65ms/step\n",
            "Epoch 90/200\n",
            "1/1 - 0s - loss: 1.3563 - acc: 0.5625 - val_loss: 1.3674 - val_acc: 0.5524 - 65ms/epoch - 65ms/step\n",
            "Epoch 91/200\n",
            "1/1 - 0s - loss: 1.3469 - acc: 0.5625 - val_loss: 1.3626 - val_acc: 0.5524 - 59ms/epoch - 59ms/step\n",
            "Epoch 92/200\n",
            "1/1 - 0s - loss: 1.3638 - acc: 0.5531 - val_loss: 1.3578 - val_acc: 0.5524 - 64ms/epoch - 64ms/step\n",
            "Epoch 93/200\n",
            "1/1 - 0s - loss: 1.3368 - acc: 0.5531 - val_loss: 1.3529 - val_acc: 0.5524 - 62ms/epoch - 62ms/step\n",
            "Epoch 94/200\n",
            "1/1 - 0s - loss: 1.3588 - acc: 0.5594 - val_loss: 1.3481 - val_acc: 0.5524 - 60ms/epoch - 60ms/step\n",
            "Epoch 95/200\n",
            "1/1 - 0s - loss: 1.3369 - acc: 0.5312 - val_loss: 1.3433 - val_acc: 0.5524 - 66ms/epoch - 66ms/step\n",
            "Epoch 96/200\n",
            "1/1 - 0s - loss: 1.3159 - acc: 0.5406 - val_loss: 1.3385 - val_acc: 0.5524 - 63ms/epoch - 63ms/step\n",
            "Epoch 97/200\n",
            "1/1 - 0s - loss: 1.3120 - acc: 0.5656 - val_loss: 1.3337 - val_acc: 0.5455 - 60ms/epoch - 60ms/step\n",
            "Epoch 98/200\n",
            "1/1 - 0s - loss: 1.3284 - acc: 0.5312 - val_loss: 1.3289 - val_acc: 0.5385 - 59ms/epoch - 59ms/step\n",
            "Epoch 99/200\n",
            "1/1 - 0s - loss: 1.3328 - acc: 0.5594 - val_loss: 1.3242 - val_acc: 0.5385 - 61ms/epoch - 61ms/step\n",
            "Epoch 100/200\n",
            "1/1 - 0s - loss: 1.3186 - acc: 0.5063 - val_loss: 1.3194 - val_acc: 0.5385 - 58ms/epoch - 58ms/step\n",
            "Epoch 101/200\n",
            "1/1 - 0s - loss: 1.3118 - acc: 0.5500 - val_loss: 1.3147 - val_acc: 0.5385 - 66ms/epoch - 66ms/step\n",
            "Epoch 102/200\n",
            "1/1 - 0s - loss: 1.3000 - acc: 0.5500 - val_loss: 1.3101 - val_acc: 0.5385 - 64ms/epoch - 64ms/step\n",
            "Epoch 103/200\n",
            "1/1 - 0s - loss: 1.2938 - acc: 0.5375 - val_loss: 1.3055 - val_acc: 0.5385 - 79ms/epoch - 79ms/step\n",
            "Epoch 104/200\n",
            "1/1 - 0s - loss: 1.2905 - acc: 0.5625 - val_loss: 1.3008 - val_acc: 0.5664 - 64ms/epoch - 64ms/step\n",
            "Epoch 105/200\n",
            "1/1 - 0s - loss: 1.2806 - acc: 0.5344 - val_loss: 1.2961 - val_acc: 0.5664 - 64ms/epoch - 64ms/step\n",
            "Epoch 106/200\n",
            "1/1 - 0s - loss: 1.2911 - acc: 0.5844 - val_loss: 1.2915 - val_acc: 0.5664 - 63ms/epoch - 63ms/step\n",
            "Epoch 107/200\n",
            "1/1 - 0s - loss: 1.2727 - acc: 0.5625 - val_loss: 1.2869 - val_acc: 0.5664 - 58ms/epoch - 58ms/step\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    train_gen,\n",
        "    epochs=200,\n",
        "    validation_data=val_gen,\n",
        "    verbose=2,\n",
        "    shuffle=False,  # this should be False, since shuffling data means shuffling the whole graph\n",
        "    callbacks=[es_callback],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADUHzeW2cT3o"
      },
      "outputs": [],
      "source": [
        "hist_df = pd.DataFrame(history.history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "L-6nAZKO0qE7",
        "outputId": "169a656c-108c-4901-c8e2-19b242b53c59"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 700x800 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "sg.utils.plot_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWQ-8M0bcbX2"
      },
      "outputs": [],
      "source": [
        "if time == 1:\n",
        "  hist_df.to_excel(\"/content/drive/MyDrive/NewZealand_data/mckee-\"+str(well_name)+\"/\"+str(cls_name)+\"/results/History_\"+str(cls_name)+\".xlsx\")\n",
        "else:\n",
        "  hist_df.to_excel(\"/content/drive/MyDrive/NewZealand_data/mckee-\"+str(well_name)+\"/\"+str(cls_name)+\"/results/History_\"+str(cls_name)+\"new\"+str(time)+\".xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waEtE_T500-_"
      },
      "outputs": [],
      "source": [
        "test_gen = generator.flow(test_subjects.index, test_targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ruleGkX03Ug",
        "outputId": "8e7dcfec-b167-43fb-a99f-9f5feece5cfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 52ms/step - loss: 1.5259 - acc: 0.5403\n",
            "\n",
            "Test Set Metrics:\n",
            "\tloss: 1.5259\n",
            "\tacc: 0.5403\n"
          ]
        }
      ],
      "source": [
        "test_metrics = model.evaluate(test_gen)\n",
        "print(\"\\nTest Set Metrics:\")\n",
        "for name, val in zip(model.metrics_names, test_metrics):\n",
        "    print(\"\\t{}: {:0.4f}\".format(name, val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_1tPB5H05VN",
        "outputId": "bc70b125-a31b-4f84-f891-b4a72929084c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 146ms/step\n"
          ]
        }
      ],
      "source": [
        "all_nodes = node_subjects.index\n",
        "all_gen = generator.flow(all_nodes)\n",
        "all_predictions = model.predict(all_gen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmI_6fUh08La"
      },
      "outputs": [],
      "source": [
        "node_predictions = target_encoding.inverse_transform(all_predictions.squeeze())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iOv1_uec1AvA",
        "outputId": "628d9b8e-bd8e-4c70-ceab-fe158e07d3c6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ccd64851-fed8-4a50-9503-1232cca6c2d1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted</th>\n",
              "      <th>True</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DEPTH</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>22710648</th>\n",
              "      <td>Moderate</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22712172</th>\n",
              "      <td>Moderate</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22713696</th>\n",
              "      <td>High</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2271522</th>\n",
              "      <td>High</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22716744</th>\n",
              "      <td>High</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22718268</th>\n",
              "      <td>High</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22719792</th>\n",
              "      <td>High</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22721316</th>\n",
              "      <td>High</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2272284</th>\n",
              "      <td>High</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22724364</th>\n",
              "      <td>High</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22725888</th>\n",
              "      <td>High</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22727412</th>\n",
              "      <td>High</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22728936</th>\n",
              "      <td>High</td>\n",
              "      <td>Very_High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2273046</th>\n",
              "      <td>High</td>\n",
              "      <td>Very_High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22731984</th>\n",
              "      <td>High</td>\n",
              "      <td>Very_High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22733508</th>\n",
              "      <td>High</td>\n",
              "      <td>Very_High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22735032</th>\n",
              "      <td>High</td>\n",
              "      <td>Very_High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22736556</th>\n",
              "      <td>High</td>\n",
              "      <td>Very_High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2273808</th>\n",
              "      <td>High</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22739604</th>\n",
              "      <td>High</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22741128</th>\n",
              "      <td>High</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22742652</th>\n",
              "      <td>High</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22744176</th>\n",
              "      <td>High</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>227457</th>\n",
              "      <td>High</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22747224</th>\n",
              "      <td>High</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22748748</th>\n",
              "      <td>High</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22750272</th>\n",
              "      <td>High</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22751796</th>\n",
              "      <td>High</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2275332</th>\n",
              "      <td>Very_Low</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22754844</th>\n",
              "      <td>Very_Low</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22756368</th>\n",
              "      <td>Very_Low</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22757892</th>\n",
              "      <td>Very_Low</td>\n",
              "      <td>Low</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22759416</th>\n",
              "      <td>Very_Low</td>\n",
              "      <td>Low</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2276094</th>\n",
              "      <td>Very_Low</td>\n",
              "      <td>Low</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22762464</th>\n",
              "      <td>Very_Low</td>\n",
              "      <td>Low</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22763988</th>\n",
              "      <td>Very_Low</td>\n",
              "      <td>Low</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22765512</th>\n",
              "      <td>Very_Low</td>\n",
              "      <td>Low</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22767036</th>\n",
              "      <td>Very_Low</td>\n",
              "      <td>Low</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2276856</th>\n",
              "      <td>Very_Low</td>\n",
              "      <td>Low</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22770084</th>\n",
              "      <td>Very_Low</td>\n",
              "      <td>Low</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ccd64851-fed8-4a50-9503-1232cca6c2d1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ccd64851-fed8-4a50-9503-1232cca6c2d1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ccd64851-fed8-4a50-9503-1232cca6c2d1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         Predicted       True\n",
              "DEPTH                        \n",
              "22710648  Moderate       High\n",
              "22712172  Moderate       High\n",
              "22713696      High       High\n",
              "2271522       High       High\n",
              "22716744      High       High\n",
              "22718268      High       High\n",
              "22719792      High       High\n",
              "22721316      High       High\n",
              "2272284       High       High\n",
              "22724364      High       High\n",
              "22725888      High       High\n",
              "22727412      High       High\n",
              "22728936      High  Very_High\n",
              "2273046       High  Very_High\n",
              "22731984      High  Very_High\n",
              "22733508      High  Very_High\n",
              "22735032      High  Very_High\n",
              "22736556      High  Very_High\n",
              "2273808       High       High\n",
              "22739604      High       High\n",
              "22741128      High       High\n",
              "22742652      High       High\n",
              "22744176      High       High\n",
              "227457        High       High\n",
              "22747224      High       High\n",
              "22748748      High       High\n",
              "22750272      High       High\n",
              "22751796      High       High\n",
              "2275332   Very_Low       High\n",
              "22754844  Very_Low       High\n",
              "22756368  Very_Low       High\n",
              "22757892  Very_Low        Low\n",
              "22759416  Very_Low        Low\n",
              "2276094   Very_Low        Low\n",
              "22762464  Very_Low        Low\n",
              "22763988  Very_Low        Low\n",
              "22765512  Very_Low        Low\n",
              "22767036  Very_Low        Low\n",
              "2276856   Very_Low        Low\n",
              "22770084  Very_Low        Low"
            ]
          },
          "execution_count": 958,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.DataFrame({\"Predicted\": node_predictions, \"True\": node_subjects})\n",
        "df.head(40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "xCdD61i1fFlr",
        "outputId": "add4861d-3dc3-42fd-e87f-0f122bef8e72"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-069b53a0-c99f-4777-a399-2207aeec01d7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Very_Low</th>\n",
              "      <td>284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Moderate</th>\n",
              "      <td>157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>High</th>\n",
              "      <td>146</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-069b53a0-c99f-4777-a399-2207aeec01d7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-069b53a0-c99f-4777-a399-2207aeec01d7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-069b53a0-c99f-4777-a399-2207aeec01d7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "          Predicted\n",
              "Very_Low        284\n",
              "Moderate        157\n",
              "High            146"
            ]
          },
          "execution_count": 959,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Predicted'].value_counts().to_frame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "5LDXMwzNfUom",
        "outputId": "56d56adf-30b6-4aaf-98a9-f6e1cda1517a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-92e42b50-56a3-494e-8ec0-1538cb254897\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>True</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>High</th>\n",
              "      <td>185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Very_Low</th>\n",
              "      <td>150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Moderate</th>\n",
              "      <td>127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Low</th>\n",
              "      <td>94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Very_High</th>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-92e42b50-56a3-494e-8ec0-1538cb254897')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-92e42b50-56a3-494e-8ec0-1538cb254897 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-92e42b50-56a3-494e-8ec0-1538cb254897');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           True\n",
              "High        185\n",
              "Very_Low    150\n",
              "Moderate    127\n",
              "Low          94\n",
              "Very_High    31"
            ]
          },
          "execution_count": 960,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['True'].value_counts().to_frame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0hzQeA11Ck0"
      },
      "outputs": [],
      "source": [
        "if time == 1:\n",
        "  df.to_excel(\"/content/drive/MyDrive/NewZealand_data/mckee-\"+str(well_name)+\"/\"+str(cls_name)+\"/results/Results_\"+str(cls_name)+\".xlsx\")\n",
        "else:\n",
        "  df.to_excel(\"/content/drive/MyDrive/NewZealand_data/mckee-\"+str(well_name)+\"/\"+str(cls_name)+\"/results/Results_\"+str(cls_name)+\" new\"+str(time)+\".xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEyhA5Sd7UuI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
